{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOpb64xHSdK1"
   },
   "source": [
    "# MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "br7MnyBAfVce"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "import math\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        Wh = torch.mm(h, self.W) # h.shape: (N, in_features), Wh.shape: (N, out_features)\n",
    "        e = self._prepare_attentional_mechanism_input(Wh)\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, Wh)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        # Wh.shape (N, out_feature)\n",
    "        # self.a.shape (2 * out_feature, 1)\n",
    "        # Wh1&2.shape (N, 1)\n",
    "        # e.shape (N, N)\n",
    "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])\n",
    "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])\n",
    "        # broadcast add\n",
    "        e = Wh1 + Wh2.T\n",
    "        return self.leakyrelu(e)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qpSIrx7f_73"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def normalize_features(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    probs = torch.exp(output)\n",
    "    preds = torch.argmax(probs, dim = 1)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def load_multi_data(folder, att_file, edge_list_name):\n",
    "    att = pd.read_csv(folder + att_file)\n",
    "    edge_list = []\n",
    "    for name in edge_list_name:\n",
    "        edge_list.append(pd.read_csv(folder + name))\n",
    "\n",
    "    #get y and x\n",
    "    labels = np.array(att[\"ever_pos\"])\n",
    "    features = sp.csr_matrix(att[[\"# Insert feature variables here\"]])    \n",
    "    #features = normalize_features(features)\n",
    "\n",
    "    #get adj mat\n",
    "    adj_list = []\n",
    "    for edge in edge_list:\n",
    "        #get row col idx for adj matrix\n",
    "        row_idx = []\n",
    "        col_idx = []\n",
    "        for i in range(edge.shape[0]):\n",
    "            id_from = edge.iloc[i,0]\n",
    "            id_to = edge.iloc[i,1]\n",
    "            # uid as unique identifier\n",
    "            row_id = att.index[att[\"uid\"] == id_from]\n",
    "            row_idx.append(row_id[0])\n",
    "            col_id = att.index[att[\"uid\"] == id_to]\n",
    "            col_idx.append(col_id[0])\n",
    "\n",
    "        if edge.shape[1] == 2:\n",
    "            adj = sp.coo_matrix((np.ones(edge.shape[0]), (row_idx, col_idx)), shape=(att.shape[0], att.shape[0]), dtype=np.float32)\n",
    "\n",
    "        elif edge.shape[1] == 3:\n",
    "            adj = sp.coo_matrix((np.array(edge.iloc[:,2]), (row_idx, col_idx)), shape=(att.shape[0], att.shape[0]), dtype=np.float32)\n",
    "\n",
    "        #make adj symmetric\n",
    "        adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "        #normaliza adj\n",
    "        adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "        adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "        adj_list.append(adj)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    dim = len(labels)\n",
    "    idx_train = range(1585)\n",
    "    idx_test = range(1585, dim)\n",
    "\n",
    "    return adj_list, features, labels, idx_train, idx_test\n",
    "\n",
    "def load_data(folder, att_file, edge_name):\n",
    "    att = pd.read_csv(folder + att_file)\n",
    "    edge_list = pd.read_csv(folder + edge_name)\n",
    "\n",
    "    #get y and x\n",
    "    labels = np.array(att[\"y\"])\n",
    "    # b,c,d are age groups\n",
    "    features = sp.csr_matrix(att[[\"# Insert feature variables here\"]])\n",
    "    #features = normalize_features(features)\n",
    "\n",
    "    #get adj mat\n",
    "    row_idx = []\n",
    "    col_idx = []\n",
    "    for i in range(edge_list.shape[0]):\n",
    "        id_from = edge_list.iloc[i,0]\n",
    "        id_to = edge_list.iloc[i,1]\n",
    "        row_id = att.index[att[\"uid\"] == id_from]\n",
    "        row_idx.append(row_id[0])\n",
    "        col_id = att.index[att[\"uid\"] == id_to]\n",
    "        col_idx.append(col_id[0])\n",
    "\n",
    "    if edge_list.shape[1] == 2:\n",
    "        adj = sp.coo_matrix((np.ones(edge_list.shape[0]), (row_idx, col_idx)), shape=(att.shape[0], att.shape[0]), dtype=np.float32)\n",
    "\n",
    "\n",
    "        #make adj symmetric\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "        #normaliza adj\n",
    "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    dim = len(labels)\n",
    "    idx_train = range(1585)\n",
    "    idx_test = range(1585, dim)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5auJI3dgKKO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"Dense version of GAT.\"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = nn.Linear(nhid * nheads, nclass)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x))\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class FusionGAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, adj_list, nheads):\n",
    "        super(FusionGAT, self).__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.nheads = nheads\n",
    "        self.adj_list = adj_list\n",
    "\n",
    "        # Define list of GAT layers for each adjacency matrix\n",
    "        self.attentions = nn.ModuleList()\n",
    "        for i in range(len(adj_list)):\n",
    "            att_list = nn.ModuleList([GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
    "            self.attentions.append(att_list)\n",
    "            for k, attention in enumerate(self.attentions):\n",
    "                self.add_module('adj{}, attention_{}'.format(i, k), attention)\n",
    "\n",
    "        # Define linear layer for integration with L1 regularization\n",
    "        self.integration_att = nn.Linear(nhid * nheads, nclass)\n",
    "        self.fusion = nn.Linear(nclass * len(adj_list), nclass)\n",
    "        self.l1_reg = nn.L1Loss(reduction='mean')\n",
    "\n",
    "    def forward(self, x, adj_list):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        # Compute output for each adjacency matrix using GAT layers\n",
    "        output_list = []\n",
    "        for i, adj in enumerate(adj_list):\n",
    "            x_i = torch.cat([att(x, adj) for att in self.attentions[i]], dim=1)\n",
    "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
    "            x_i = F.elu(self.integration_att(x_i))\n",
    "            output_list.append(x_i)\n",
    "        output = torch.cat(output_list, dim=1)\n",
    "\n",
    "        # Apply linear layer for integration with L1 regularization\n",
    "        output = F.dropout(output, self.dropout, training=self.training)\n",
    "        output = self.fusion(output.view(output.size(0), -1))\n",
    "        l1_loss = self.l1_reg(self.fusion.weight, torch.zeros_like(self.fusion.weight))\n",
    "        return F.log_softmax(output, dim=1), l1_loss\n",
    "\n",
    "\n",
    "\n",
    "class FusionGAT2(nn.Module):\n",
    "    def __init__(self, nfeat, nhid1, nhid2, nclass, dropout, alpha, adj_list, nheads):\n",
    "        super(FusionGAT2, self).__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.nheads = nheads\n",
    "        self.adj_list = adj_list\n",
    "\n",
    "        # Define list of GAT layers for each adjacency matrix in attention 1\n",
    "        self.attentions1 = nn.ModuleList()\n",
    "        for i in range(len(adj_list)):\n",
    "            att_list = nn.ModuleList([GraphAttentionLayer(nfeat, nhid1, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
    "            self.attentions1.append(att_list)\n",
    "            for k, attention in enumerate(self.attentions1):\n",
    "                self.add_module('adj{}, attention_layer1_{}'.format(i, k), attention)\n",
    "\n",
    "        # Define linear layer for integration of multihead attention1\n",
    "        self.integration_att1 = nn.Linear(nhid1 * nheads, nhid1)\n",
    "\n",
    "        self.attentions2 = nn.ModuleList()\n",
    "        for i in range(len(adj_list)):\n",
    "            att_list2 = nn.ModuleList([GraphAttentionLayer(nhid1, nhid2, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
    "            self.attentions2.append(att_list2)\n",
    "            for k, attention in enumerate(self.attentions2):\n",
    "                self.add_module('adj{}, attention_layer2_{}'.format(i, k), attention)\n",
    "\n",
    "        # Define linear layer for integration of multihead attention2\n",
    "        self.integration_att2 = nn.Linear(nhid2 * nheads, nclass)\n",
    "\n",
    "        #fusion layer with l1 penalty\n",
    "        self.fusion_att = nn.Linear(nclass * len(adj_list), nclass)\n",
    "        self.l1_reg = nn.L1Loss(reduction='mean')\n",
    "\n",
    "    def forward(self, x, adj_list):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        # Compute output for each adjacency matrix using GAT layers\n",
    "        output_list = []\n",
    "        for i, adj in enumerate(adj_list):\n",
    "            #attention layer 1\n",
    "            x_i = torch.cat([att(x, adj) for att in self.attentions1[i]], dim=1)\n",
    "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
    "            x_i = F.elu(self.integration_att1(x_i))\n",
    "\n",
    "            #attention layer 2\n",
    "            x_i = torch.cat([att(x_i, adj) for att in self.attentions2[i]], dim=1)\n",
    "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
    "            x_i = F.elu(self.integration_att2(x_i))\n",
    "            output_list.append(x_i)\n",
    "\n",
    "        output = torch.cat(output_list, dim=1)\n",
    "        output = F.dropout(output, self.dropout, training=self.training)\n",
    "        output = self.fusion_att(output.view(output.size(0), -1))\n",
    "        l1_loss = self.l1_reg(self.fusion_att.weight, torch.zeros_like(self.fusion_att.weight))\n",
    "        return F.log_softmax(output, dim=1), l1_loss\n",
    "\n",
    "\n",
    "\n",
    "class FusionGAT3(nn.Module):\n",
    "    def __init__(self, nfeat, nhid1, nhid2, fusion1_dim, nclass, dropout, alpha, adj_list, nheads):\n",
    "        super(FusionGAT3, self).__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.nheads = nheads\n",
    "        self.adj_list = adj_list\n",
    "\n",
    "        # Define list of GAT layers for each adjacency matrix\n",
    "        #att 1\n",
    "        self.attentions1 = nn.ModuleList()\n",
    "        for i in range(len(adj_list)):\n",
    "            att_list = nn.ModuleList([GraphAttentionLayer(nfeat, nhid1, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
    "            self.attentions1.append(att_list)\n",
    "            for k, attention in enumerate(self.attentions1):\n",
    "                self.add_module('adj{}, attention_layer1_{}'.format(i, k), attention)\n",
    "\n",
    "        # fusion1\n",
    "        self.integration_att1 = nn.Linear(nhid1 * nheads, fusion1_dim)\n",
    "        self.fusion_att1 = nn.Linear(fusion1_dim * len(adj_list), fusion1_dim)\n",
    "        self.l1_reg1 = nn.L1Loss(reduction='mean')\n",
    "\n",
    "        #att 2\n",
    "        self.attentions2 = nn.ModuleList()\n",
    "        for i in range(len(adj_list)):\n",
    "            att_list = nn.ModuleList([GraphAttentionLayer(fusion1_dim, nhid2, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
    "            self.attentions2.append(att_list)\n",
    "            for k, attention in enumerate(self.attentions2):\n",
    "                self.add_module('adj{}, attention_layer2_{}'.format(i, k), attention)\n",
    "\n",
    "        #fusion2\n",
    "        self.integration_att2 = nn.Linear(nhid2 * nheads, nclass)\n",
    "        self.fusion_att2 = nn.Linear(nclass * len(adj_list), nclass)\n",
    "        self.l1_reg2 = nn.L1Loss(reduction='mean')\n",
    "\n",
    "\n",
    "    def forward(self, x, adj_list):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "        # Compute output for each adjacency matrix using GAT layers\n",
    "        output_list = []\n",
    "        for i, adj in enumerate(adj_list):\n",
    "            x_i = x\n",
    "            x_i = torch.cat([att(x, adj) for att in self.attentions1[i]], dim=1)\n",
    "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
    "            x_i = F.elu(self.integration_att1(x_i))\n",
    "            output_list.append(x_i)\n",
    "        output = torch.cat(output_list, dim=1)\n",
    "\n",
    "        # Apply linear layer for integration with L1 regularization\n",
    "        output = F.dropout(output, self.dropout, training=self.training)\n",
    "        output = self.fusion_att1(output.view(output.size(0), -1))\n",
    "        l1_loss1 = self.l1_reg1(self.fusion_att1.weight, torch.zeros_like(self.fusion_att1.weight))\n",
    "\n",
    "\n",
    "        output_list2 = []\n",
    "        for i, adj in enumerate(adj_list):\n",
    "            x_i = torch.cat([att(output, adj) for att in self.attentions2[i]], dim=1)\n",
    "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
    "            x_i = F.elu(self.integration_att2(x_i))\n",
    "            output_list2.append(x_i)\n",
    "        output2 = torch.cat(output_list2, dim=1)\n",
    "\n",
    "        # Apply linear layer for integration with L1 regularization\n",
    "        output2 = F.dropout(output2, self.dropout, training=self.training)\n",
    "        output2 = self.fusion_att2(output2.view(output.size(0), -1))\n",
    "        l1_loss2 = self.l1_reg2(self.fusion_att2.weight, torch.zeros_like(self.fusion_att2.weight))\n",
    "\n",
    "        return F.log_softmax(output2, dim=1), l1_loss1 + l1_loss2\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 741395,
     "status": "ok",
     "timestamp": 1731091063814,
     "user": {
      "displayName": "Baode Gao",
      "userId": "00825733222134474059"
     },
     "user_tz": 360
    },
    "id": "mmct_H8lgNms",
    "outputId": "6c85f8fe-6e27-40b4-9273-f5a5b605af8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 0.693 acc_train: 0.536 loss_test: 0.702 acc_test: 0.517 AUC = 0.52 time: 2.365s\n",
      "Epoch: 002 loss_train: 0.693 acc_train: 0.560 loss_test: 0.699 acc_test: 0.517 AUC = 0.50 time: 0.493s\n",
      "Epoch: 003 loss_train: 0.688 acc_train: 0.560 loss_test: 0.707 acc_test: 0.521 AUC = 0.48 time: 0.494s\n",
      "Epoch: 004 loss_train: 0.684 acc_train: 0.582 loss_test: 0.712 acc_test: 0.474 AUC = 0.47 time: 0.494s\n",
      "Epoch: 005 loss_train: 0.684 acc_train: 0.565 loss_test: 0.704 acc_test: 0.532 AUC = 0.52 time: 0.493s\n",
      "Epoch: 006 loss_train: 0.675 acc_train: 0.602 loss_test: 0.704 acc_test: 0.545 AUC = 0.53 time: 0.493s\n",
      "Epoch: 007 loss_train: 0.681 acc_train: 0.604 loss_test: 0.690 acc_test: 0.535 AUC = 0.55 time: 0.494s\n",
      "Epoch: 008 loss_train: 0.672 acc_train: 0.631 loss_test: 0.704 acc_test: 0.560 AUC = 0.53 time: 0.494s\n",
      "Epoch: 009 loss_train: 0.665 acc_train: 0.632 loss_test: 0.699 acc_test: 0.543 AUC = 0.54 time: 0.494s\n",
      "Epoch: 010 loss_train: 0.664 acc_train: 0.650 loss_test: 0.697 acc_test: 0.530 AUC = 0.52 time: 0.494s\n",
      "Epoch: 011 loss_train: 0.658 acc_train: 0.650 loss_test: 0.720 acc_test: 0.560 AUC = 0.54 time: 0.494s\n",
      "Epoch: 012 loss_train: 0.660 acc_train: 0.649 loss_test: 0.704 acc_test: 0.549 AUC = 0.55 time: 0.495s\n",
      "Epoch: 013 loss_train: 0.650 acc_train: 0.673 loss_test: 0.705 acc_test: 0.532 AUC = 0.53 time: 0.494s\n",
      "Epoch: 014 loss_train: 0.650 acc_train: 0.669 loss_test: 0.711 acc_test: 0.561 AUC = 0.55 time: 0.494s\n",
      "Epoch: 015 loss_train: 0.661 acc_train: 0.679 loss_test: 0.717 acc_test: 0.535 AUC = 0.52 time: 0.494s\n",
      "Epoch: 016 loss_train: 0.644 acc_train: 0.679 loss_test: 0.706 acc_test: 0.551 AUC = 0.54 time: 0.494s\n",
      "Epoch: 017 loss_train: 0.640 acc_train: 0.676 loss_test: 0.719 acc_test: 0.532 AUC = 0.54 time: 0.494s\n",
      "Epoch: 018 loss_train: 0.634 acc_train: 0.685 loss_test: 0.698 acc_test: 0.554 AUC = 0.58 time: 0.495s\n",
      "Epoch: 019 loss_train: 0.630 acc_train: 0.697 loss_test: 0.708 acc_test: 0.546 AUC = 0.59 time: 0.494s\n",
      "Epoch: 020 loss_train: 0.633 acc_train: 0.693 loss_test: 0.715 acc_test: 0.551 AUC = 0.56 time: 0.494s\n",
      "Epoch: 021 loss_train: 0.635 acc_train: 0.688 loss_test: 0.710 acc_test: 0.557 AUC = 0.62 time: 0.494s\n",
      "Epoch: 022 loss_train: 0.639 acc_train: 0.690 loss_test: 0.729 acc_test: 0.552 AUC = 0.54 time: 0.494s\n",
      "Epoch: 023 loss_train: 0.634 acc_train: 0.698 loss_test: 0.735 acc_test: 0.545 AUC = 0.57 time: 0.494s\n",
      "Epoch: 024 loss_train: 0.628 acc_train: 0.680 loss_test: 0.706 acc_test: 0.561 AUC = 0.60 time: 0.494s\n",
      "Epoch: 025 loss_train: 0.617 acc_train: 0.706 loss_test: 0.705 acc_test: 0.558 AUC = 0.59 time: 0.494s\n",
      "Epoch: 026 loss_train: 0.622 acc_train: 0.702 loss_test: 0.720 acc_test: 0.558 AUC = 0.61 time: 0.495s\n",
      "Epoch: 027 loss_train: 0.613 acc_train: 0.703 loss_test: 0.713 acc_test: 0.567 AUC = 0.60 time: 0.495s\n",
      "Epoch: 028 loss_train: 0.615 acc_train: 0.698 loss_test: 0.692 acc_test: 0.552 AUC = 0.64 time: 0.495s\n",
      "Epoch: 029 loss_train: 0.610 acc_train: 0.703 loss_test: 0.707 acc_test: 0.573 AUC = 0.59 time: 0.495s\n",
      "Epoch: 030 loss_train: 0.605 acc_train: 0.705 loss_test: 0.695 acc_test: 0.586 AUC = 0.63 time: 0.495s\n",
      "Epoch: 031 loss_train: 0.589 acc_train: 0.712 loss_test: 0.711 acc_test: 0.561 AUC = 0.60 time: 0.495s\n",
      "Epoch: 032 loss_train: 0.601 acc_train: 0.707 loss_test: 0.707 acc_test: 0.580 AUC = 0.62 time: 0.494s\n",
      "Epoch: 033 loss_train: 0.589 acc_train: 0.712 loss_test: 0.690 acc_test: 0.583 AUC = 0.61 time: 0.495s\n",
      "Epoch: 034 loss_train: 0.595 acc_train: 0.702 loss_test: 0.681 acc_test: 0.602 AUC = 0.63 time: 0.496s\n",
      "Epoch: 035 loss_train: 0.590 acc_train: 0.731 loss_test: 0.699 acc_test: 0.598 AUC = 0.65 time: 0.495s\n",
      "Epoch: 036 loss_train: 0.598 acc_train: 0.703 loss_test: 0.683 acc_test: 0.598 AUC = 0.62 time: 0.495s\n",
      "Epoch: 037 loss_train: 0.574 acc_train: 0.727 loss_test: 0.701 acc_test: 0.610 AUC = 0.64 time: 0.495s\n",
      "Epoch: 038 loss_train: 0.584 acc_train: 0.717 loss_test: 0.674 acc_test: 0.607 AUC = 0.65 time: 0.495s\n",
      "Epoch: 039 loss_train: 0.573 acc_train: 0.726 loss_test: 0.677 acc_test: 0.613 AUC = 0.66 time: 0.495s\n",
      "Epoch: 040 loss_train: 0.585 acc_train: 0.723 loss_test: 0.668 acc_test: 0.617 AUC = 0.66 time: 0.495s\n",
      "Epoch: 041 loss_train: 0.583 acc_train: 0.717 loss_test: 0.662 acc_test: 0.613 AUC = 0.67 time: 0.495s\n",
      "Epoch: 042 loss_train: 0.576 acc_train: 0.729 loss_test: 0.675 acc_test: 0.626 AUC = 0.66 time: 0.495s\n",
      "Epoch: 043 loss_train: 0.570 acc_train: 0.734 loss_test: 0.690 acc_test: 0.623 AUC = 0.64 time: 0.495s\n",
      "Epoch: 044 loss_train: 0.590 acc_train: 0.726 loss_test: 0.684 acc_test: 0.616 AUC = 0.67 time: 0.495s\n",
      "Epoch: 045 loss_train: 0.578 acc_train: 0.744 loss_test: 0.689 acc_test: 0.607 AUC = 0.65 time: 0.495s\n",
      "Epoch: 046 loss_train: 0.551 acc_train: 0.742 loss_test: 0.684 acc_test: 0.619 AUC = 0.65 time: 0.495s\n",
      "Epoch: 047 loss_train: 0.572 acc_train: 0.733 loss_test: 0.690 acc_test: 0.623 AUC = 0.65 time: 0.495s\n",
      "Epoch: 048 loss_train: 0.555 acc_train: 0.750 loss_test: 0.642 acc_test: 0.648 AUC = 0.69 time: 0.495s\n",
      "Epoch: 049 loss_train: 0.570 acc_train: 0.741 loss_test: 0.694 acc_test: 0.623 AUC = 0.66 time: 0.496s\n",
      "Epoch: 050 loss_train: 0.555 acc_train: 0.739 loss_test: 0.654 acc_test: 0.632 AUC = 0.68 time: 0.496s\n",
      "Epoch: 051 loss_train: 0.558 acc_train: 0.747 loss_test: 0.650 acc_test: 0.632 AUC = 0.71 time: 0.496s\n",
      "Epoch: 052 loss_train: 0.562 acc_train: 0.734 loss_test: 0.666 acc_test: 0.644 AUC = 0.69 time: 0.495s\n",
      "Epoch: 053 loss_train: 0.547 acc_train: 0.749 loss_test: 0.703 acc_test: 0.642 AUC = 0.65 time: 0.496s\n",
      "Epoch: 054 loss_train: 0.551 acc_train: 0.744 loss_test: 0.679 acc_test: 0.639 AUC = 0.67 time: 0.495s\n",
      "Epoch: 055 loss_train: 0.549 acc_train: 0.748 loss_test: 0.642 acc_test: 0.661 AUC = 0.72 time: 0.496s\n",
      "Epoch: 056 loss_train: 0.557 acc_train: 0.741 loss_test: 0.673 acc_test: 0.651 AUC = 0.68 time: 0.495s\n",
      "Epoch: 057 loss_train: 0.559 acc_train: 0.751 loss_test: 0.705 acc_test: 0.626 AUC = 0.66 time: 0.496s\n",
      "Epoch: 058 loss_train: 0.546 acc_train: 0.753 loss_test: 0.679 acc_test: 0.639 AUC = 0.67 time: 0.495s\n",
      "Epoch: 059 loss_train: 0.557 acc_train: 0.741 loss_test: 0.673 acc_test: 0.645 AUC = 0.68 time: 0.496s\n",
      "Epoch: 060 loss_train: 0.552 acc_train: 0.746 loss_test: 0.689 acc_test: 0.632 AUC = 0.68 time: 0.496s\n",
      "Epoch: 061 loss_train: 0.547 acc_train: 0.753 loss_test: 0.660 acc_test: 0.667 AUC = 0.69 time: 0.496s\n",
      "Epoch: 062 loss_train: 0.545 acc_train: 0.747 loss_test: 0.689 acc_test: 0.642 AUC = 0.68 time: 0.495s\n",
      "Epoch: 063 loss_train: 0.549 acc_train: 0.743 loss_test: 0.654 acc_test: 0.648 AUC = 0.70 time: 0.496s\n",
      "Epoch: 064 loss_train: 0.543 acc_train: 0.750 loss_test: 0.647 acc_test: 0.651 AUC = 0.71 time: 0.496s\n",
      "Epoch: 065 loss_train: 0.544 acc_train: 0.744 loss_test: 0.691 acc_test: 0.636 AUC = 0.68 time: 0.496s\n",
      "Epoch: 066 loss_train: 0.547 acc_train: 0.742 loss_test: 0.672 acc_test: 0.645 AUC = 0.69 time: 0.495s\n",
      "Epoch: 067 loss_train: 0.538 acc_train: 0.763 loss_test: 0.650 acc_test: 0.642 AUC = 0.72 time: 0.495s\n",
      "Epoch: 068 loss_train: 0.552 acc_train: 0.740 loss_test: 0.649 acc_test: 0.648 AUC = 0.71 time: 0.496s\n",
      "Epoch: 069 loss_train: 0.536 acc_train: 0.769 loss_test: 0.647 acc_test: 0.657 AUC = 0.71 time: 0.496s\n",
      "Epoch: 070 loss_train: 0.533 acc_train: 0.758 loss_test: 0.659 acc_test: 0.645 AUC = 0.70 time: 0.497s\n",
      "Epoch: 071 loss_train: 0.549 acc_train: 0.753 loss_test: 0.644 acc_test: 0.641 AUC = 0.72 time: 0.496s\n",
      "Epoch: 072 loss_train: 0.533 acc_train: 0.747 loss_test: 0.669 acc_test: 0.651 AUC = 0.69 time: 0.496s\n",
      "Epoch: 073 loss_train: 0.544 acc_train: 0.744 loss_test: 0.662 acc_test: 0.672 AUC = 0.72 time: 0.496s\n",
      "Epoch: 074 loss_train: 0.542 acc_train: 0.756 loss_test: 0.649 acc_test: 0.664 AUC = 0.72 time: 0.496s\n",
      "Epoch: 075 loss_train: 0.547 acc_train: 0.746 loss_test: 0.654 acc_test: 0.666 AUC = 0.71 time: 0.496s\n",
      "Epoch: 076 loss_train: 0.531 acc_train: 0.749 loss_test: 0.640 acc_test: 0.664 AUC = 0.70 time: 0.496s\n",
      "Epoch: 077 loss_train: 0.543 acc_train: 0.747 loss_test: 0.669 acc_test: 0.632 AUC = 0.70 time: 0.496s\n",
      "Epoch: 078 loss_train: 0.538 acc_train: 0.753 loss_test: 0.659 acc_test: 0.651 AUC = 0.71 time: 0.496s\n",
      "Epoch: 079 loss_train: 0.547 acc_train: 0.753 loss_test: 0.625 acc_test: 0.682 AUC = 0.73 time: 0.496s\n",
      "Epoch: 080 loss_train: 0.521 acc_train: 0.764 loss_test: 0.630 acc_test: 0.652 AUC = 0.73 time: 0.496s\n",
      "Epoch: 081 loss_train: 0.529 acc_train: 0.763 loss_test: 0.630 acc_test: 0.664 AUC = 0.73 time: 0.496s\n",
      "Epoch: 082 loss_train: 0.540 acc_train: 0.755 loss_test: 0.633 acc_test: 0.658 AUC = 0.72 time: 0.497s\n",
      "Epoch: 083 loss_train: 0.525 acc_train: 0.759 loss_test: 0.650 acc_test: 0.661 AUC = 0.72 time: 0.496s\n",
      "Epoch: 084 loss_train: 0.529 acc_train: 0.750 loss_test: 0.668 acc_test: 0.651 AUC = 0.70 time: 0.496s\n",
      "Epoch: 085 loss_train: 0.530 acc_train: 0.759 loss_test: 0.638 acc_test: 0.655 AUC = 0.72 time: 0.496s\n",
      "Epoch: 086 loss_train: 0.524 acc_train: 0.772 loss_test: 0.649 acc_test: 0.658 AUC = 0.71 time: 0.496s\n",
      "Epoch: 087 loss_train: 0.538 acc_train: 0.763 loss_test: 0.640 acc_test: 0.663 AUC = 0.72 time: 0.497s\n",
      "Epoch: 088 loss_train: 0.517 acc_train: 0.760 loss_test: 0.663 acc_test: 0.655 AUC = 0.71 time: 0.499s\n",
      "Epoch: 089 loss_train: 0.524 acc_train: 0.759 loss_test: 0.661 acc_test: 0.663 AUC = 0.71 time: 0.497s\n",
      "Epoch: 090 loss_train: 0.530 acc_train: 0.766 loss_test: 0.607 acc_test: 0.673 AUC = 0.75 time: 0.497s\n",
      "Epoch: 091 loss_train: 0.536 acc_train: 0.755 loss_test: 0.619 acc_test: 0.679 AUC = 0.75 time: 0.497s\n",
      "Epoch: 092 loss_train: 0.521 acc_train: 0.755 loss_test: 0.617 acc_test: 0.676 AUC = 0.73 time: 0.497s\n",
      "Epoch: 093 loss_train: 0.514 acc_train: 0.764 loss_test: 0.639 acc_test: 0.666 AUC = 0.74 time: 0.497s\n",
      "Epoch: 094 loss_train: 0.508 acc_train: 0.770 loss_test: 0.637 acc_test: 0.669 AUC = 0.72 time: 0.497s\n",
      "Epoch: 095 loss_train: 0.524 acc_train: 0.754 loss_test: 0.629 acc_test: 0.677 AUC = 0.74 time: 0.497s\n",
      "Epoch: 096 loss_train: 0.512 acc_train: 0.768 loss_test: 0.656 acc_test: 0.636 AUC = 0.71 time: 0.497s\n",
      "Epoch: 097 loss_train: 0.521 acc_train: 0.768 loss_test: 0.634 acc_test: 0.675 AUC = 0.74 time: 0.497s\n",
      "Epoch: 098 loss_train: 0.516 acc_train: 0.753 loss_test: 0.633 acc_test: 0.669 AUC = 0.74 time: 0.497s\n",
      "Epoch: 099 loss_train: 0.511 acc_train: 0.765 loss_test: 0.623 acc_test: 0.673 AUC = 0.75 time: 0.497s\n",
      "Epoch: 100 loss_train: 0.518 acc_train: 0.767 loss_test: 0.627 acc_test: 0.672 AUC = 0.74 time: 0.497s\n",
      "Epoch: 101 loss_train: 0.516 acc_train: 0.767 loss_test: 0.655 acc_test: 0.663 AUC = 0.74 time: 0.497s\n",
      "Epoch: 102 loss_train: 0.516 acc_train: 0.763 loss_test: 0.639 acc_test: 0.667 AUC = 0.74 time: 0.497s\n",
      "Epoch: 103 loss_train: 0.513 acc_train: 0.763 loss_test: 0.618 acc_test: 0.648 AUC = 0.75 time: 0.497s\n",
      "Epoch: 104 loss_train: 0.531 acc_train: 0.753 loss_test: 0.640 acc_test: 0.652 AUC = 0.73 time: 0.497s\n",
      "Epoch: 105 loss_train: 0.526 acc_train: 0.763 loss_test: 0.631 acc_test: 0.686 AUC = 0.75 time: 0.497s\n",
      "Epoch: 106 loss_train: 0.525 acc_train: 0.765 loss_test: 0.631 acc_test: 0.664 AUC = 0.73 time: 0.498s\n",
      "Epoch: 107 loss_train: 0.517 acc_train: 0.766 loss_test: 0.620 acc_test: 0.675 AUC = 0.74 time: 0.497s\n",
      "Epoch: 108 loss_train: 0.511 acc_train: 0.774 loss_test: 0.618 acc_test: 0.679 AUC = 0.74 time: 0.499s\n",
      "Epoch: 109 loss_train: 0.516 acc_train: 0.760 loss_test: 0.614 acc_test: 0.679 AUC = 0.76 time: 0.497s\n",
      "Epoch: 110 loss_train: 0.518 acc_train: 0.765 loss_test: 0.634 acc_test: 0.667 AUC = 0.74 time: 0.497s\n",
      "Epoch: 111 loss_train: 0.520 acc_train: 0.769 loss_test: 0.598 acc_test: 0.697 AUC = 0.77 time: 0.498s\n",
      "Epoch: 112 loss_train: 0.515 acc_train: 0.762 loss_test: 0.677 acc_test: 0.649 AUC = 0.71 time: 0.498s\n",
      "Epoch: 113 loss_train: 0.512 acc_train: 0.772 loss_test: 0.598 acc_test: 0.686 AUC = 0.76 time: 0.498s\n",
      "Epoch: 114 loss_train: 0.514 acc_train: 0.767 loss_test: 0.634 acc_test: 0.667 AUC = 0.74 time: 0.498s\n",
      "Epoch: 115 loss_train: 0.516 acc_train: 0.767 loss_test: 0.588 acc_test: 0.675 AUC = 0.77 time: 0.498s\n",
      "Epoch: 116 loss_train: 0.515 acc_train: 0.767 loss_test: 0.622 acc_test: 0.679 AUC = 0.76 time: 0.498s\n",
      "Epoch: 117 loss_train: 0.517 acc_train: 0.761 loss_test: 0.604 acc_test: 0.679 AUC = 0.77 time: 0.498s\n",
      "Epoch: 118 loss_train: 0.522 acc_train: 0.767 loss_test: 0.608 acc_test: 0.682 AUC = 0.75 time: 0.498s\n",
      "Epoch: 119 loss_train: 0.500 acc_train: 0.780 loss_test: 0.610 acc_test: 0.677 AUC = 0.75 time: 0.498s\n",
      "Epoch: 120 loss_train: 0.509 acc_train: 0.768 loss_test: 0.629 acc_test: 0.670 AUC = 0.74 time: 0.498s\n",
      "Epoch: 121 loss_train: 0.506 acc_train: 0.773 loss_test: 0.596 acc_test: 0.680 AUC = 0.76 time: 0.498s\n",
      "Epoch: 122 loss_train: 0.516 acc_train: 0.768 loss_test: 0.601 acc_test: 0.680 AUC = 0.76 time: 0.499s\n",
      "Epoch: 123 loss_train: 0.504 acc_train: 0.772 loss_test: 0.645 acc_test: 0.664 AUC = 0.74 time: 0.498s\n",
      "Epoch: 124 loss_train: 0.509 acc_train: 0.767 loss_test: 0.631 acc_test: 0.685 AUC = 0.75 time: 0.500s\n",
      "Epoch: 125 loss_train: 0.495 acc_train: 0.780 loss_test: 0.601 acc_test: 0.689 AUC = 0.76 time: 0.498s\n",
      "Epoch: 126 loss_train: 0.503 acc_train: 0.763 loss_test: 0.611 acc_test: 0.672 AUC = 0.76 time: 0.498s\n",
      "Epoch: 127 loss_train: 0.496 acc_train: 0.783 loss_test: 0.582 acc_test: 0.688 AUC = 0.78 time: 0.498s\n",
      "Epoch: 128 loss_train: 0.494 acc_train: 0.787 loss_test: 0.601 acc_test: 0.691 AUC = 0.75 time: 0.502s\n",
      "Epoch: 129 loss_train: 0.500 acc_train: 0.777 loss_test: 0.645 acc_test: 0.682 AUC = 0.73 time: 0.499s\n",
      "Epoch: 130 loss_train: 0.495 acc_train: 0.780 loss_test: 0.604 acc_test: 0.688 AUC = 0.77 time: 0.498s\n",
      "Epoch: 131 loss_train: 0.499 acc_train: 0.780 loss_test: 0.623 acc_test: 0.683 AUC = 0.75 time: 0.499s\n",
      "Epoch: 132 loss_train: 0.501 acc_train: 0.765 loss_test: 0.599 acc_test: 0.685 AUC = 0.77 time: 0.499s\n",
      "Epoch: 133 loss_train: 0.496 acc_train: 0.784 loss_test: 0.610 acc_test: 0.679 AUC = 0.76 time: 0.499s\n",
      "Epoch: 134 loss_train: 0.490 acc_train: 0.789 loss_test: 0.624 acc_test: 0.677 AUC = 0.76 time: 0.498s\n",
      "Epoch: 135 loss_train: 0.505 acc_train: 0.768 loss_test: 0.605 acc_test: 0.667 AUC = 0.77 time: 0.499s\n",
      "Epoch: 136 loss_train: 0.509 acc_train: 0.766 loss_test: 0.604 acc_test: 0.694 AUC = 0.76 time: 0.499s\n",
      "Epoch: 137 loss_train: 0.504 acc_train: 0.777 loss_test: 0.606 acc_test: 0.686 AUC = 0.76 time: 0.499s\n",
      "Epoch: 138 loss_train: 0.504 acc_train: 0.778 loss_test: 0.600 acc_test: 0.698 AUC = 0.75 time: 0.499s\n",
      "Epoch: 139 loss_train: 0.492 acc_train: 0.772 loss_test: 0.613 acc_test: 0.675 AUC = 0.76 time: 0.499s\n",
      "Epoch: 140 loss_train: 0.498 acc_train: 0.772 loss_test: 0.622 acc_test: 0.667 AUC = 0.75 time: 0.499s\n",
      "Epoch: 141 loss_train: 0.495 acc_train: 0.780 loss_test: 0.587 acc_test: 0.700 AUC = 0.77 time: 0.499s\n",
      "Epoch: 142 loss_train: 0.490 acc_train: 0.782 loss_test: 0.596 acc_test: 0.698 AUC = 0.76 time: 0.499s\n",
      "Epoch: 143 loss_train: 0.504 acc_train: 0.786 loss_test: 0.589 acc_test: 0.675 AUC = 0.77 time: 0.499s\n",
      "Epoch: 144 loss_train: 0.502 acc_train: 0.776 loss_test: 0.574 acc_test: 0.698 AUC = 0.79 time: 0.500s\n",
      "Epoch: 145 loss_train: 0.497 acc_train: 0.778 loss_test: 0.613 acc_test: 0.695 AUC = 0.76 time: 0.500s\n",
      "Epoch: 146 loss_train: 0.502 acc_train: 0.768 loss_test: 0.590 acc_test: 0.692 AUC = 0.77 time: 0.499s\n",
      "Epoch: 147 loss_train: 0.498 acc_train: 0.775 loss_test: 0.604 acc_test: 0.695 AUC = 0.77 time: 0.500s\n",
      "Epoch: 148 loss_train: 0.505 acc_train: 0.772 loss_test: 0.607 acc_test: 0.682 AUC = 0.76 time: 0.499s\n",
      "Epoch: 149 loss_train: 0.492 acc_train: 0.779 loss_test: 0.612 acc_test: 0.689 AUC = 0.77 time: 0.500s\n",
      "Epoch: 150 loss_train: 0.489 acc_train: 0.787 loss_test: 0.599 acc_test: 0.689 AUC = 0.77 time: 0.500s\n",
      "Epoch: 151 loss_train: 0.489 acc_train: 0.777 loss_test: 0.581 acc_test: 0.701 AUC = 0.79 time: 0.500s\n",
      "Epoch: 152 loss_train: 0.495 acc_train: 0.764 loss_test: 0.580 acc_test: 0.707 AUC = 0.79 time: 0.500s\n",
      "Epoch: 153 loss_train: 0.482 acc_train: 0.782 loss_test: 0.576 acc_test: 0.688 AUC = 0.78 time: 0.500s\n",
      "Epoch: 154 loss_train: 0.501 acc_train: 0.777 loss_test: 0.600 acc_test: 0.694 AUC = 0.76 time: 0.500s\n",
      "Epoch: 155 loss_train: 0.497 acc_train: 0.780 loss_test: 0.589 acc_test: 0.689 AUC = 0.78 time: 0.500s\n",
      "Epoch: 156 loss_train: 0.482 acc_train: 0.782 loss_test: 0.593 acc_test: 0.700 AUC = 0.78 time: 0.500s\n",
      "Epoch: 157 loss_train: 0.498 acc_train: 0.772 loss_test: 0.591 acc_test: 0.697 AUC = 0.79 time: 0.500s\n",
      "Epoch: 158 loss_train: 0.491 acc_train: 0.773 loss_test: 0.580 acc_test: 0.708 AUC = 0.78 time: 0.500s\n",
      "Epoch: 159 loss_train: 0.479 acc_train: 0.791 loss_test: 0.551 acc_test: 0.705 AUC = 0.80 time: 0.500s\n",
      "Epoch: 160 loss_train: 0.489 acc_train: 0.773 loss_test: 0.586 acc_test: 0.689 AUC = 0.78 time: 0.500s\n",
      "Epoch: 161 loss_train: 0.486 acc_train: 0.779 loss_test: 0.585 acc_test: 0.700 AUC = 0.78 time: 0.500s\n",
      "Epoch: 162 loss_train: 0.492 acc_train: 0.779 loss_test: 0.568 acc_test: 0.692 AUC = 0.78 time: 0.500s\n",
      "Epoch: 163 loss_train: 0.486 acc_train: 0.780 loss_test: 0.592 acc_test: 0.682 AUC = 0.77 time: 0.500s\n",
      "Epoch: 164 loss_train: 0.486 acc_train: 0.788 loss_test: 0.582 acc_test: 0.707 AUC = 0.78 time: 0.502s\n",
      "Epoch: 165 loss_train: 0.491 acc_train: 0.784 loss_test: 0.592 acc_test: 0.686 AUC = 0.78 time: 0.500s\n",
      "Epoch: 166 loss_train: 0.481 acc_train: 0.787 loss_test: 0.589 acc_test: 0.695 AUC = 0.78 time: 0.500s\n",
      "Epoch: 167 loss_train: 0.489 acc_train: 0.784 loss_test: 0.550 acc_test: 0.726 AUC = 0.81 time: 0.500s\n",
      "Epoch: 168 loss_train: 0.490 acc_train: 0.789 loss_test: 0.575 acc_test: 0.697 AUC = 0.80 time: 0.499s\n",
      "Epoch: 169 loss_train: 0.493 acc_train: 0.777 loss_test: 0.592 acc_test: 0.695 AUC = 0.78 time: 0.499s\n",
      "Epoch: 170 loss_train: 0.499 acc_train: 0.788 loss_test: 0.593 acc_test: 0.707 AUC = 0.78 time: 0.501s\n",
      "Epoch: 171 loss_train: 0.486 acc_train: 0.780 loss_test: 0.568 acc_test: 0.710 AUC = 0.79 time: 0.499s\n",
      "Epoch: 172 loss_train: 0.479 acc_train: 0.785 loss_test: 0.629 acc_test: 0.666 AUC = 0.74 time: 0.499s\n",
      "Epoch: 173 loss_train: 0.484 acc_train: 0.785 loss_test: 0.586 acc_test: 0.689 AUC = 0.78 time: 0.499s\n",
      "Epoch: 174 loss_train: 0.492 acc_train: 0.789 loss_test: 0.562 acc_test: 0.710 AUC = 0.80 time: 0.499s\n",
      "Epoch: 175 loss_train: 0.475 acc_train: 0.781 loss_test: 0.571 acc_test: 0.707 AUC = 0.79 time: 0.499s\n",
      "Epoch: 176 loss_train: 0.482 acc_train: 0.782 loss_test: 0.589 acc_test: 0.711 AUC = 0.77 time: 0.502s\n",
      "Epoch: 177 loss_train: 0.489 acc_train: 0.772 loss_test: 0.585 acc_test: 0.695 AUC = 0.77 time: 0.499s\n",
      "Epoch: 178 loss_train: 0.473 acc_train: 0.792 loss_test: 0.574 acc_test: 0.707 AUC = 0.78 time: 0.499s\n",
      "Epoch: 179 loss_train: 0.471 acc_train: 0.790 loss_test: 0.582 acc_test: 0.688 AUC = 0.77 time: 0.499s\n",
      "Epoch: 180 loss_train: 0.473 acc_train: 0.800 loss_test: 0.579 acc_test: 0.710 AUC = 0.79 time: 0.499s\n",
      "Epoch: 181 loss_train: 0.477 acc_train: 0.782 loss_test: 0.593 acc_test: 0.700 AUC = 0.78 time: 0.499s\n",
      "Epoch: 182 loss_train: 0.487 acc_train: 0.791 loss_test: 0.591 acc_test: 0.703 AUC = 0.79 time: 0.499s\n",
      "Epoch: 183 loss_train: 0.484 acc_train: 0.782 loss_test: 0.550 acc_test: 0.722 AUC = 0.81 time: 0.498s\n",
      "Epoch: 184 loss_train: 0.479 acc_train: 0.790 loss_test: 0.571 acc_test: 0.711 AUC = 0.80 time: 0.499s\n",
      "Epoch: 185 loss_train: 0.472 acc_train: 0.785 loss_test: 0.587 acc_test: 0.698 AUC = 0.80 time: 0.499s\n",
      "Epoch: 186 loss_train: 0.474 acc_train: 0.785 loss_test: 0.570 acc_test: 0.711 AUC = 0.79 time: 0.499s\n",
      "Epoch: 187 loss_train: 0.481 acc_train: 0.785 loss_test: 0.571 acc_test: 0.701 AUC = 0.79 time: 0.499s\n",
      "Epoch: 188 loss_train: 0.472 acc_train: 0.794 loss_test: 0.573 acc_test: 0.716 AUC = 0.80 time: 0.498s\n",
      "Epoch: 189 loss_train: 0.469 acc_train: 0.789 loss_test: 0.565 acc_test: 0.711 AUC = 0.80 time: 0.498s\n",
      "Epoch: 190 loss_train: 0.474 acc_train: 0.785 loss_test: 0.555 acc_test: 0.701 AUC = 0.81 time: 0.498s\n",
      "Epoch: 191 loss_train: 0.478 acc_train: 0.784 loss_test: 0.574 acc_test: 0.710 AUC = 0.79 time: 0.498s\n",
      "Epoch: 192 loss_train: 0.474 acc_train: 0.795 loss_test: 0.575 acc_test: 0.695 AUC = 0.80 time: 0.498s\n",
      "Epoch: 193 loss_train: 0.467 acc_train: 0.787 loss_test: 0.573 acc_test: 0.692 AUC = 0.79 time: 0.498s\n",
      "Epoch: 194 loss_train: 0.463 acc_train: 0.793 loss_test: 0.567 acc_test: 0.716 AUC = 0.80 time: 0.498s\n",
      "Epoch: 195 loss_train: 0.465 acc_train: 0.796 loss_test: 0.579 acc_test: 0.705 AUC = 0.79 time: 0.498s\n",
      "Epoch: 196 loss_train: 0.477 acc_train: 0.787 loss_test: 0.551 acc_test: 0.704 AUC = 0.81 time: 0.498s\n",
      "Epoch: 197 loss_train: 0.466 acc_train: 0.793 loss_test: 0.574 acc_test: 0.713 AUC = 0.78 time: 0.498s\n",
      "Epoch: 198 loss_train: 0.466 acc_train: 0.801 loss_test: 0.596 acc_test: 0.694 AUC = 0.77 time: 0.499s\n",
      "Epoch: 199 loss_train: 0.473 acc_train: 0.783 loss_test: 0.564 acc_test: 0.701 AUC = 0.80 time: 0.498s\n",
      "Epoch: 200 loss_train: 0.484 acc_train: 0.785 loss_test: 0.594 acc_test: 0.689 AUC = 0.78 time: 0.498s\n",
      "Epoch: 201 loss_train: 0.465 acc_train: 0.798 loss_test: 0.583 acc_test: 0.703 AUC = 0.79 time: 0.498s\n",
      "Epoch: 202 loss_train: 0.482 acc_train: 0.785 loss_test: 0.568 acc_test: 0.708 AUC = 0.80 time: 0.499s\n",
      "Epoch: 203 loss_train: 0.468 acc_train: 0.792 loss_test: 0.558 acc_test: 0.703 AUC = 0.80 time: 0.498s\n",
      "Epoch: 204 loss_train: 0.471 acc_train: 0.789 loss_test: 0.551 acc_test: 0.705 AUC = 0.81 time: 0.498s\n",
      "Epoch: 205 loss_train: 0.481 acc_train: 0.785 loss_test: 0.591 acc_test: 0.704 AUC = 0.79 time: 0.498s\n",
      "Epoch: 206 loss_train: 0.472 acc_train: 0.790 loss_test: 0.591 acc_test: 0.692 AUC = 0.78 time: 0.498s\n",
      "Epoch: 207 loss_train: 0.479 acc_train: 0.790 loss_test: 0.562 acc_test: 0.713 AUC = 0.80 time: 0.498s\n",
      "Epoch: 208 loss_train: 0.464 acc_train: 0.796 loss_test: 0.560 acc_test: 0.716 AUC = 0.81 time: 0.499s\n",
      "Epoch: 209 loss_train: 0.470 acc_train: 0.804 loss_test: 0.598 acc_test: 0.704 AUC = 0.79 time: 0.499s\n",
      "Epoch: 210 loss_train: 0.467 acc_train: 0.789 loss_test: 0.542 acc_test: 0.720 AUC = 0.82 time: 0.498s\n",
      "Epoch: 211 loss_train: 0.472 acc_train: 0.784 loss_test: 0.555 acc_test: 0.708 AUC = 0.80 time: 0.498s\n",
      "Epoch: 212 loss_train: 0.461 acc_train: 0.799 loss_test: 0.562 acc_test: 0.707 AUC = 0.79 time: 0.498s\n",
      "Epoch: 213 loss_train: 0.466 acc_train: 0.791 loss_test: 0.576 acc_test: 0.694 AUC = 0.79 time: 0.498s\n",
      "Epoch: 214 loss_train: 0.463 acc_train: 0.790 loss_test: 0.575 acc_test: 0.707 AUC = 0.78 time: 0.498s\n",
      "Epoch: 215 loss_train: 0.472 acc_train: 0.787 loss_test: 0.547 acc_test: 0.719 AUC = 0.81 time: 0.498s\n",
      "Epoch: 216 loss_train: 0.460 acc_train: 0.789 loss_test: 0.534 acc_test: 0.723 AUC = 0.83 time: 0.498s\n",
      "Epoch: 217 loss_train: 0.464 acc_train: 0.793 loss_test: 0.542 acc_test: 0.725 AUC = 0.82 time: 0.498s\n",
      "Epoch: 218 loss_train: 0.461 acc_train: 0.786 loss_test: 0.551 acc_test: 0.722 AUC = 0.81 time: 0.498s\n",
      "Epoch: 219 loss_train: 0.465 acc_train: 0.792 loss_test: 0.557 acc_test: 0.719 AUC = 0.80 time: 0.499s\n",
      "Epoch: 220 loss_train: 0.470 acc_train: 0.796 loss_test: 0.562 acc_test: 0.708 AUC = 0.80 time: 0.498s\n",
      "Epoch: 221 loss_train: 0.469 acc_train: 0.787 loss_test: 0.541 acc_test: 0.722 AUC = 0.81 time: 0.498s\n",
      "Epoch: 222 loss_train: 0.455 acc_train: 0.796 loss_test: 0.546 acc_test: 0.707 AUC = 0.82 time: 0.498s\n",
      "Epoch: 223 loss_train: 0.461 acc_train: 0.799 loss_test: 0.558 acc_test: 0.689 AUC = 0.81 time: 0.498s\n",
      "Epoch: 224 loss_train: 0.459 acc_train: 0.801 loss_test: 0.557 acc_test: 0.707 AUC = 0.81 time: 0.498s\n",
      "Epoch: 225 loss_train: 0.462 acc_train: 0.793 loss_test: 0.567 acc_test: 0.682 AUC = 0.80 time: 0.498s\n",
      "Epoch: 226 loss_train: 0.451 acc_train: 0.796 loss_test: 0.551 acc_test: 0.717 AUC = 0.81 time: 0.497s\n",
      "Epoch: 227 loss_train: 0.467 acc_train: 0.792 loss_test: 0.540 acc_test: 0.720 AUC = 0.81 time: 0.498s\n",
      "Epoch: 228 loss_train: 0.462 acc_train: 0.796 loss_test: 0.528 acc_test: 0.730 AUC = 0.84 time: 0.498s\n",
      "Epoch: 229 loss_train: 0.457 acc_train: 0.799 loss_test: 0.563 acc_test: 0.714 AUC = 0.80 time: 0.498s\n",
      "Epoch: 230 loss_train: 0.469 acc_train: 0.794 loss_test: 0.577 acc_test: 0.707 AUC = 0.79 time: 0.498s\n",
      "Epoch: 231 loss_train: 0.461 acc_train: 0.796 loss_test: 0.562 acc_test: 0.719 AUC = 0.79 time: 0.498s\n",
      "Epoch: 232 loss_train: 0.474 acc_train: 0.787 loss_test: 0.542 acc_test: 0.720 AUC = 0.82 time: 0.498s\n",
      "Epoch: 233 loss_train: 0.463 acc_train: 0.789 loss_test: 0.542 acc_test: 0.719 AUC = 0.82 time: 0.498s\n",
      "Epoch: 234 loss_train: 0.472 acc_train: 0.792 loss_test: 0.561 acc_test: 0.710 AUC = 0.81 time: 0.498s\n",
      "Epoch: 235 loss_train: 0.450 acc_train: 0.797 loss_test: 0.543 acc_test: 0.719 AUC = 0.82 time: 0.498s\n",
      "Epoch: 236 loss_train: 0.461 acc_train: 0.796 loss_test: 0.522 acc_test: 0.728 AUC = 0.84 time: 0.498s\n",
      "Epoch: 237 loss_train: 0.456 acc_train: 0.797 loss_test: 0.550 acc_test: 0.720 AUC = 0.81 time: 0.498s\n",
      "Epoch: 238 loss_train: 0.457 acc_train: 0.801 loss_test: 0.520 acc_test: 0.730 AUC = 0.83 time: 0.498s\n",
      "Epoch: 239 loss_train: 0.457 acc_train: 0.804 loss_test: 0.572 acc_test: 0.703 AUC = 0.79 time: 0.498s\n",
      "Epoch: 240 loss_train: 0.456 acc_train: 0.798 loss_test: 0.574 acc_test: 0.717 AUC = 0.80 time: 0.498s\n",
      "Epoch: 241 loss_train: 0.441 acc_train: 0.800 loss_test: 0.551 acc_test: 0.713 AUC = 0.82 time: 0.498s\n",
      "Epoch: 242 loss_train: 0.463 acc_train: 0.793 loss_test: 0.528 acc_test: 0.741 AUC = 0.83 time: 0.501s\n",
      "Epoch: 243 loss_train: 0.447 acc_train: 0.809 loss_test: 0.573 acc_test: 0.725 AUC = 0.79 time: 0.498s\n",
      "Epoch: 244 loss_train: 0.463 acc_train: 0.787 loss_test: 0.568 acc_test: 0.711 AUC = 0.80 time: 0.498s\n",
      "Epoch: 245 loss_train: 0.466 acc_train: 0.801 loss_test: 0.606 acc_test: 0.710 AUC = 0.80 time: 0.498s\n",
      "Epoch: 246 loss_train: 0.466 acc_train: 0.801 loss_test: 0.567 acc_test: 0.716 AUC = 0.80 time: 0.549s\n",
      "Epoch: 247 loss_train: 0.457 acc_train: 0.798 loss_test: 0.553 acc_test: 0.710 AUC = 0.82 time: 0.498s\n",
      "Epoch: 248 loss_train: 0.477 acc_train: 0.792 loss_test: 0.584 acc_test: 0.705 AUC = 0.80 time: 0.500s\n",
      "Epoch: 249 loss_train: 0.460 acc_train: 0.797 loss_test: 0.580 acc_test: 0.700 AUC = 0.79 time: 0.498s\n",
      "Epoch: 250 loss_train: 0.453 acc_train: 0.797 loss_test: 0.570 acc_test: 0.707 AUC = 0.80 time: 0.498s\n",
      "Epoch: 251 loss_train: 0.446 acc_train: 0.802 loss_test: 0.519 acc_test: 0.738 AUC = 0.84 time: 0.498s\n",
      "Epoch: 252 loss_train: 0.461 acc_train: 0.807 loss_test: 0.515 acc_test: 0.741 AUC = 0.83 time: 0.498s\n",
      "Epoch: 253 loss_train: 0.434 acc_train: 0.806 loss_test: 0.572 acc_test: 0.710 AUC = 0.79 time: 0.498s\n",
      "Epoch: 254 loss_train: 0.446 acc_train: 0.806 loss_test: 0.536 acc_test: 0.728 AUC = 0.81 time: 0.498s\n",
      "Epoch: 255 loss_train: 0.450 acc_train: 0.803 loss_test: 0.555 acc_test: 0.713 AUC = 0.80 time: 0.498s\n",
      "Epoch: 256 loss_train: 0.444 acc_train: 0.809 loss_test: 0.535 acc_test: 0.722 AUC = 0.83 time: 0.498s\n",
      "Epoch: 257 loss_train: 0.474 acc_train: 0.804 loss_test: 0.544 acc_test: 0.726 AUC = 0.82 time: 0.498s\n",
      "Epoch: 258 loss_train: 0.464 acc_train: 0.791 loss_test: 0.527 acc_test: 0.722 AUC = 0.83 time: 0.498s\n",
      "Epoch: 259 loss_train: 0.465 acc_train: 0.794 loss_test: 0.543 acc_test: 0.717 AUC = 0.81 time: 0.498s\n",
      "Epoch: 260 loss_train: 0.456 acc_train: 0.796 loss_test: 0.570 acc_test: 0.704 AUC = 0.81 time: 0.498s\n",
      "Epoch: 261 loss_train: 0.447 acc_train: 0.799 loss_test: 0.583 acc_test: 0.697 AUC = 0.79 time: 0.499s\n",
      "Epoch: 262 loss_train: 0.452 acc_train: 0.801 loss_test: 0.545 acc_test: 0.726 AUC = 0.82 time: 0.498s\n",
      "Epoch: 263 loss_train: 0.446 acc_train: 0.800 loss_test: 0.557 acc_test: 0.710 AUC = 0.81 time: 0.499s\n",
      "Epoch: 264 loss_train: 0.456 acc_train: 0.808 loss_test: 0.593 acc_test: 0.710 AUC = 0.80 time: 0.499s\n",
      "Epoch: 265 loss_train: 0.434 acc_train: 0.806 loss_test: 0.534 acc_test: 0.728 AUC = 0.82 time: 0.498s\n",
      "Epoch: 266 loss_train: 0.449 acc_train: 0.802 loss_test: 0.540 acc_test: 0.719 AUC = 0.82 time: 0.499s\n",
      "Epoch: 267 loss_train: 0.440 acc_train: 0.815 loss_test: 0.528 acc_test: 0.738 AUC = 0.84 time: 0.501s\n",
      "Epoch: 268 loss_train: 0.457 acc_train: 0.802 loss_test: 0.556 acc_test: 0.716 AUC = 0.81 time: 0.499s\n",
      "Epoch: 269 loss_train: 0.461 acc_train: 0.796 loss_test: 0.576 acc_test: 0.703 AUC = 0.81 time: 0.499s\n",
      "Epoch: 270 loss_train: 0.444 acc_train: 0.803 loss_test: 0.541 acc_test: 0.726 AUC = 0.82 time: 0.499s\n",
      "Epoch: 271 loss_train: 0.454 acc_train: 0.793 loss_test: 0.537 acc_test: 0.713 AUC = 0.82 time: 0.499s\n",
      "Epoch: 272 loss_train: 0.439 acc_train: 0.804 loss_test: 0.511 acc_test: 0.736 AUC = 0.84 time: 0.499s\n",
      "Epoch: 273 loss_train: 0.464 acc_train: 0.804 loss_test: 0.597 acc_test: 0.705 AUC = 0.79 time: 0.499s\n",
      "Epoch: 274 loss_train: 0.452 acc_train: 0.805 loss_test: 0.545 acc_test: 0.714 AUC = 0.83 time: 0.499s\n",
      "Epoch: 275 loss_train: 0.438 acc_train: 0.805 loss_test: 0.556 acc_test: 0.735 AUC = 0.82 time: 0.500s\n",
      "Epoch: 276 loss_train: 0.433 acc_train: 0.809 loss_test: 0.532 acc_test: 0.708 AUC = 0.81 time: 0.499s\n",
      "Epoch: 277 loss_train: 0.459 acc_train: 0.805 loss_test: 0.509 acc_test: 0.742 AUC = 0.84 time: 0.499s\n",
      "Epoch: 278 loss_train: 0.454 acc_train: 0.799 loss_test: 0.515 acc_test: 0.735 AUC = 0.84 time: 0.499s\n",
      "Epoch: 279 loss_train: 0.441 acc_train: 0.803 loss_test: 0.556 acc_test: 0.713 AUC = 0.81 time: 0.499s\n",
      "Epoch: 280 loss_train: 0.439 acc_train: 0.799 loss_test: 0.542 acc_test: 0.719 AUC = 0.82 time: 0.499s\n",
      "Epoch: 281 loss_train: 0.458 acc_train: 0.797 loss_test: 0.552 acc_test: 0.730 AUC = 0.82 time: 0.499s\n",
      "Epoch: 282 loss_train: 0.433 acc_train: 0.815 loss_test: 0.540 acc_test: 0.722 AUC = 0.82 time: 0.499s\n",
      "Epoch: 283 loss_train: 0.445 acc_train: 0.811 loss_test: 0.525 acc_test: 0.720 AUC = 0.82 time: 0.499s\n",
      "Epoch: 284 loss_train: 0.450 acc_train: 0.808 loss_test: 0.519 acc_test: 0.729 AUC = 0.84 time: 0.499s\n",
      "Epoch: 285 loss_train: 0.446 acc_train: 0.808 loss_test: 0.535 acc_test: 0.735 AUC = 0.82 time: 0.499s\n",
      "Epoch: 286 loss_train: 0.446 acc_train: 0.806 loss_test: 0.536 acc_test: 0.719 AUC = 0.83 time: 0.499s\n",
      "Epoch: 287 loss_train: 0.453 acc_train: 0.796 loss_test: 0.534 acc_test: 0.726 AUC = 0.82 time: 0.499s\n",
      "Epoch: 288 loss_train: 0.454 acc_train: 0.797 loss_test: 0.538 acc_test: 0.720 AUC = 0.83 time: 0.499s\n",
      "Epoch: 289 loss_train: 0.442 acc_train: 0.809 loss_test: 0.571 acc_test: 0.714 AUC = 0.80 time: 0.499s\n",
      "Epoch: 290 loss_train: 0.433 acc_train: 0.811 loss_test: 0.548 acc_test: 0.714 AUC = 0.82 time: 0.499s\n",
      "Epoch: 291 loss_train: 0.455 acc_train: 0.792 loss_test: 0.546 acc_test: 0.726 AUC = 0.82 time: 0.499s\n",
      "Epoch: 292 loss_train: 0.425 acc_train: 0.813 loss_test: 0.503 acc_test: 0.726 AUC = 0.85 time: 0.499s\n",
      "Epoch: 293 loss_train: 0.436 acc_train: 0.811 loss_test: 0.520 acc_test: 0.728 AUC = 0.84 time: 0.499s\n",
      "Epoch: 294 loss_train: 0.442 acc_train: 0.797 loss_test: 0.535 acc_test: 0.717 AUC = 0.83 time: 0.499s\n",
      "Epoch: 295 loss_train: 0.447 acc_train: 0.795 loss_test: 0.526 acc_test: 0.722 AUC = 0.83 time: 0.499s\n",
      "Epoch: 296 loss_train: 0.452 acc_train: 0.801 loss_test: 0.518 acc_test: 0.732 AUC = 0.85 time: 0.499s\n",
      "Epoch: 297 loss_train: 0.435 acc_train: 0.809 loss_test: 0.520 acc_test: 0.739 AUC = 0.84 time: 0.499s\n",
      "Epoch: 298 loss_train: 0.437 acc_train: 0.804 loss_test: 0.529 acc_test: 0.744 AUC = 0.84 time: 0.499s\n",
      "Epoch: 299 loss_train: 0.446 acc_train: 0.799 loss_test: 0.571 acc_test: 0.698 AUC = 0.79 time: 0.499s\n",
      "Epoch: 300 loss_train: 0.430 acc_train: 0.815 loss_test: 0.551 acc_test: 0.708 AUC = 0.81 time: 0.500s\n",
      "Epoch: 301 loss_train: 0.436 acc_train: 0.811 loss_test: 0.557 acc_test: 0.725 AUC = 0.81 time: 0.499s\n",
      "Epoch: 302 loss_train: 0.440 acc_train: 0.812 loss_test: 0.563 acc_test: 0.719 AUC = 0.80 time: 0.500s\n",
      "Epoch: 303 loss_train: 0.453 acc_train: 0.799 loss_test: 0.517 acc_test: 0.735 AUC = 0.84 time: 0.499s\n",
      "Epoch: 304 loss_train: 0.430 acc_train: 0.811 loss_test: 0.515 acc_test: 0.729 AUC = 0.83 time: 0.499s\n",
      "Epoch: 305 loss_train: 0.436 acc_train: 0.807 loss_test: 0.516 acc_test: 0.728 AUC = 0.84 time: 0.499s\n",
      "Epoch: 306 loss_train: 0.445 acc_train: 0.808 loss_test: 0.514 acc_test: 0.741 AUC = 0.83 time: 0.499s\n",
      "Epoch: 307 loss_train: 0.440 acc_train: 0.811 loss_test: 0.531 acc_test: 0.739 AUC = 0.82 time: 0.499s\n",
      "Epoch: 308 loss_train: 0.428 acc_train: 0.812 loss_test: 0.533 acc_test: 0.726 AUC = 0.82 time: 0.499s\n",
      "Epoch: 309 loss_train: 0.433 acc_train: 0.814 loss_test: 0.535 acc_test: 0.733 AUC = 0.82 time: 0.499s\n",
      "Epoch: 310 loss_train: 0.445 acc_train: 0.801 loss_test: 0.532 acc_test: 0.725 AUC = 0.84 time: 0.499s\n",
      "Epoch: 311 loss_train: 0.442 acc_train: 0.802 loss_test: 0.519 acc_test: 0.742 AUC = 0.84 time: 0.499s\n",
      "Epoch: 312 loss_train: 0.446 acc_train: 0.801 loss_test: 0.552 acc_test: 0.703 AUC = 0.81 time: 0.499s\n",
      "Epoch: 313 loss_train: 0.446 acc_train: 0.812 loss_test: 0.509 acc_test: 0.733 AUC = 0.85 time: 0.499s\n",
      "Epoch: 314 loss_train: 0.446 acc_train: 0.801 loss_test: 0.537 acc_test: 0.732 AUC = 0.83 time: 0.499s\n",
      "Epoch: 315 loss_train: 0.427 acc_train: 0.813 loss_test: 0.507 acc_test: 0.736 AUC = 0.84 time: 0.499s\n",
      "Epoch: 316 loss_train: 0.452 acc_train: 0.796 loss_test: 0.512 acc_test: 0.726 AUC = 0.83 time: 0.499s\n",
      "Epoch: 317 loss_train: 0.448 acc_train: 0.804 loss_test: 0.554 acc_test: 0.719 AUC = 0.82 time: 0.499s\n",
      "Epoch: 318 loss_train: 0.437 acc_train: 0.805 loss_test: 0.500 acc_test: 0.738 AUC = 0.84 time: 0.499s\n",
      "Epoch: 319 loss_train: 0.431 acc_train: 0.797 loss_test: 0.539 acc_test: 0.720 AUC = 0.83 time: 0.499s\n",
      "Epoch: 320 loss_train: 0.464 acc_train: 0.798 loss_test: 0.532 acc_test: 0.708 AUC = 0.83 time: 0.499s\n",
      "Epoch: 321 loss_train: 0.433 acc_train: 0.809 loss_test: 0.537 acc_test: 0.738 AUC = 0.82 time: 0.499s\n",
      "Epoch: 322 loss_train: 0.439 acc_train: 0.805 loss_test: 0.514 acc_test: 0.725 AUC = 0.83 time: 0.499s\n",
      "Epoch: 323 loss_train: 0.435 acc_train: 0.813 loss_test: 0.521 acc_test: 0.736 AUC = 0.82 time: 0.499s\n",
      "Epoch: 324 loss_train: 0.427 acc_train: 0.820 loss_test: 0.535 acc_test: 0.735 AUC = 0.82 time: 0.499s\n",
      "Epoch: 325 loss_train: 0.451 acc_train: 0.800 loss_test: 0.503 acc_test: 0.748 AUC = 0.84 time: 0.499s\n",
      "Epoch: 326 loss_train: 0.454 acc_train: 0.794 loss_test: 0.519 acc_test: 0.713 AUC = 0.83 time: 0.499s\n",
      "Epoch: 327 loss_train: 0.438 acc_train: 0.806 loss_test: 0.509 acc_test: 0.729 AUC = 0.84 time: 0.499s\n",
      "Epoch: 328 loss_train: 0.441 acc_train: 0.801 loss_test: 0.534 acc_test: 0.730 AUC = 0.83 time: 0.499s\n",
      "Epoch: 329 loss_train: 0.439 acc_train: 0.807 loss_test: 0.517 acc_test: 0.753 AUC = 0.85 time: 0.498s\n",
      "Epoch: 330 loss_train: 0.435 acc_train: 0.821 loss_test: 0.493 acc_test: 0.738 AUC = 0.85 time: 0.499s\n",
      "Epoch: 331 loss_train: 0.429 acc_train: 0.813 loss_test: 0.494 acc_test: 0.753 AUC = 0.85 time: 0.499s\n",
      "Epoch: 332 loss_train: 0.446 acc_train: 0.806 loss_test: 0.544 acc_test: 0.726 AUC = 0.81 time: 0.499s\n",
      "Epoch: 333 loss_train: 0.427 acc_train: 0.815 loss_test: 0.526 acc_test: 0.717 AUC = 0.83 time: 0.499s\n",
      "Epoch: 334 loss_train: 0.454 acc_train: 0.801 loss_test: 0.537 acc_test: 0.722 AUC = 0.82 time: 0.498s\n",
      "Epoch: 335 loss_train: 0.435 acc_train: 0.809 loss_test: 0.541 acc_test: 0.723 AUC = 0.82 time: 0.498s\n",
      "Epoch: 336 loss_train: 0.441 acc_train: 0.810 loss_test: 0.519 acc_test: 0.739 AUC = 0.83 time: 0.500s\n",
      "Epoch: 337 loss_train: 0.430 acc_train: 0.801 loss_test: 0.520 acc_test: 0.732 AUC = 0.84 time: 0.499s\n",
      "Epoch: 338 loss_train: 0.443 acc_train: 0.797 loss_test: 0.511 acc_test: 0.742 AUC = 0.84 time: 0.498s\n",
      "Epoch: 339 loss_train: 0.439 acc_train: 0.806 loss_test: 0.548 acc_test: 0.710 AUC = 0.82 time: 0.499s\n",
      "Epoch: 340 loss_train: 0.445 acc_train: 0.815 loss_test: 0.521 acc_test: 0.735 AUC = 0.84 time: 0.499s\n",
      "Epoch: 341 loss_train: 0.427 acc_train: 0.814 loss_test: 0.541 acc_test: 0.728 AUC = 0.84 time: 0.499s\n",
      "Epoch: 342 loss_train: 0.418 acc_train: 0.815 loss_test: 0.549 acc_test: 0.719 AUC = 0.82 time: 0.499s\n",
      "Epoch: 343 loss_train: 0.438 acc_train: 0.803 loss_test: 0.524 acc_test: 0.742 AUC = 0.83 time: 0.499s\n",
      "Epoch: 344 loss_train: 0.429 acc_train: 0.811 loss_test: 0.492 acc_test: 0.757 AUC = 0.85 time: 0.499s\n",
      "Epoch: 345 loss_train: 0.426 acc_train: 0.801 loss_test: 0.520 acc_test: 0.728 AUC = 0.84 time: 0.498s\n",
      "Epoch: 346 loss_train: 0.440 acc_train: 0.809 loss_test: 0.525 acc_test: 0.723 AUC = 0.82 time: 0.498s\n",
      "Epoch: 347 loss_train: 0.420 acc_train: 0.813 loss_test: 0.555 acc_test: 0.728 AUC = 0.82 time: 0.499s\n",
      "Epoch: 348 loss_train: 0.431 acc_train: 0.816 loss_test: 0.525 acc_test: 0.725 AUC = 0.84 time: 0.498s\n",
      "Epoch: 349 loss_train: 0.444 acc_train: 0.809 loss_test: 0.513 acc_test: 0.744 AUC = 0.85 time: 0.499s\n",
      "Epoch: 350 loss_train: 0.427 acc_train: 0.811 loss_test: 0.511 acc_test: 0.735 AUC = 0.84 time: 0.498s\n",
      "Epoch: 351 loss_train: 0.434 acc_train: 0.811 loss_test: 0.541 acc_test: 0.723 AUC = 0.82 time: 0.498s\n",
      "Epoch: 352 loss_train: 0.437 acc_train: 0.808 loss_test: 0.558 acc_test: 0.736 AUC = 0.82 time: 0.499s\n",
      "Epoch: 353 loss_train: 0.416 acc_train: 0.815 loss_test: 0.531 acc_test: 0.735 AUC = 0.83 time: 0.499s\n",
      "Epoch: 354 loss_train: 0.429 acc_train: 0.811 loss_test: 0.503 acc_test: 0.738 AUC = 0.84 time: 0.499s\n",
      "Epoch: 355 loss_train: 0.432 acc_train: 0.813 loss_test: 0.511 acc_test: 0.720 AUC = 0.84 time: 0.499s\n",
      "Epoch: 356 loss_train: 0.421 acc_train: 0.813 loss_test: 0.501 acc_test: 0.742 AUC = 0.85 time: 0.499s\n",
      "Epoch: 357 loss_train: 0.432 acc_train: 0.807 loss_test: 0.551 acc_test: 0.714 AUC = 0.82 time: 0.499s\n",
      "Epoch: 358 loss_train: 0.425 acc_train: 0.825 loss_test: 0.540 acc_test: 0.739 AUC = 0.83 time: 0.499s\n",
      "Epoch: 359 loss_train: 0.430 acc_train: 0.813 loss_test: 0.532 acc_test: 0.704 AUC = 0.82 time: 0.498s\n",
      "Epoch: 360 loss_train: 0.421 acc_train: 0.826 loss_test: 0.480 acc_test: 0.754 AUC = 0.86 time: 0.498s\n",
      "Epoch: 361 loss_train: 0.435 acc_train: 0.806 loss_test: 0.494 acc_test: 0.748 AUC = 0.85 time: 0.498s\n",
      "Epoch: 362 loss_train: 0.439 acc_train: 0.797 loss_test: 0.518 acc_test: 0.732 AUC = 0.83 time: 0.498s\n",
      "Epoch: 363 loss_train: 0.425 acc_train: 0.808 loss_test: 0.524 acc_test: 0.735 AUC = 0.84 time: 0.498s\n",
      "Epoch: 364 loss_train: 0.433 acc_train: 0.815 loss_test: 0.520 acc_test: 0.744 AUC = 0.84 time: 0.498s\n",
      "Epoch: 365 loss_train: 0.430 acc_train: 0.807 loss_test: 0.529 acc_test: 0.741 AUC = 0.84 time: 0.498s\n",
      "Epoch: 366 loss_train: 0.426 acc_train: 0.817 loss_test: 0.532 acc_test: 0.728 AUC = 0.83 time: 0.498s\n",
      "Epoch: 367 loss_train: 0.413 acc_train: 0.833 loss_test: 0.556 acc_test: 0.720 AUC = 0.82 time: 0.498s\n",
      "Epoch: 368 loss_train: 0.423 acc_train: 0.808 loss_test: 0.538 acc_test: 0.725 AUC = 0.83 time: 0.498s\n",
      "Epoch: 369 loss_train: 0.427 acc_train: 0.816 loss_test: 0.496 acc_test: 0.742 AUC = 0.85 time: 0.498s\n",
      "Epoch: 370 loss_train: 0.421 acc_train: 0.819 loss_test: 0.534 acc_test: 0.719 AUC = 0.83 time: 0.498s\n",
      "Epoch: 371 loss_train: 0.418 acc_train: 0.816 loss_test: 0.547 acc_test: 0.725 AUC = 0.83 time: 0.498s\n",
      "Epoch: 372 loss_train: 0.427 acc_train: 0.820 loss_test: 0.558 acc_test: 0.729 AUC = 0.82 time: 0.498s\n",
      "Epoch: 373 loss_train: 0.424 acc_train: 0.804 loss_test: 0.515 acc_test: 0.742 AUC = 0.84 time: 0.498s\n",
      "Epoch: 374 loss_train: 0.427 acc_train: 0.824 loss_test: 0.521 acc_test: 0.739 AUC = 0.84 time: 0.499s\n",
      "Epoch: 375 loss_train: 0.425 acc_train: 0.815 loss_test: 0.536 acc_test: 0.733 AUC = 0.84 time: 0.498s\n",
      "Epoch: 376 loss_train: 0.431 acc_train: 0.816 loss_test: 0.522 acc_test: 0.741 AUC = 0.83 time: 0.498s\n",
      "Epoch: 377 loss_train: 0.441 acc_train: 0.809 loss_test: 0.546 acc_test: 0.736 AUC = 0.82 time: 0.498s\n",
      "Epoch: 378 loss_train: 0.399 acc_train: 0.830 loss_test: 0.497 acc_test: 0.736 AUC = 0.84 time: 0.498s\n",
      "Epoch: 379 loss_train: 0.429 acc_train: 0.815 loss_test: 0.550 acc_test: 0.744 AUC = 0.82 time: 0.554s\n",
      "Epoch: 380 loss_train: 0.437 acc_train: 0.808 loss_test: 0.517 acc_test: 0.739 AUC = 0.84 time: 0.498s\n",
      "Epoch: 381 loss_train: 0.424 acc_train: 0.810 loss_test: 0.497 acc_test: 0.750 AUC = 0.85 time: 0.498s\n",
      "Epoch: 382 loss_train: 0.421 acc_train: 0.816 loss_test: 0.555 acc_test: 0.722 AUC = 0.81 time: 0.498s\n",
      "Epoch: 383 loss_train: 0.431 acc_train: 0.811 loss_test: 0.491 acc_test: 0.736 AUC = 0.86 time: 0.498s\n",
      "Epoch: 384 loss_train: 0.431 acc_train: 0.815 loss_test: 0.517 acc_test: 0.738 AUC = 0.84 time: 0.498s\n",
      "Epoch: 385 loss_train: 0.430 acc_train: 0.814 loss_test: 0.536 acc_test: 0.739 AUC = 0.83 time: 0.498s\n",
      "Epoch: 386 loss_train: 0.434 acc_train: 0.823 loss_test: 0.525 acc_test: 0.736 AUC = 0.84 time: 0.499s\n",
      "Epoch: 387 loss_train: 0.441 acc_train: 0.812 loss_test: 0.520 acc_test: 0.728 AUC = 0.84 time: 0.498s\n",
      "Epoch: 388 loss_train: 0.412 acc_train: 0.823 loss_test: 0.524 acc_test: 0.744 AUC = 0.83 time: 0.498s\n",
      "Epoch: 389 loss_train: 0.425 acc_train: 0.819 loss_test: 0.528 acc_test: 0.725 AUC = 0.83 time: 0.498s\n",
      "Epoch: 390 loss_train: 0.437 acc_train: 0.808 loss_test: 0.513 acc_test: 0.730 AUC = 0.84 time: 0.499s\n",
      "Epoch: 391 loss_train: 0.408 acc_train: 0.831 loss_test: 0.501 acc_test: 0.745 AUC = 0.85 time: 0.498s\n",
      "Epoch: 392 loss_train: 0.422 acc_train: 0.813 loss_test: 0.550 acc_test: 0.722 AUC = 0.83 time: 0.498s\n",
      "Epoch: 393 loss_train: 0.430 acc_train: 0.824 loss_test: 0.510 acc_test: 0.747 AUC = 0.84 time: 0.498s\n",
      "Epoch: 394 loss_train: 0.442 acc_train: 0.794 loss_test: 0.522 acc_test: 0.738 AUC = 0.83 time: 0.498s\n",
      "Epoch: 395 loss_train: 0.410 acc_train: 0.816 loss_test: 0.506 acc_test: 0.736 AUC = 0.84 time: 0.498s\n",
      "Epoch: 396 loss_train: 0.429 acc_train: 0.810 loss_test: 0.491 acc_test: 0.761 AUC = 0.85 time: 0.498s\n",
      "Epoch: 397 loss_train: 0.431 acc_train: 0.818 loss_test: 0.498 acc_test: 0.741 AUC = 0.86 time: 0.498s\n",
      "Epoch: 398 loss_train: 0.420 acc_train: 0.824 loss_test: 0.506 acc_test: 0.741 AUC = 0.84 time: 0.501s\n",
      "Epoch: 399 loss_train: 0.415 acc_train: 0.822 loss_test: 0.502 acc_test: 0.723 AUC = 0.85 time: 0.498s\n",
      "Epoch: 400 loss_train: 0.423 acc_train: 0.816 loss_test: 0.505 acc_test: 0.745 AUC = 0.84 time: 0.498s\n",
      "Epoch: 401 loss_train: 0.433 acc_train: 0.808 loss_test: 0.522 acc_test: 0.723 AUC = 0.84 time: 0.498s\n",
      "Epoch: 402 loss_train: 0.428 acc_train: 0.821 loss_test: 0.483 acc_test: 0.748 AUC = 0.87 time: 0.498s\n",
      "Epoch: 403 loss_train: 0.424 acc_train: 0.811 loss_test: 0.503 acc_test: 0.735 AUC = 0.84 time: 0.498s\n",
      "Epoch: 404 loss_train: 0.425 acc_train: 0.809 loss_test: 0.524 acc_test: 0.741 AUC = 0.84 time: 0.498s\n",
      "Epoch: 405 loss_train: 0.429 acc_train: 0.814 loss_test: 0.512 acc_test: 0.758 AUC = 0.85 time: 0.498s\n",
      "Epoch: 406 loss_train: 0.419 acc_train: 0.816 loss_test: 0.520 acc_test: 0.741 AUC = 0.84 time: 0.498s\n",
      "Epoch: 407 loss_train: 0.424 acc_train: 0.817 loss_test: 0.544 acc_test: 0.722 AUC = 0.83 time: 0.499s\n",
      "Epoch: 408 loss_train: 0.432 acc_train: 0.807 loss_test: 0.538 acc_test: 0.738 AUC = 0.83 time: 0.498s\n",
      "Epoch: 409 loss_train: 0.424 acc_train: 0.815 loss_test: 0.545 acc_test: 0.739 AUC = 0.83 time: 0.498s\n",
      "Epoch: 410 loss_train: 0.413 acc_train: 0.818 loss_test: 0.518 acc_test: 0.733 AUC = 0.84 time: 0.498s\n",
      "Epoch: 411 loss_train: 0.417 acc_train: 0.813 loss_test: 0.527 acc_test: 0.728 AUC = 0.83 time: 0.498s\n",
      "Epoch: 412 loss_train: 0.420 acc_train: 0.818 loss_test: 0.491 acc_test: 0.754 AUC = 0.86 time: 0.498s\n",
      "Epoch: 413 loss_train: 0.422 acc_train: 0.820 loss_test: 0.475 acc_test: 0.761 AUC = 0.87 time: 0.498s\n",
      "Epoch: 414 loss_train: 0.421 acc_train: 0.817 loss_test: 0.524 acc_test: 0.729 AUC = 0.83 time: 0.499s\n",
      "Epoch: 415 loss_train: 0.416 acc_train: 0.812 loss_test: 0.499 acc_test: 0.739 AUC = 0.85 time: 0.502s\n",
      "Epoch: 416 loss_train: 0.431 acc_train: 0.808 loss_test: 0.503 acc_test: 0.733 AUC = 0.86 time: 0.499s\n",
      "Epoch: 417 loss_train: 0.431 acc_train: 0.809 loss_test: 0.544 acc_test: 0.723 AUC = 0.83 time: 0.499s\n",
      "Epoch: 418 loss_train: 0.419 acc_train: 0.809 loss_test: 0.483 acc_test: 0.742 AUC = 0.86 time: 0.501s\n",
      "Epoch: 419 loss_train: 0.416 acc_train: 0.816 loss_test: 0.518 acc_test: 0.744 AUC = 0.85 time: 0.499s\n",
      "Epoch: 420 loss_train: 0.431 acc_train: 0.818 loss_test: 0.501 acc_test: 0.744 AUC = 0.85 time: 0.499s\n",
      "Epoch: 421 loss_train: 0.411 acc_train: 0.821 loss_test: 0.528 acc_test: 0.741 AUC = 0.83 time: 0.499s\n",
      "Epoch: 422 loss_train: 0.422 acc_train: 0.816 loss_test: 0.502 acc_test: 0.741 AUC = 0.85 time: 0.499s\n",
      "Epoch: 423 loss_train: 0.433 acc_train: 0.816 loss_test: 0.531 acc_test: 0.739 AUC = 0.83 time: 0.498s\n",
      "Epoch: 424 loss_train: 0.431 acc_train: 0.813 loss_test: 0.539 acc_test: 0.735 AUC = 0.82 time: 0.499s\n",
      "Epoch: 425 loss_train: 0.433 acc_train: 0.816 loss_test: 0.496 acc_test: 0.753 AUC = 0.85 time: 0.499s\n",
      "Epoch: 426 loss_train: 0.407 acc_train: 0.818 loss_test: 0.535 acc_test: 0.717 AUC = 0.82 time: 0.499s\n",
      "Epoch: 427 loss_train: 0.409 acc_train: 0.821 loss_test: 0.504 acc_test: 0.760 AUC = 0.84 time: 0.499s\n",
      "Epoch: 428 loss_train: 0.436 acc_train: 0.807 loss_test: 0.481 acc_test: 0.747 AUC = 0.86 time: 0.499s\n",
      "Epoch: 429 loss_train: 0.412 acc_train: 0.818 loss_test: 0.467 acc_test: 0.767 AUC = 0.88 time: 0.499s\n",
      "Epoch: 430 loss_train: 0.425 acc_train: 0.810 loss_test: 0.494 acc_test: 0.750 AUC = 0.85 time: 0.499s\n",
      "Epoch: 431 loss_train: 0.421 acc_train: 0.806 loss_test: 0.533 acc_test: 0.730 AUC = 0.82 time: 0.499s\n",
      "Epoch: 432 loss_train: 0.416 acc_train: 0.816 loss_test: 0.505 acc_test: 0.733 AUC = 0.84 time: 0.499s\n",
      "Epoch: 433 loss_train: 0.404 acc_train: 0.825 loss_test: 0.508 acc_test: 0.741 AUC = 0.84 time: 0.499s\n",
      "Epoch: 434 loss_train: 0.418 acc_train: 0.818 loss_test: 0.520 acc_test: 0.730 AUC = 0.84 time: 0.499s\n",
      "Epoch: 435 loss_train: 0.419 acc_train: 0.810 loss_test: 0.501 acc_test: 0.747 AUC = 0.85 time: 0.499s\n",
      "Epoch: 436 loss_train: 0.427 acc_train: 0.822 loss_test: 0.516 acc_test: 0.747 AUC = 0.84 time: 0.499s\n",
      "Epoch: 437 loss_train: 0.421 acc_train: 0.814 loss_test: 0.531 acc_test: 0.747 AUC = 0.84 time: 0.498s\n",
      "Epoch: 438 loss_train: 0.425 acc_train: 0.815 loss_test: 0.536 acc_test: 0.735 AUC = 0.83 time: 0.498s\n",
      "Epoch: 439 loss_train: 0.415 acc_train: 0.820 loss_test: 0.506 acc_test: 0.748 AUC = 0.85 time: 0.499s\n",
      "Epoch: 440 loss_train: 0.418 acc_train: 0.808 loss_test: 0.524 acc_test: 0.735 AUC = 0.83 time: 0.499s\n",
      "Epoch: 441 loss_train: 0.411 acc_train: 0.820 loss_test: 0.551 acc_test: 0.720 AUC = 0.82 time: 0.499s\n",
      "Epoch: 442 loss_train: 0.420 acc_train: 0.815 loss_test: 0.509 acc_test: 0.735 AUC = 0.84 time: 0.498s\n",
      "Epoch: 443 loss_train: 0.423 acc_train: 0.816 loss_test: 0.519 acc_test: 0.747 AUC = 0.85 time: 0.499s\n",
      "Epoch: 444 loss_train: 0.414 acc_train: 0.819 loss_test: 0.502 acc_test: 0.722 AUC = 0.85 time: 0.499s\n",
      "Epoch: 445 loss_train: 0.413 acc_train: 0.823 loss_test: 0.528 acc_test: 0.720 AUC = 0.84 time: 0.498s\n",
      "Epoch: 446 loss_train: 0.418 acc_train: 0.819 loss_test: 0.492 acc_test: 0.748 AUC = 0.85 time: 0.499s\n",
      "Epoch: 447 loss_train: 0.418 acc_train: 0.811 loss_test: 0.515 acc_test: 0.745 AUC = 0.85 time: 0.498s\n",
      "Epoch: 448 loss_train: 0.404 acc_train: 0.826 loss_test: 0.515 acc_test: 0.730 AUC = 0.83 time: 0.498s\n",
      "Epoch: 449 loss_train: 0.405 acc_train: 0.831 loss_test: 0.532 acc_test: 0.719 AUC = 0.83 time: 0.498s\n",
      "Epoch: 450 loss_train: 0.418 acc_train: 0.814 loss_test: 0.537 acc_test: 0.716 AUC = 0.82 time: 0.498s\n",
      "Epoch: 451 loss_train: 0.418 acc_train: 0.818 loss_test: 0.491 acc_test: 0.739 AUC = 0.86 time: 0.499s\n",
      "Epoch: 452 loss_train: 0.434 acc_train: 0.806 loss_test: 0.486 acc_test: 0.750 AUC = 0.85 time: 0.498s\n",
      "Epoch: 453 loss_train: 0.434 acc_train: 0.818 loss_test: 0.496 acc_test: 0.751 AUC = 0.85 time: 0.499s\n",
      "Epoch: 454 loss_train: 0.414 acc_train: 0.811 loss_test: 0.505 acc_test: 0.747 AUC = 0.84 time: 0.498s\n",
      "Epoch: 455 loss_train: 0.411 acc_train: 0.822 loss_test: 0.504 acc_test: 0.739 AUC = 0.85 time: 0.500s\n",
      "Epoch: 456 loss_train: 0.421 acc_train: 0.811 loss_test: 0.491 acc_test: 0.741 AUC = 0.86 time: 0.500s\n",
      "Epoch: 457 loss_train: 0.426 acc_train: 0.824 loss_test: 0.516 acc_test: 0.742 AUC = 0.85 time: 0.499s\n",
      "Epoch: 458 loss_train: 0.420 acc_train: 0.818 loss_test: 0.515 acc_test: 0.738 AUC = 0.84 time: 0.498s\n",
      "Epoch: 459 loss_train: 0.414 acc_train: 0.824 loss_test: 0.504 acc_test: 0.744 AUC = 0.84 time: 0.498s\n",
      "Epoch: 460 loss_train: 0.412 acc_train: 0.824 loss_test: 0.502 acc_test: 0.732 AUC = 0.86 time: 0.499s\n",
      "Epoch: 461 loss_train: 0.413 acc_train: 0.817 loss_test: 0.533 acc_test: 0.733 AUC = 0.84 time: 0.498s\n",
      "Epoch: 462 loss_train: 0.428 acc_train: 0.819 loss_test: 0.501 acc_test: 0.730 AUC = 0.85 time: 0.498s\n",
      "Epoch: 463 loss_train: 0.408 acc_train: 0.821 loss_test: 0.488 acc_test: 0.747 AUC = 0.85 time: 0.499s\n",
      "Epoch: 464 loss_train: 0.426 acc_train: 0.816 loss_test: 0.495 acc_test: 0.742 AUC = 0.84 time: 0.498s\n",
      "Epoch: 465 loss_train: 0.402 acc_train: 0.827 loss_test: 0.480 acc_test: 0.754 AUC = 0.86 time: 0.498s\n",
      "Epoch: 466 loss_train: 0.421 acc_train: 0.820 loss_test: 0.486 acc_test: 0.757 AUC = 0.85 time: 0.500s\n",
      "Epoch: 467 loss_train: 0.416 acc_train: 0.824 loss_test: 0.506 acc_test: 0.747 AUC = 0.85 time: 0.498s\n",
      "Epoch: 468 loss_train: 0.421 acc_train: 0.822 loss_test: 0.496 acc_test: 0.753 AUC = 0.85 time: 0.498s\n",
      "Epoch: 469 loss_train: 0.403 acc_train: 0.820 loss_test: 0.488 acc_test: 0.750 AUC = 0.85 time: 0.498s\n",
      "Epoch: 470 loss_train: 0.417 acc_train: 0.815 loss_test: 0.496 acc_test: 0.751 AUC = 0.85 time: 0.499s\n",
      "Epoch: 471 loss_train: 0.417 acc_train: 0.817 loss_test: 0.504 acc_test: 0.756 AUC = 0.85 time: 0.498s\n",
      "Epoch: 472 loss_train: 0.421 acc_train: 0.819 loss_test: 0.501 acc_test: 0.730 AUC = 0.85 time: 0.499s\n",
      "Epoch: 473 loss_train: 0.408 acc_train: 0.824 loss_test: 0.489 acc_test: 0.757 AUC = 0.86 time: 0.498s\n",
      "Epoch: 474 loss_train: 0.407 acc_train: 0.818 loss_test: 0.503 acc_test: 0.742 AUC = 0.85 time: 0.499s\n",
      "Epoch: 475 loss_train: 0.400 acc_train: 0.825 loss_test: 0.503 acc_test: 0.730 AUC = 0.84 time: 0.498s\n",
      "Epoch: 476 loss_train: 0.412 acc_train: 0.823 loss_test: 0.509 acc_test: 0.739 AUC = 0.84 time: 0.498s\n",
      "Epoch: 477 loss_train: 0.412 acc_train: 0.821 loss_test: 0.516 acc_test: 0.745 AUC = 0.84 time: 0.499s\n",
      "Epoch: 478 loss_train: 0.408 acc_train: 0.823 loss_test: 0.501 acc_test: 0.756 AUC = 0.85 time: 0.498s\n",
      "Epoch: 479 loss_train: 0.400 acc_train: 0.826 loss_test: 0.517 acc_test: 0.729 AUC = 0.84 time: 0.498s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 304.9019s\n",
      "Loading 428th epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-a4368b6b2731>:179: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(sv_path +'{}.pkl'.format(best_epoch)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.4232 accuracy= 0.7835 AUC = 0.90 False Alarm Rate = 0.3078 F1 Score = 0.7189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-a4368b6b2731>:144: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  idx_train = torch.tensor(idx_train).cuda()\n",
      "<ipython-input-5-a4368b6b2731>:145: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  idx_test = torch.tensor(idx_test).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 0.797 acc_train: 0.303 loss_test: 0.738 acc_test: 0.461 AUC = 0.45 time: 0.498s\n",
      "Epoch: 002 loss_train: 0.787 acc_train: 0.313 loss_test: 0.730 acc_test: 0.461 AUC = 0.47 time: 0.498s\n",
      "Epoch: 003 loss_train: 0.786 acc_train: 0.299 loss_test: 0.739 acc_test: 0.457 AUC = 0.44 time: 0.499s\n",
      "Epoch: 004 loss_train: 0.766 acc_train: 0.329 loss_test: 0.720 acc_test: 0.474 AUC = 0.51 time: 0.500s\n",
      "Epoch: 005 loss_train: 0.760 acc_train: 0.336 loss_test: 0.734 acc_test: 0.451 AUC = 0.43 time: 0.498s\n",
      "Epoch: 006 loss_train: 0.755 acc_train: 0.338 loss_test: 0.727 acc_test: 0.458 AUC = 0.44 time: 0.499s\n",
      "Epoch: 007 loss_train: 0.751 acc_train: 0.332 loss_test: 0.719 acc_test: 0.462 AUC = 0.47 time: 0.498s\n",
      "Epoch: 008 loss_train: 0.747 acc_train: 0.362 loss_test: 0.729 acc_test: 0.434 AUC = 0.46 time: 0.498s\n",
      "Epoch: 009 loss_train: 0.741 acc_train: 0.363 loss_test: 0.710 acc_test: 0.471 AUC = 0.51 time: 0.498s\n",
      "Epoch: 010 loss_train: 0.739 acc_train: 0.396 loss_test: 0.716 acc_test: 0.476 AUC = 0.48 time: 0.498s\n",
      "Epoch: 011 loss_train: 0.727 acc_train: 0.408 loss_test: 0.726 acc_test: 0.451 AUC = 0.45 time: 0.498s\n",
      "Epoch: 012 loss_train: 0.720 acc_train: 0.435 loss_test: 0.716 acc_test: 0.493 AUC = 0.50 time: 0.555s\n",
      "Epoch: 013 loss_train: 0.713 acc_train: 0.460 loss_test: 0.707 acc_test: 0.486 AUC = 0.50 time: 0.498s\n",
      "Epoch: 014 loss_train: 0.705 acc_train: 0.497 loss_test: 0.717 acc_test: 0.499 AUC = 0.49 time: 0.499s\n",
      "Epoch: 015 loss_train: 0.699 acc_train: 0.526 loss_test: 0.698 acc_test: 0.518 AUC = 0.51 time: 0.498s\n",
      "Epoch: 016 loss_train: 0.699 acc_train: 0.538 loss_test: 0.719 acc_test: 0.504 AUC = 0.50 time: 0.499s\n",
      "Epoch: 017 loss_train: 0.684 acc_train: 0.546 loss_test: 0.701 acc_test: 0.530 AUC = 0.54 time: 0.498s\n",
      "Epoch: 018 loss_train: 0.688 acc_train: 0.555 loss_test: 0.708 acc_test: 0.520 AUC = 0.52 time: 0.498s\n",
      "Epoch: 019 loss_train: 0.673 acc_train: 0.594 loss_test: 0.715 acc_test: 0.529 AUC = 0.51 time: 0.498s\n",
      "Epoch: 020 loss_train: 0.676 acc_train: 0.581 loss_test: 0.728 acc_test: 0.523 AUC = 0.50 time: 0.499s\n",
      "Epoch: 021 loss_train: 0.679 acc_train: 0.615 loss_test: 0.724 acc_test: 0.526 AUC = 0.54 time: 0.498s\n",
      "Epoch: 022 loss_train: 0.659 acc_train: 0.619 loss_test: 0.725 acc_test: 0.545 AUC = 0.50 time: 0.499s\n",
      "Epoch: 023 loss_train: 0.661 acc_train: 0.635 loss_test: 0.697 acc_test: 0.546 AUC = 0.58 time: 0.499s\n",
      "Epoch: 024 loss_train: 0.663 acc_train: 0.633 loss_test: 0.706 acc_test: 0.545 AUC = 0.55 time: 0.499s\n",
      "Epoch: 025 loss_train: 0.659 acc_train: 0.656 loss_test: 0.723 acc_test: 0.535 AUC = 0.54 time: 0.499s\n",
      "Epoch: 026 loss_train: 0.650 acc_train: 0.650 loss_test: 0.702 acc_test: 0.588 AUC = 0.60 time: 0.499s\n",
      "Epoch: 027 loss_train: 0.639 acc_train: 0.669 loss_test: 0.745 acc_test: 0.558 AUC = 0.53 time: 0.498s\n",
      "Epoch: 028 loss_train: 0.638 acc_train: 0.676 loss_test: 0.729 acc_test: 0.543 AUC = 0.55 time: 0.498s\n",
      "Epoch: 029 loss_train: 0.636 acc_train: 0.669 loss_test: 0.704 acc_test: 0.541 AUC = 0.59 time: 0.499s\n",
      "Epoch: 030 loss_train: 0.634 acc_train: 0.685 loss_test: 0.708 acc_test: 0.563 AUC = 0.60 time: 0.499s\n",
      "Epoch: 031 loss_train: 0.626 acc_train: 0.684 loss_test: 0.715 acc_test: 0.568 AUC = 0.58 time: 0.499s\n",
      "Epoch: 032 loss_train: 0.617 acc_train: 0.704 loss_test: 0.706 acc_test: 0.561 AUC = 0.59 time: 0.500s\n",
      "Epoch: 033 loss_train: 0.620 acc_train: 0.694 loss_test: 0.725 acc_test: 0.560 AUC = 0.59 time: 0.500s\n",
      "Epoch: 034 loss_train: 0.615 acc_train: 0.693 loss_test: 0.700 acc_test: 0.567 AUC = 0.61 time: 0.499s\n",
      "Epoch: 035 loss_train: 0.601 acc_train: 0.711 loss_test: 0.704 acc_test: 0.580 AUC = 0.62 time: 0.498s\n",
      "Epoch: 036 loss_train: 0.613 acc_train: 0.707 loss_test: 0.714 acc_test: 0.568 AUC = 0.61 time: 0.500s\n",
      "Epoch: 037 loss_train: 0.603 acc_train: 0.715 loss_test: 0.675 acc_test: 0.607 AUC = 0.66 time: 0.499s\n",
      "Epoch: 038 loss_train: 0.592 acc_train: 0.710 loss_test: 0.682 acc_test: 0.599 AUC = 0.66 time: 0.499s\n",
      "Epoch: 039 loss_train: 0.589 acc_train: 0.722 loss_test: 0.689 acc_test: 0.608 AUC = 0.63 time: 0.499s\n",
      "Epoch: 040 loss_train: 0.599 acc_train: 0.708 loss_test: 0.684 acc_test: 0.611 AUC = 0.65 time: 0.499s\n",
      "Epoch: 041 loss_train: 0.574 acc_train: 0.744 loss_test: 0.662 acc_test: 0.623 AUC = 0.67 time: 0.499s\n",
      "Epoch: 042 loss_train: 0.570 acc_train: 0.734 loss_test: 0.666 acc_test: 0.636 AUC = 0.65 time: 0.499s\n",
      "Epoch: 043 loss_train: 0.587 acc_train: 0.729 loss_test: 0.696 acc_test: 0.605 AUC = 0.62 time: 0.499s\n",
      "Epoch: 044 loss_train: 0.581 acc_train: 0.738 loss_test: 0.661 acc_test: 0.622 AUC = 0.67 time: 0.498s\n",
      "Epoch: 045 loss_train: 0.574 acc_train: 0.726 loss_test: 0.691 acc_test: 0.633 AUC = 0.64 time: 0.499s\n",
      "Epoch: 046 loss_train: 0.572 acc_train: 0.745 loss_test: 0.685 acc_test: 0.623 AUC = 0.64 time: 0.499s\n",
      "Epoch: 047 loss_train: 0.572 acc_train: 0.744 loss_test: 0.656 acc_test: 0.620 AUC = 0.67 time: 0.498s\n",
      "Epoch: 048 loss_train: 0.561 acc_train: 0.738 loss_test: 0.658 acc_test: 0.630 AUC = 0.68 time: 0.498s\n",
      "Epoch: 049 loss_train: 0.565 acc_train: 0.745 loss_test: 0.645 acc_test: 0.654 AUC = 0.69 time: 0.499s\n",
      "Epoch: 050 loss_train: 0.582 acc_train: 0.744 loss_test: 0.647 acc_test: 0.661 AUC = 0.70 time: 0.499s\n",
      "Epoch: 051 loss_train: 0.553 acc_train: 0.751 loss_test: 0.656 acc_test: 0.682 AUC = 0.69 time: 0.499s\n",
      "Epoch: 052 loss_train: 0.567 acc_train: 0.739 loss_test: 0.661 acc_test: 0.648 AUC = 0.69 time: 0.499s\n",
      "Epoch: 053 loss_train: 0.563 acc_train: 0.741 loss_test: 0.675 acc_test: 0.632 AUC = 0.68 time: 0.499s\n",
      "Epoch: 054 loss_train: 0.543 acc_train: 0.754 loss_test: 0.674 acc_test: 0.657 AUC = 0.68 time: 0.499s\n",
      "Epoch: 055 loss_train: 0.559 acc_train: 0.753 loss_test: 0.669 acc_test: 0.648 AUC = 0.69 time: 0.499s\n",
      "Epoch: 056 loss_train: 0.562 acc_train: 0.757 loss_test: 0.657 acc_test: 0.657 AUC = 0.71 time: 0.498s\n",
      "Epoch: 057 loss_train: 0.543 acc_train: 0.755 loss_test: 0.676 acc_test: 0.636 AUC = 0.69 time: 0.498s\n",
      "Epoch: 058 loss_train: 0.561 acc_train: 0.750 loss_test: 0.659 acc_test: 0.649 AUC = 0.70 time: 0.499s\n",
      "Epoch: 059 loss_train: 0.564 acc_train: 0.753 loss_test: 0.683 acc_test: 0.664 AUC = 0.70 time: 0.499s\n",
      "Epoch: 060 loss_train: 0.555 acc_train: 0.751 loss_test: 0.654 acc_test: 0.663 AUC = 0.70 time: 0.499s\n",
      "Epoch: 061 loss_train: 0.563 acc_train: 0.743 loss_test: 0.686 acc_test: 0.649 AUC = 0.67 time: 0.499s\n",
      "Epoch: 062 loss_train: 0.556 acc_train: 0.755 loss_test: 0.634 acc_test: 0.664 AUC = 0.71 time: 0.499s\n",
      "Epoch: 063 loss_train: 0.537 acc_train: 0.762 loss_test: 0.645 acc_test: 0.663 AUC = 0.73 time: 0.499s\n",
      "Epoch: 064 loss_train: 0.551 acc_train: 0.753 loss_test: 0.669 acc_test: 0.647 AUC = 0.71 time: 0.499s\n",
      "Epoch: 065 loss_train: 0.526 acc_train: 0.764 loss_test: 0.680 acc_test: 0.663 AUC = 0.69 time: 0.498s\n",
      "Epoch: 066 loss_train: 0.534 acc_train: 0.758 loss_test: 0.669 acc_test: 0.666 AUC = 0.69 time: 0.499s\n",
      "Epoch: 067 loss_train: 0.537 acc_train: 0.765 loss_test: 0.668 acc_test: 0.654 AUC = 0.68 time: 0.498s\n",
      "Epoch: 068 loss_train: 0.538 acc_train: 0.761 loss_test: 0.673 acc_test: 0.644 AUC = 0.69 time: 0.499s\n",
      "Epoch: 069 loss_train: 0.540 acc_train: 0.761 loss_test: 0.646 acc_test: 0.669 AUC = 0.71 time: 0.498s\n",
      "Epoch: 070 loss_train: 0.542 acc_train: 0.748 loss_test: 0.654 acc_test: 0.649 AUC = 0.71 time: 0.499s\n",
      "Epoch: 071 loss_train: 0.543 acc_train: 0.760 loss_test: 0.665 acc_test: 0.657 AUC = 0.69 time: 0.498s\n",
      "Epoch: 072 loss_train: 0.534 acc_train: 0.767 loss_test: 0.675 acc_test: 0.652 AUC = 0.69 time: 0.499s\n",
      "Epoch: 073 loss_train: 0.540 acc_train: 0.759 loss_test: 0.642 acc_test: 0.667 AUC = 0.71 time: 0.498s\n",
      "Epoch: 074 loss_train: 0.535 acc_train: 0.768 loss_test: 0.626 acc_test: 0.675 AUC = 0.73 time: 0.498s\n",
      "Epoch: 075 loss_train: 0.541 acc_train: 0.761 loss_test: 0.647 acc_test: 0.655 AUC = 0.71 time: 0.499s\n",
      "Epoch: 076 loss_train: 0.530 acc_train: 0.757 loss_test: 0.645 acc_test: 0.666 AUC = 0.72 time: 0.498s\n",
      "Epoch: 077 loss_train: 0.544 acc_train: 0.760 loss_test: 0.645 acc_test: 0.673 AUC = 0.72 time: 0.499s\n",
      "Epoch: 078 loss_train: 0.536 acc_train: 0.767 loss_test: 0.650 acc_test: 0.667 AUC = 0.70 time: 0.498s\n",
      "Epoch: 079 loss_train: 0.528 acc_train: 0.760 loss_test: 0.635 acc_test: 0.669 AUC = 0.72 time: 0.498s\n",
      "Epoch: 080 loss_train: 0.539 acc_train: 0.759 loss_test: 0.606 acc_test: 0.673 AUC = 0.74 time: 0.498s\n",
      "Epoch: 081 loss_train: 0.520 acc_train: 0.775 loss_test: 0.628 acc_test: 0.669 AUC = 0.72 time: 0.498s\n",
      "Epoch: 082 loss_train: 0.531 acc_train: 0.773 loss_test: 0.633 acc_test: 0.677 AUC = 0.73 time: 0.499s\n",
      "Epoch: 083 loss_train: 0.533 acc_train: 0.766 loss_test: 0.661 acc_test: 0.663 AUC = 0.71 time: 0.498s\n",
      "Epoch: 084 loss_train: 0.533 acc_train: 0.760 loss_test: 0.627 acc_test: 0.670 AUC = 0.73 time: 0.499s\n",
      "Epoch: 085 loss_train: 0.526 acc_train: 0.768 loss_test: 0.635 acc_test: 0.660 AUC = 0.72 time: 0.498s\n",
      "Epoch: 086 loss_train: 0.517 acc_train: 0.766 loss_test: 0.608 acc_test: 0.673 AUC = 0.75 time: 0.499s\n",
      "Epoch: 087 loss_train: 0.518 acc_train: 0.765 loss_test: 0.635 acc_test: 0.658 AUC = 0.73 time: 0.499s\n",
      "Epoch: 088 loss_train: 0.533 acc_train: 0.761 loss_test: 0.640 acc_test: 0.660 AUC = 0.71 time: 0.498s\n",
      "Epoch: 089 loss_train: 0.525 acc_train: 0.763 loss_test: 0.652 acc_test: 0.658 AUC = 0.71 time: 0.498s\n",
      "Epoch: 090 loss_train: 0.518 acc_train: 0.767 loss_test: 0.668 acc_test: 0.666 AUC = 0.70 time: 0.499s\n",
      "Epoch: 091 loss_train: 0.537 acc_train: 0.756 loss_test: 0.642 acc_test: 0.664 AUC = 0.72 time: 0.499s\n",
      "Epoch: 092 loss_train: 0.533 acc_train: 0.777 loss_test: 0.652 acc_test: 0.682 AUC = 0.72 time: 0.499s\n",
      "Epoch: 093 loss_train: 0.509 acc_train: 0.769 loss_test: 0.642 acc_test: 0.667 AUC = 0.73 time: 0.498s\n",
      "Epoch: 094 loss_train: 0.538 acc_train: 0.756 loss_test: 0.670 acc_test: 0.655 AUC = 0.72 time: 0.498s\n",
      "Epoch: 095 loss_train: 0.507 acc_train: 0.770 loss_test: 0.579 acc_test: 0.698 AUC = 0.78 time: 0.498s\n",
      "Epoch: 096 loss_train: 0.512 acc_train: 0.777 loss_test: 0.609 acc_test: 0.682 AUC = 0.75 time: 0.498s\n",
      "Epoch: 097 loss_train: 0.545 acc_train: 0.750 loss_test: 0.626 acc_test: 0.680 AUC = 0.73 time: 0.498s\n",
      "Epoch: 098 loss_train: 0.519 acc_train: 0.775 loss_test: 0.606 acc_test: 0.669 AUC = 0.75 time: 0.498s\n",
      "Epoch: 099 loss_train: 0.515 acc_train: 0.765 loss_test: 0.606 acc_test: 0.667 AUC = 0.75 time: 0.498s\n",
      "Epoch: 100 loss_train: 0.518 acc_train: 0.767 loss_test: 0.627 acc_test: 0.676 AUC = 0.74 time: 0.498s\n",
      "Epoch: 101 loss_train: 0.508 acc_train: 0.769 loss_test: 0.600 acc_test: 0.698 AUC = 0.75 time: 0.499s\n",
      "Epoch: 102 loss_train: 0.524 acc_train: 0.767 loss_test: 0.690 acc_test: 0.661 AUC = 0.70 time: 0.498s\n",
      "Epoch: 103 loss_train: 0.515 acc_train: 0.771 loss_test: 0.638 acc_test: 0.669 AUC = 0.73 time: 0.498s\n",
      "Epoch: 104 loss_train: 0.504 acc_train: 0.784 loss_test: 0.619 acc_test: 0.683 AUC = 0.74 time: 0.499s\n",
      "Epoch: 105 loss_train: 0.514 acc_train: 0.763 loss_test: 0.606 acc_test: 0.686 AUC = 0.76 time: 0.499s\n",
      "Epoch: 106 loss_train: 0.513 acc_train: 0.770 loss_test: 0.651 acc_test: 0.657 AUC = 0.72 time: 0.498s\n",
      "Epoch: 107 loss_train: 0.512 acc_train: 0.776 loss_test: 0.637 acc_test: 0.675 AUC = 0.73 time: 0.498s\n",
      "Epoch: 108 loss_train: 0.516 acc_train: 0.780 loss_test: 0.618 acc_test: 0.664 AUC = 0.73 time: 0.498s\n",
      "Epoch: 109 loss_train: 0.527 acc_train: 0.757 loss_test: 0.599 acc_test: 0.689 AUC = 0.76 time: 0.499s\n",
      "Epoch: 110 loss_train: 0.510 acc_train: 0.774 loss_test: 0.621 acc_test: 0.677 AUC = 0.74 time: 0.498s\n",
      "Epoch: 111 loss_train: 0.514 acc_train: 0.770 loss_test: 0.641 acc_test: 0.661 AUC = 0.72 time: 0.500s\n",
      "Epoch: 112 loss_train: 0.522 acc_train: 0.756 loss_test: 0.599 acc_test: 0.697 AUC = 0.75 time: 0.498s\n",
      "Epoch: 113 loss_train: 0.524 acc_train: 0.771 loss_test: 0.624 acc_test: 0.682 AUC = 0.75 time: 0.498s\n",
      "Epoch: 114 loss_train: 0.512 acc_train: 0.770 loss_test: 0.640 acc_test: 0.677 AUC = 0.72 time: 0.498s\n",
      "Epoch: 115 loss_train: 0.524 acc_train: 0.767 loss_test: 0.638 acc_test: 0.667 AUC = 0.74 time: 0.498s\n",
      "Epoch: 116 loss_train: 0.505 acc_train: 0.768 loss_test: 0.584 acc_test: 0.694 AUC = 0.77 time: 0.498s\n",
      "Epoch: 117 loss_train: 0.519 acc_train: 0.780 loss_test: 0.603 acc_test: 0.682 AUC = 0.74 time: 0.498s\n",
      "Epoch: 118 loss_train: 0.506 acc_train: 0.770 loss_test: 0.624 acc_test: 0.673 AUC = 0.74 time: 0.499s\n",
      "Epoch: 119 loss_train: 0.511 acc_train: 0.774 loss_test: 0.595 acc_test: 0.694 AUC = 0.76 time: 0.498s\n",
      "Epoch: 120 loss_train: 0.511 acc_train: 0.776 loss_test: 0.608 acc_test: 0.670 AUC = 0.74 time: 0.499s\n",
      "Epoch: 121 loss_train: 0.508 acc_train: 0.776 loss_test: 0.637 acc_test: 0.670 AUC = 0.73 time: 0.499s\n",
      "Epoch: 122 loss_train: 0.517 acc_train: 0.774 loss_test: 0.617 acc_test: 0.695 AUC = 0.74 time: 0.499s\n",
      "Epoch: 123 loss_train: 0.511 acc_train: 0.774 loss_test: 0.616 acc_test: 0.688 AUC = 0.75 time: 0.498s\n",
      "Epoch: 124 loss_train: 0.506 acc_train: 0.770 loss_test: 0.623 acc_test: 0.667 AUC = 0.73 time: 0.498s\n",
      "Epoch: 125 loss_train: 0.506 acc_train: 0.772 loss_test: 0.632 acc_test: 0.672 AUC = 0.74 time: 0.498s\n",
      "Epoch: 126 loss_train: 0.502 acc_train: 0.772 loss_test: 0.608 acc_test: 0.682 AUC = 0.75 time: 0.498s\n",
      "Epoch: 127 loss_train: 0.526 acc_train: 0.774 loss_test: 0.622 acc_test: 0.688 AUC = 0.75 time: 0.498s\n",
      "Epoch: 128 loss_train: 0.504 acc_train: 0.775 loss_test: 0.588 acc_test: 0.695 AUC = 0.76 time: 0.499s\n",
      "Epoch: 129 loss_train: 0.511 acc_train: 0.763 loss_test: 0.628 acc_test: 0.688 AUC = 0.75 time: 0.499s\n",
      "Epoch: 130 loss_train: 0.498 acc_train: 0.786 loss_test: 0.602 acc_test: 0.682 AUC = 0.75 time: 0.498s\n",
      "Epoch: 131 loss_train: 0.498 acc_train: 0.783 loss_test: 0.629 acc_test: 0.670 AUC = 0.73 time: 0.498s\n",
      "Epoch: 132 loss_train: 0.518 acc_train: 0.772 loss_test: 0.629 acc_test: 0.683 AUC = 0.73 time: 0.498s\n",
      "Epoch: 133 loss_train: 0.507 acc_train: 0.774 loss_test: 0.603 acc_test: 0.680 AUC = 0.76 time: 0.498s\n",
      "Epoch: 134 loss_train: 0.519 acc_train: 0.774 loss_test: 0.604 acc_test: 0.675 AUC = 0.75 time: 0.498s\n",
      "Epoch: 135 loss_train: 0.521 acc_train: 0.773 loss_test: 0.621 acc_test: 0.688 AUC = 0.75 time: 0.498s\n",
      "Epoch: 136 loss_train: 0.502 acc_train: 0.768 loss_test: 0.601 acc_test: 0.692 AUC = 0.77 time: 0.498s\n",
      "Epoch: 137 loss_train: 0.515 acc_train: 0.770 loss_test: 0.664 acc_test: 0.675 AUC = 0.72 time: 0.498s\n",
      "Epoch: 138 loss_train: 0.498 acc_train: 0.779 loss_test: 0.596 acc_test: 0.700 AUC = 0.76 time: 0.498s\n",
      "Epoch: 139 loss_train: 0.497 acc_train: 0.779 loss_test: 0.624 acc_test: 0.677 AUC = 0.74 time: 0.498s\n",
      "Epoch: 140 loss_train: 0.496 acc_train: 0.782 loss_test: 0.609 acc_test: 0.686 AUC = 0.75 time: 0.499s\n",
      "Epoch: 141 loss_train: 0.513 acc_train: 0.771 loss_test: 0.590 acc_test: 0.700 AUC = 0.77 time: 0.498s\n",
      "Epoch: 142 loss_train: 0.505 acc_train: 0.772 loss_test: 0.607 acc_test: 0.683 AUC = 0.75 time: 0.498s\n",
      "Epoch: 143 loss_train: 0.494 acc_train: 0.780 loss_test: 0.616 acc_test: 0.688 AUC = 0.75 time: 0.498s\n",
      "Epoch: 144 loss_train: 0.512 acc_train: 0.777 loss_test: 0.610 acc_test: 0.686 AUC = 0.76 time: 0.498s\n",
      "Epoch: 145 loss_train: 0.501 acc_train: 0.771 loss_test: 0.609 acc_test: 0.680 AUC = 0.76 time: 0.498s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 91.5559s\n",
      "Loading 94th epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-a4368b6b2731>:179: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(sv_path +'{}.pkl'.format(best_epoch)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.5819 accuracy= 0.6878 AUC = 0.80 False Alarm Rate = 0.2975 F1 Score = 0.5891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-a4368b6b2731>:144: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  idx_train = torch.tensor(idx_train).cuda()\n",
      "<ipython-input-5-a4368b6b2731>:145: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  idx_test = torch.tensor(idx_test).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 0.785 acc_train: 0.311 loss_test: 0.733 acc_test: 0.457 AUC = 0.47 time: 0.498s\n",
      "Epoch: 002 loss_train: 0.781 acc_train: 0.318 loss_test: 0.724 acc_test: 0.468 AUC = 0.50 time: 0.498s\n",
      "Epoch: 003 loss_train: 0.767 acc_train: 0.326 loss_test: 0.724 acc_test: 0.486 AUC = 0.50 time: 0.501s\n",
      "Epoch: 004 loss_train: 0.769 acc_train: 0.319 loss_test: 0.733 acc_test: 0.452 AUC = 0.46 time: 0.498s\n",
      "Epoch: 005 loss_train: 0.765 acc_train: 0.315 loss_test: 0.733 acc_test: 0.442 AUC = 0.47 time: 0.498s\n",
      "Epoch: 006 loss_train: 0.764 acc_train: 0.323 loss_test: 0.717 acc_test: 0.470 AUC = 0.49 time: 0.498s\n",
      "Epoch: 007 loss_train: 0.758 acc_train: 0.322 loss_test: 0.715 acc_test: 0.490 AUC = 0.51 time: 0.499s\n",
      "Epoch: 008 loss_train: 0.754 acc_train: 0.322 loss_test: 0.716 acc_test: 0.467 AUC = 0.49 time: 0.498s\n",
      "Epoch: 009 loss_train: 0.749 acc_train: 0.336 loss_test: 0.719 acc_test: 0.446 AUC = 0.48 time: 0.498s\n",
      "Epoch: 010 loss_train: 0.744 acc_train: 0.344 loss_test: 0.717 acc_test: 0.449 AUC = 0.49 time: 0.498s\n",
      "Epoch: 011 loss_train: 0.748 acc_train: 0.322 loss_test: 0.715 acc_test: 0.458 AUC = 0.51 time: 0.498s\n",
      "Epoch: 012 loss_train: 0.741 acc_train: 0.352 loss_test: 0.711 acc_test: 0.461 AUC = 0.51 time: 0.498s\n",
      "Epoch: 013 loss_train: 0.736 acc_train: 0.346 loss_test: 0.710 acc_test: 0.476 AUC = 0.53 time: 0.498s\n",
      "Epoch: 014 loss_train: 0.730 acc_train: 0.366 loss_test: 0.706 acc_test: 0.490 AUC = 0.52 time: 0.498s\n",
      "Epoch: 015 loss_train: 0.727 acc_train: 0.376 loss_test: 0.712 acc_test: 0.477 AUC = 0.49 time: 0.498s\n",
      "Epoch: 016 loss_train: 0.731 acc_train: 0.366 loss_test: 0.705 acc_test: 0.498 AUC = 0.53 time: 0.498s\n",
      "Epoch: 017 loss_train: 0.727 acc_train: 0.387 loss_test: 0.712 acc_test: 0.473 AUC = 0.50 time: 0.498s\n",
      "Epoch: 018 loss_train: 0.725 acc_train: 0.411 loss_test: 0.706 acc_test: 0.502 AUC = 0.52 time: 0.498s\n",
      "Epoch: 019 loss_train: 0.715 acc_train: 0.418 loss_test: 0.710 acc_test: 0.473 AUC = 0.49 time: 0.499s\n",
      "Epoch: 020 loss_train: 0.718 acc_train: 0.413 loss_test: 0.701 acc_test: 0.502 AUC = 0.52 time: 0.499s\n",
      "Epoch: 021 loss_train: 0.713 acc_train: 0.436 loss_test: 0.705 acc_test: 0.502 AUC = 0.52 time: 0.499s\n",
      "Epoch: 022 loss_train: 0.710 acc_train: 0.464 loss_test: 0.712 acc_test: 0.524 AUC = 0.51 time: 0.499s\n",
      "Epoch: 023 loss_train: 0.701 acc_train: 0.483 loss_test: 0.713 acc_test: 0.518 AUC = 0.52 time: 0.498s\n",
      "Epoch: 024 loss_train: 0.695 acc_train: 0.512 loss_test: 0.706 acc_test: 0.527 AUC = 0.52 time: 0.498s\n",
      "Epoch: 025 loss_train: 0.696 acc_train: 0.513 loss_test: 0.693 acc_test: 0.539 AUC = 0.54 time: 0.498s\n",
      "Epoch: 026 loss_train: 0.687 acc_train: 0.546 loss_test: 0.693 acc_test: 0.536 AUC = 0.53 time: 0.499s\n",
      "Epoch: 027 loss_train: 0.686 acc_train: 0.550 loss_test: 0.695 acc_test: 0.555 AUC = 0.55 time: 0.499s\n",
      "Epoch: 028 loss_train: 0.675 acc_train: 0.584 loss_test: 0.707 acc_test: 0.510 AUC = 0.51 time: 0.499s\n",
      "Epoch: 029 loss_train: 0.677 acc_train: 0.599 loss_test: 0.698 acc_test: 0.548 AUC = 0.55 time: 0.498s\n",
      "Epoch: 030 loss_train: 0.670 acc_train: 0.594 loss_test: 0.706 acc_test: 0.521 AUC = 0.51 time: 0.548s\n",
      "Epoch: 031 loss_train: 0.661 acc_train: 0.626 loss_test: 0.708 acc_test: 0.535 AUC = 0.53 time: 0.499s\n",
      "Epoch: 032 loss_train: 0.660 acc_train: 0.631 loss_test: 0.700 acc_test: 0.552 AUC = 0.54 time: 0.499s\n",
      "Epoch: 033 loss_train: 0.657 acc_train: 0.650 loss_test: 0.723 acc_test: 0.536 AUC = 0.52 time: 0.498s\n",
      "Epoch: 034 loss_train: 0.653 acc_train: 0.637 loss_test: 0.686 acc_test: 0.567 AUC = 0.58 time: 0.499s\n",
      "Epoch: 035 loss_train: 0.654 acc_train: 0.659 loss_test: 0.747 acc_test: 0.558 AUC = 0.55 time: 0.499s\n",
      "Epoch: 036 loss_train: 0.648 acc_train: 0.649 loss_test: 0.721 acc_test: 0.539 AUC = 0.55 time: 0.498s\n",
      "Epoch: 037 loss_train: 0.641 acc_train: 0.672 loss_test: 0.704 acc_test: 0.563 AUC = 0.56 time: 0.498s\n",
      "Epoch: 038 loss_train: 0.637 acc_train: 0.668 loss_test: 0.717 acc_test: 0.551 AUC = 0.56 time: 0.498s\n",
      "Epoch: 039 loss_train: 0.633 acc_train: 0.676 loss_test: 0.696 acc_test: 0.561 AUC = 0.60 time: 0.498s\n",
      "Epoch: 040 loss_train: 0.633 acc_train: 0.679 loss_test: 0.699 acc_test: 0.533 AUC = 0.63 time: 0.499s\n",
      "Epoch: 041 loss_train: 0.628 acc_train: 0.690 loss_test: 0.709 acc_test: 0.567 AUC = 0.59 time: 0.499s\n",
      "Epoch: 042 loss_train: 0.623 acc_train: 0.678 loss_test: 0.705 acc_test: 0.561 AUC = 0.61 time: 0.499s\n",
      "Epoch: 043 loss_train: 0.618 acc_train: 0.686 loss_test: 0.702 acc_test: 0.546 AUC = 0.61 time: 0.498s\n",
      "Epoch: 044 loss_train: 0.612 acc_train: 0.698 loss_test: 0.683 acc_test: 0.561 AUC = 0.65 time: 0.498s\n",
      "Epoch: 045 loss_train: 0.620 acc_train: 0.698 loss_test: 0.687 acc_test: 0.591 AUC = 0.64 time: 0.498s\n",
      "Epoch: 046 loss_train: 0.602 acc_train: 0.703 loss_test: 0.701 acc_test: 0.568 AUC = 0.63 time: 0.499s\n",
      "Epoch: 047 loss_train: 0.602 acc_train: 0.708 loss_test: 0.681 acc_test: 0.598 AUC = 0.66 time: 0.499s\n",
      "Epoch: 048 loss_train: 0.598 acc_train: 0.705 loss_test: 0.685 acc_test: 0.598 AUC = 0.66 time: 0.499s\n",
      "Epoch: 049 loss_train: 0.586 acc_train: 0.717 loss_test: 0.671 acc_test: 0.604 AUC = 0.67 time: 0.499s\n",
      "Epoch: 050 loss_train: 0.587 acc_train: 0.712 loss_test: 0.681 acc_test: 0.607 AUC = 0.66 time: 0.499s\n",
      "Epoch: 051 loss_train: 0.591 acc_train: 0.718 loss_test: 0.682 acc_test: 0.594 AUC = 0.65 time: 0.499s\n",
      "Epoch: 052 loss_train: 0.589 acc_train: 0.724 loss_test: 0.675 acc_test: 0.599 AUC = 0.68 time: 0.498s\n",
      "Epoch: 053 loss_train: 0.580 acc_train: 0.727 loss_test: 0.677 acc_test: 0.617 AUC = 0.67 time: 0.498s\n",
      "Epoch: 054 loss_train: 0.571 acc_train: 0.732 loss_test: 0.645 acc_test: 0.632 AUC = 0.71 time: 0.499s\n",
      "Epoch: 055 loss_train: 0.569 acc_train: 0.731 loss_test: 0.669 acc_test: 0.636 AUC = 0.67 time: 0.499s\n",
      "Epoch: 056 loss_train: 0.570 acc_train: 0.731 loss_test: 0.661 acc_test: 0.627 AUC = 0.68 time: 0.499s\n",
      "Epoch: 057 loss_train: 0.557 acc_train: 0.741 loss_test: 0.642 acc_test: 0.635 AUC = 0.71 time: 0.498s\n",
      "Epoch: 058 loss_train: 0.566 acc_train: 0.736 loss_test: 0.643 acc_test: 0.649 AUC = 0.70 time: 0.499s\n",
      "Epoch: 059 loss_train: 0.561 acc_train: 0.739 loss_test: 0.651 acc_test: 0.655 AUC = 0.70 time: 0.498s\n",
      "Epoch: 060 loss_train: 0.565 acc_train: 0.747 loss_test: 0.670 acc_test: 0.642 AUC = 0.67 time: 0.499s\n",
      "Epoch: 061 loss_train: 0.563 acc_train: 0.738 loss_test: 0.652 acc_test: 0.645 AUC = 0.69 time: 0.498s\n",
      "Epoch: 062 loss_train: 0.543 acc_train: 0.756 loss_test: 0.686 acc_test: 0.626 AUC = 0.65 time: 0.498s\n",
      "Epoch: 063 loss_train: 0.556 acc_train: 0.749 loss_test: 0.690 acc_test: 0.638 AUC = 0.66 time: 0.498s\n",
      "Epoch: 064 loss_train: 0.560 acc_train: 0.746 loss_test: 0.651 acc_test: 0.645 AUC = 0.71 time: 0.499s\n",
      "Epoch: 065 loss_train: 0.560 acc_train: 0.755 loss_test: 0.655 acc_test: 0.663 AUC = 0.70 time: 0.499s\n",
      "Epoch: 066 loss_train: 0.552 acc_train: 0.746 loss_test: 0.655 acc_test: 0.639 AUC = 0.70 time: 0.499s\n",
      "Epoch: 067 loss_train: 0.545 acc_train: 0.755 loss_test: 0.639 acc_test: 0.666 AUC = 0.72 time: 0.498s\n",
      "Epoch: 068 loss_train: 0.550 acc_train: 0.754 loss_test: 0.656 acc_test: 0.654 AUC = 0.70 time: 0.498s\n",
      "Epoch: 069 loss_train: 0.550 acc_train: 0.748 loss_test: 0.694 acc_test: 0.630 AUC = 0.67 time: 0.498s\n",
      "Epoch: 070 loss_train: 0.552 acc_train: 0.756 loss_test: 0.663 acc_test: 0.654 AUC = 0.70 time: 0.498s\n",
      "Epoch: 071 loss_train: 0.546 acc_train: 0.745 loss_test: 0.660 acc_test: 0.651 AUC = 0.70 time: 0.498s\n",
      "Epoch: 072 loss_train: 0.549 acc_train: 0.762 loss_test: 0.648 acc_test: 0.655 AUC = 0.72 time: 0.498s\n",
      "Epoch: 073 loss_train: 0.549 acc_train: 0.752 loss_test: 0.635 acc_test: 0.680 AUC = 0.72 time: 0.498s\n",
      "Epoch: 074 loss_train: 0.545 acc_train: 0.760 loss_test: 0.668 acc_test: 0.645 AUC = 0.70 time: 0.498s\n",
      "Epoch: 075 loss_train: 0.535 acc_train: 0.755 loss_test: 0.657 acc_test: 0.664 AUC = 0.72 time: 0.498s\n",
      "Epoch: 076 loss_train: 0.537 acc_train: 0.763 loss_test: 0.652 acc_test: 0.669 AUC = 0.72 time: 0.498s\n",
      "Epoch: 077 loss_train: 0.549 acc_train: 0.758 loss_test: 0.646 acc_test: 0.658 AUC = 0.71 time: 0.499s\n",
      "Epoch: 078 loss_train: 0.531 acc_train: 0.776 loss_test: 0.663 acc_test: 0.644 AUC = 0.71 time: 0.499s\n",
      "Epoch: 079 loss_train: 0.528 acc_train: 0.757 loss_test: 0.659 acc_test: 0.658 AUC = 0.71 time: 0.499s\n",
      "Epoch: 080 loss_train: 0.538 acc_train: 0.761 loss_test: 0.657 acc_test: 0.658 AUC = 0.70 time: 0.549s\n",
      "Epoch: 081 loss_train: 0.544 acc_train: 0.763 loss_test: 0.631 acc_test: 0.675 AUC = 0.73 time: 0.499s\n",
      "Epoch: 082 loss_train: 0.531 acc_train: 0.755 loss_test: 0.647 acc_test: 0.677 AUC = 0.71 time: 0.498s\n",
      "Epoch: 083 loss_train: 0.531 acc_train: 0.770 loss_test: 0.643 acc_test: 0.664 AUC = 0.72 time: 0.498s\n",
      "Epoch: 084 loss_train: 0.526 acc_train: 0.755 loss_test: 0.649 acc_test: 0.645 AUC = 0.71 time: 0.499s\n",
      "Epoch: 085 loss_train: 0.543 acc_train: 0.758 loss_test: 0.658 acc_test: 0.658 AUC = 0.70 time: 0.498s\n",
      "Epoch: 086 loss_train: 0.526 acc_train: 0.763 loss_test: 0.610 acc_test: 0.682 AUC = 0.75 time: 0.498s\n",
      "Epoch: 087 loss_train: 0.529 acc_train: 0.758 loss_test: 0.646 acc_test: 0.660 AUC = 0.72 time: 0.498s\n",
      "Epoch: 088 loss_train: 0.520 acc_train: 0.754 loss_test: 0.620 acc_test: 0.682 AUC = 0.74 time: 0.498s\n",
      "Epoch: 089 loss_train: 0.531 acc_train: 0.762 loss_test: 0.616 acc_test: 0.691 AUC = 0.74 time: 0.499s\n",
      "Epoch: 090 loss_train: 0.528 acc_train: 0.772 loss_test: 0.645 acc_test: 0.660 AUC = 0.71 time: 0.499s\n",
      "Epoch: 091 loss_train: 0.529 acc_train: 0.771 loss_test: 0.617 acc_test: 0.670 AUC = 0.75 time: 0.498s\n",
      "Epoch: 092 loss_train: 0.538 acc_train: 0.758 loss_test: 0.632 acc_test: 0.670 AUC = 0.73 time: 0.499s\n",
      "Epoch: 093 loss_train: 0.528 acc_train: 0.767 loss_test: 0.637 acc_test: 0.667 AUC = 0.73 time: 0.498s\n",
      "Epoch: 094 loss_train: 0.520 acc_train: 0.768 loss_test: 0.645 acc_test: 0.660 AUC = 0.73 time: 0.499s\n",
      "Epoch: 095 loss_train: 0.522 acc_train: 0.760 loss_test: 0.643 acc_test: 0.691 AUC = 0.72 time: 0.498s\n",
      "Epoch: 096 loss_train: 0.523 acc_train: 0.759 loss_test: 0.620 acc_test: 0.670 AUC = 0.74 time: 0.498s\n",
      "Epoch: 097 loss_train: 0.520 acc_train: 0.769 loss_test: 0.633 acc_test: 0.676 AUC = 0.73 time: 0.499s\n",
      "Epoch: 098 loss_train: 0.509 acc_train: 0.764 loss_test: 0.620 acc_test: 0.670 AUC = 0.73 time: 0.499s\n",
      "Epoch: 099 loss_train: 0.520 acc_train: 0.760 loss_test: 0.602 acc_test: 0.676 AUC = 0.76 time: 0.499s\n",
      "Epoch: 100 loss_train: 0.522 acc_train: 0.751 loss_test: 0.626 acc_test: 0.664 AUC = 0.74 time: 0.499s\n",
      "Epoch: 101 loss_train: 0.525 acc_train: 0.751 loss_test: 0.614 acc_test: 0.679 AUC = 0.75 time: 0.499s\n",
      "Epoch: 102 loss_train: 0.511 acc_train: 0.767 loss_test: 0.637 acc_test: 0.649 AUC = 0.73 time: 0.498s\n",
      "Epoch: 103 loss_train: 0.521 acc_train: 0.763 loss_test: 0.625 acc_test: 0.669 AUC = 0.74 time: 0.499s\n",
      "Epoch: 104 loss_train: 0.528 acc_train: 0.755 loss_test: 0.636 acc_test: 0.667 AUC = 0.73 time: 0.499s\n",
      "Epoch: 105 loss_train: 0.522 acc_train: 0.775 loss_test: 0.604 acc_test: 0.667 AUC = 0.76 time: 0.498s\n",
      "Epoch: 106 loss_train: 0.517 acc_train: 0.762 loss_test: 0.612 acc_test: 0.667 AUC = 0.75 time: 0.499s\n",
      "Epoch: 107 loss_train: 0.509 acc_train: 0.768 loss_test: 0.606 acc_test: 0.677 AUC = 0.75 time: 0.499s\n",
      "Epoch: 108 loss_train: 0.513 acc_train: 0.765 loss_test: 0.600 acc_test: 0.679 AUC = 0.76 time: 0.499s\n",
      "Epoch: 109 loss_train: 0.524 acc_train: 0.764 loss_test: 0.599 acc_test: 0.689 AUC = 0.75 time: 0.498s\n",
      "Epoch: 110 loss_train: 0.515 acc_train: 0.779 loss_test: 0.628 acc_test: 0.664 AUC = 0.74 time: 0.498s\n",
      "Epoch: 111 loss_train: 0.507 acc_train: 0.777 loss_test: 0.621 acc_test: 0.670 AUC = 0.74 time: 0.498s\n",
      "Epoch: 112 loss_train: 0.510 acc_train: 0.773 loss_test: 0.616 acc_test: 0.676 AUC = 0.75 time: 0.499s\n",
      "Epoch: 113 loss_train: 0.519 acc_train: 0.772 loss_test: 0.602 acc_test: 0.692 AUC = 0.76 time: 0.498s\n",
      "Epoch: 114 loss_train: 0.509 acc_train: 0.770 loss_test: 0.622 acc_test: 0.692 AUC = 0.75 time: 0.498s\n",
      "Epoch: 115 loss_train: 0.503 acc_train: 0.779 loss_test: 0.608 acc_test: 0.685 AUC = 0.75 time: 0.498s\n",
      "Epoch: 116 loss_train: 0.521 acc_train: 0.763 loss_test: 0.606 acc_test: 0.711 AUC = 0.75 time: 0.498s\n",
      "Epoch: 117 loss_train: 0.516 acc_train: 0.766 loss_test: 0.627 acc_test: 0.689 AUC = 0.76 time: 0.499s\n",
      "Epoch: 118 loss_train: 0.508 acc_train: 0.759 loss_test: 0.599 acc_test: 0.685 AUC = 0.76 time: 0.499s\n",
      "Epoch: 119 loss_train: 0.509 acc_train: 0.767 loss_test: 0.603 acc_test: 0.673 AUC = 0.76 time: 0.499s\n",
      "Epoch: 120 loss_train: 0.514 acc_train: 0.768 loss_test: 0.604 acc_test: 0.686 AUC = 0.76 time: 0.499s\n",
      "Epoch: 121 loss_train: 0.504 acc_train: 0.760 loss_test: 0.619 acc_test: 0.688 AUC = 0.75 time: 0.498s\n",
      "Epoch: 122 loss_train: 0.517 acc_train: 0.762 loss_test: 0.615 acc_test: 0.689 AUC = 0.75 time: 0.498s\n",
      "Epoch: 123 loss_train: 0.503 acc_train: 0.774 loss_test: 0.619 acc_test: 0.689 AUC = 0.75 time: 0.499s\n",
      "Epoch: 124 loss_train: 0.498 acc_train: 0.777 loss_test: 0.610 acc_test: 0.672 AUC = 0.75 time: 0.499s\n",
      "Epoch: 125 loss_train: 0.503 acc_train: 0.773 loss_test: 0.622 acc_test: 0.675 AUC = 0.75 time: 0.499s\n",
      "Epoch: 126 loss_train: 0.505 acc_train: 0.785 loss_test: 0.579 acc_test: 0.692 AUC = 0.78 time: 0.499s\n",
      "Epoch: 127 loss_train: 0.508 acc_train: 0.762 loss_test: 0.601 acc_test: 0.694 AUC = 0.76 time: 0.499s\n",
      "Epoch: 128 loss_train: 0.503 acc_train: 0.772 loss_test: 0.599 acc_test: 0.688 AUC = 0.76 time: 0.499s\n",
      "Epoch: 129 loss_train: 0.514 acc_train: 0.765 loss_test: 0.589 acc_test: 0.694 AUC = 0.77 time: 0.499s\n",
      "Epoch: 130 loss_train: 0.517 acc_train: 0.765 loss_test: 0.580 acc_test: 0.686 AUC = 0.78 time: 0.499s\n",
      "Epoch: 131 loss_train: 0.501 acc_train: 0.771 loss_test: 0.589 acc_test: 0.685 AUC = 0.77 time: 0.499s\n",
      "Epoch: 132 loss_train: 0.509 acc_train: 0.772 loss_test: 0.590 acc_test: 0.691 AUC = 0.77 time: 0.499s\n",
      "Epoch: 133 loss_train: 0.512 acc_train: 0.763 loss_test: 0.621 acc_test: 0.686 AUC = 0.75 time: 0.499s\n",
      "Epoch: 134 loss_train: 0.501 acc_train: 0.768 loss_test: 0.600 acc_test: 0.685 AUC = 0.77 time: 0.498s\n",
      "Epoch: 135 loss_train: 0.497 acc_train: 0.779 loss_test: 0.602 acc_test: 0.675 AUC = 0.76 time: 0.499s\n",
      "Epoch: 136 loss_train: 0.490 acc_train: 0.781 loss_test: 0.599 acc_test: 0.695 AUC = 0.77 time: 0.499s\n",
      "Epoch: 137 loss_train: 0.518 acc_train: 0.760 loss_test: 0.598 acc_test: 0.689 AUC = 0.77 time: 0.499s\n",
      "Epoch: 138 loss_train: 0.500 acc_train: 0.767 loss_test: 0.590 acc_test: 0.701 AUC = 0.79 time: 0.499s\n",
      "Epoch: 139 loss_train: 0.505 acc_train: 0.767 loss_test: 0.612 acc_test: 0.669 AUC = 0.76 time: 0.499s\n",
      "Epoch: 140 loss_train: 0.496 acc_train: 0.772 loss_test: 0.577 acc_test: 0.700 AUC = 0.78 time: 0.499s\n",
      "Epoch: 141 loss_train: 0.495 acc_train: 0.771 loss_test: 0.608 acc_test: 0.677 AUC = 0.76 time: 0.499s\n",
      "Epoch: 142 loss_train: 0.486 acc_train: 0.782 loss_test: 0.600 acc_test: 0.697 AUC = 0.76 time: 0.499s\n",
      "Epoch: 143 loss_train: 0.495 acc_train: 0.775 loss_test: 0.593 acc_test: 0.701 AUC = 0.77 time: 0.498s\n",
      "Epoch: 144 loss_train: 0.492 acc_train: 0.782 loss_test: 0.561 acc_test: 0.705 AUC = 0.79 time: 0.499s\n",
      "Epoch: 145 loss_train: 0.503 acc_train: 0.770 loss_test: 0.590 acc_test: 0.685 AUC = 0.77 time: 0.498s\n",
      "Epoch: 146 loss_train: 0.496 acc_train: 0.777 loss_test: 0.592 acc_test: 0.675 AUC = 0.77 time: 0.498s\n",
      "Epoch: 147 loss_train: 0.493 acc_train: 0.783 loss_test: 0.603 acc_test: 0.689 AUC = 0.76 time: 0.499s\n",
      "Epoch: 148 loss_train: 0.494 acc_train: 0.772 loss_test: 0.610 acc_test: 0.686 AUC = 0.76 time: 0.499s\n",
      "Epoch: 149 loss_train: 0.489 acc_train: 0.785 loss_test: 0.575 acc_test: 0.689 AUC = 0.77 time: 0.498s\n",
      "Epoch: 150 loss_train: 0.502 acc_train: 0.779 loss_test: 0.594 acc_test: 0.676 AUC = 0.78 time: 0.498s\n",
      "Epoch: 151 loss_train: 0.487 acc_train: 0.772 loss_test: 0.594 acc_test: 0.695 AUC = 0.76 time: 0.499s\n",
      "Epoch: 152 loss_train: 0.503 acc_train: 0.771 loss_test: 0.611 acc_test: 0.676 AUC = 0.75 time: 0.499s\n",
      "Epoch: 153 loss_train: 0.511 acc_train: 0.772 loss_test: 0.592 acc_test: 0.677 AUC = 0.77 time: 0.499s\n",
      "Epoch: 154 loss_train: 0.499 acc_train: 0.771 loss_test: 0.565 acc_test: 0.710 AUC = 0.79 time: 0.499s\n",
      "Epoch: 155 loss_train: 0.489 acc_train: 0.770 loss_test: 0.577 acc_test: 0.697 AUC = 0.78 time: 0.499s\n",
      "Epoch: 156 loss_train: 0.489 acc_train: 0.782 loss_test: 0.598 acc_test: 0.701 AUC = 0.77 time: 0.499s\n",
      "Epoch: 157 loss_train: 0.485 acc_train: 0.784 loss_test: 0.583 acc_test: 0.691 AUC = 0.78 time: 0.499s\n",
      "Epoch: 158 loss_train: 0.498 acc_train: 0.773 loss_test: 0.577 acc_test: 0.700 AUC = 0.78 time: 0.498s\n",
      "Epoch: 159 loss_train: 0.493 acc_train: 0.780 loss_test: 0.550 acc_test: 0.710 AUC = 0.80 time: 0.498s\n",
      "Epoch: 160 loss_train: 0.494 acc_train: 0.782 loss_test: 0.597 acc_test: 0.700 AUC = 0.77 time: 0.499s\n",
      "Epoch: 161 loss_train: 0.488 acc_train: 0.780 loss_test: 0.583 acc_test: 0.692 AUC = 0.77 time: 0.498s\n",
      "Epoch: 162 loss_train: 0.493 acc_train: 0.780 loss_test: 0.575 acc_test: 0.689 AUC = 0.78 time: 0.498s\n",
      "Epoch: 163 loss_train: 0.490 acc_train: 0.780 loss_test: 0.600 acc_test: 0.683 AUC = 0.76 time: 0.499s\n",
      "Epoch: 164 loss_train: 0.479 acc_train: 0.783 loss_test: 0.590 acc_test: 0.698 AUC = 0.77 time: 0.499s\n",
      "Epoch: 165 loss_train: 0.478 acc_train: 0.777 loss_test: 0.571 acc_test: 0.698 AUC = 0.79 time: 0.498s\n",
      "Epoch: 166 loss_train: 0.488 acc_train: 0.776 loss_test: 0.580 acc_test: 0.698 AUC = 0.78 time: 0.499s\n",
      "Epoch: 167 loss_train: 0.497 acc_train: 0.782 loss_test: 0.591 acc_test: 0.683 AUC = 0.78 time: 0.498s\n",
      "Epoch: 168 loss_train: 0.485 acc_train: 0.787 loss_test: 0.585 acc_test: 0.691 AUC = 0.78 time: 0.499s\n",
      "Epoch: 169 loss_train: 0.489 acc_train: 0.787 loss_test: 0.568 acc_test: 0.683 AUC = 0.79 time: 0.499s\n",
      "Epoch: 170 loss_train: 0.487 acc_train: 0.783 loss_test: 0.591 acc_test: 0.695 AUC = 0.78 time: 0.500s\n",
      "Epoch: 171 loss_train: 0.486 acc_train: 0.784 loss_test: 0.599 acc_test: 0.692 AUC = 0.77 time: 0.499s\n",
      "Epoch: 172 loss_train: 0.495 acc_train: 0.770 loss_test: 0.601 acc_test: 0.689 AUC = 0.77 time: 0.499s\n",
      "Epoch: 173 loss_train: 0.479 acc_train: 0.784 loss_test: 0.584 acc_test: 0.701 AUC = 0.78 time: 0.499s\n",
      "Epoch: 174 loss_train: 0.494 acc_train: 0.775 loss_test: 0.557 acc_test: 0.695 AUC = 0.80 time: 0.499s\n",
      "Epoch: 175 loss_train: 0.478 acc_train: 0.787 loss_test: 0.575 acc_test: 0.708 AUC = 0.79 time: 0.499s\n",
      "Epoch: 176 loss_train: 0.478 acc_train: 0.783 loss_test: 0.561 acc_test: 0.701 AUC = 0.80 time: 0.499s\n",
      "Epoch: 177 loss_train: 0.479 acc_train: 0.783 loss_test: 0.583 acc_test: 0.695 AUC = 0.79 time: 0.501s\n",
      "Epoch: 178 loss_train: 0.487 acc_train: 0.775 loss_test: 0.585 acc_test: 0.713 AUC = 0.77 time: 0.499s\n",
      "Epoch: 179 loss_train: 0.484 acc_train: 0.782 loss_test: 0.578 acc_test: 0.698 AUC = 0.79 time: 0.499s\n",
      "Epoch: 180 loss_train: 0.483 acc_train: 0.784 loss_test: 0.565 acc_test: 0.710 AUC = 0.80 time: 0.498s\n",
      "Epoch: 181 loss_train: 0.491 acc_train: 0.786 loss_test: 0.612 acc_test: 0.707 AUC = 0.77 time: 0.499s\n",
      "Epoch: 182 loss_train: 0.488 acc_train: 0.781 loss_test: 0.591 acc_test: 0.698 AUC = 0.78 time: 0.499s\n",
      "Epoch: 183 loss_train: 0.486 acc_train: 0.779 loss_test: 0.565 acc_test: 0.694 AUC = 0.79 time: 0.499s\n",
      "Epoch: 184 loss_train: 0.474 acc_train: 0.789 loss_test: 0.569 acc_test: 0.710 AUC = 0.80 time: 0.499s\n",
      "Epoch: 185 loss_train: 0.472 acc_train: 0.791 loss_test: 0.571 acc_test: 0.704 AUC = 0.79 time: 0.499s\n",
      "Epoch: 186 loss_train: 0.483 acc_train: 0.785 loss_test: 0.562 acc_test: 0.710 AUC = 0.79 time: 0.499s\n",
      "Epoch: 187 loss_train: 0.479 acc_train: 0.782 loss_test: 0.565 acc_test: 0.711 AUC = 0.79 time: 0.499s\n",
      "Epoch: 188 loss_train: 0.503 acc_train: 0.778 loss_test: 0.564 acc_test: 0.701 AUC = 0.79 time: 0.499s\n",
      "Epoch: 189 loss_train: 0.479 acc_train: 0.791 loss_test: 0.578 acc_test: 0.703 AUC = 0.78 time: 0.499s\n",
      "Epoch: 190 loss_train: 0.476 acc_train: 0.785 loss_test: 0.570 acc_test: 0.711 AUC = 0.79 time: 0.499s\n",
      "Epoch: 191 loss_train: 0.473 acc_train: 0.782 loss_test: 0.542 acc_test: 0.720 AUC = 0.80 time: 0.499s\n",
      "Epoch: 192 loss_train: 0.478 acc_train: 0.780 loss_test: 0.556 acc_test: 0.723 AUC = 0.79 time: 0.499s\n",
      "Epoch: 193 loss_train: 0.480 acc_train: 0.786 loss_test: 0.572 acc_test: 0.707 AUC = 0.79 time: 0.499s\n",
      "Epoch: 194 loss_train: 0.469 acc_train: 0.795 loss_test: 0.566 acc_test: 0.705 AUC = 0.80 time: 0.499s\n",
      "Epoch: 195 loss_train: 0.470 acc_train: 0.782 loss_test: 0.568 acc_test: 0.713 AUC = 0.80 time: 0.499s\n",
      "Epoch: 196 loss_train: 0.476 acc_train: 0.789 loss_test: 0.568 acc_test: 0.713 AUC = 0.79 time: 0.499s\n",
      "Epoch: 197 loss_train: 0.478 acc_train: 0.783 loss_test: 0.567 acc_test: 0.714 AUC = 0.80 time: 0.499s\n",
      "Epoch: 198 loss_train: 0.462 acc_train: 0.794 loss_test: 0.589 acc_test: 0.697 AUC = 0.78 time: 0.499s\n",
      "Epoch: 199 loss_train: 0.484 acc_train: 0.786 loss_test: 0.569 acc_test: 0.705 AUC = 0.79 time: 0.499s\n",
      "Epoch: 200 loss_train: 0.460 acc_train: 0.796 loss_test: 0.556 acc_test: 0.711 AUC = 0.81 time: 0.499s\n",
      "Epoch: 201 loss_train: 0.480 acc_train: 0.783 loss_test: 0.534 acc_test: 0.732 AUC = 0.82 time: 0.499s\n",
      "Epoch: 202 loss_train: 0.477 acc_train: 0.792 loss_test: 0.561 acc_test: 0.716 AUC = 0.80 time: 0.499s\n",
      "Epoch: 203 loss_train: 0.482 acc_train: 0.784 loss_test: 0.558 acc_test: 0.722 AUC = 0.79 time: 0.499s\n",
      "Epoch: 204 loss_train: 0.469 acc_train: 0.787 loss_test: 0.563 acc_test: 0.703 AUC = 0.80 time: 0.499s\n",
      "Epoch: 205 loss_train: 0.472 acc_train: 0.794 loss_test: 0.560 acc_test: 0.716 AUC = 0.80 time: 0.499s\n",
      "Epoch: 206 loss_train: 0.473 acc_train: 0.791 loss_test: 0.555 acc_test: 0.717 AUC = 0.80 time: 0.499s\n",
      "Epoch: 207 loss_train: 0.466 acc_train: 0.793 loss_test: 0.570 acc_test: 0.701 AUC = 0.79 time: 0.498s\n",
      "Epoch: 208 loss_train: 0.477 acc_train: 0.791 loss_test: 0.574 acc_test: 0.711 AUC = 0.80 time: 0.499s\n",
      "Epoch: 209 loss_train: 0.458 acc_train: 0.791 loss_test: 0.542 acc_test: 0.723 AUC = 0.81 time: 0.499s\n",
      "Epoch: 210 loss_train: 0.473 acc_train: 0.794 loss_test: 0.573 acc_test: 0.714 AUC = 0.79 time: 0.499s\n",
      "Epoch: 211 loss_train: 0.455 acc_train: 0.801 loss_test: 0.572 acc_test: 0.704 AUC = 0.80 time: 0.498s\n",
      "Epoch: 212 loss_train: 0.467 acc_train: 0.793 loss_test: 0.539 acc_test: 0.725 AUC = 0.82 time: 0.498s\n",
      "Epoch: 213 loss_train: 0.467 acc_train: 0.799 loss_test: 0.536 acc_test: 0.739 AUC = 0.82 time: 0.498s\n",
      "Epoch: 214 loss_train: 0.467 acc_train: 0.797 loss_test: 0.535 acc_test: 0.728 AUC = 0.82 time: 0.501s\n",
      "Epoch: 215 loss_train: 0.467 acc_train: 0.798 loss_test: 0.550 acc_test: 0.713 AUC = 0.81 time: 0.501s\n",
      "Epoch: 216 loss_train: 0.469 acc_train: 0.794 loss_test: 0.589 acc_test: 0.705 AUC = 0.79 time: 0.499s\n",
      "Epoch: 217 loss_train: 0.491 acc_train: 0.792 loss_test: 0.547 acc_test: 0.726 AUC = 0.81 time: 0.498s\n",
      "Epoch: 218 loss_train: 0.457 acc_train: 0.795 loss_test: 0.558 acc_test: 0.728 AUC = 0.80 time: 0.498s\n",
      "Epoch: 219 loss_train: 0.472 acc_train: 0.792 loss_test: 0.553 acc_test: 0.730 AUC = 0.80 time: 0.498s\n",
      "Epoch: 220 loss_train: 0.454 acc_train: 0.792 loss_test: 0.543 acc_test: 0.733 AUC = 0.81 time: 0.498s\n",
      "Epoch: 221 loss_train: 0.473 acc_train: 0.792 loss_test: 0.602 acc_test: 0.713 AUC = 0.77 time: 0.499s\n",
      "Epoch: 222 loss_train: 0.461 acc_train: 0.800 loss_test: 0.546 acc_test: 0.723 AUC = 0.80 time: 0.499s\n",
      "Epoch: 223 loss_train: 0.477 acc_train: 0.777 loss_test: 0.573 acc_test: 0.716 AUC = 0.79 time: 0.499s\n",
      "Epoch: 224 loss_train: 0.455 acc_train: 0.800 loss_test: 0.522 acc_test: 0.730 AUC = 0.82 time: 0.499s\n",
      "Epoch: 225 loss_train: 0.473 acc_train: 0.781 loss_test: 0.590 acc_test: 0.691 AUC = 0.78 time: 0.499s\n",
      "Epoch: 226 loss_train: 0.463 acc_train: 0.794 loss_test: 0.572 acc_test: 0.722 AUC = 0.80 time: 0.498s\n",
      "Epoch: 227 loss_train: 0.478 acc_train: 0.788 loss_test: 0.557 acc_test: 0.705 AUC = 0.80 time: 0.498s\n",
      "Epoch: 228 loss_train: 0.445 acc_train: 0.811 loss_test: 0.556 acc_test: 0.720 AUC = 0.81 time: 0.498s\n",
      "Epoch: 229 loss_train: 0.465 acc_train: 0.802 loss_test: 0.563 acc_test: 0.722 AUC = 0.80 time: 0.498s\n",
      "Epoch: 230 loss_train: 0.453 acc_train: 0.803 loss_test: 0.546 acc_test: 0.708 AUC = 0.81 time: 0.498s\n",
      "Epoch: 231 loss_train: 0.455 acc_train: 0.796 loss_test: 0.551 acc_test: 0.717 AUC = 0.80 time: 0.498s\n",
      "Epoch: 232 loss_train: 0.461 acc_train: 0.804 loss_test: 0.532 acc_test: 0.716 AUC = 0.83 time: 0.499s\n",
      "Epoch: 233 loss_train: 0.471 acc_train: 0.785 loss_test: 0.563 acc_test: 0.700 AUC = 0.81 time: 0.499s\n",
      "Epoch: 234 loss_train: 0.456 acc_train: 0.794 loss_test: 0.563 acc_test: 0.707 AUC = 0.80 time: 0.499s\n",
      "Epoch: 235 loss_train: 0.455 acc_train: 0.799 loss_test: 0.567 acc_test: 0.711 AUC = 0.80 time: 0.499s\n",
      "Epoch: 236 loss_train: 0.448 acc_train: 0.808 loss_test: 0.573 acc_test: 0.713 AUC = 0.80 time: 0.498s\n",
      "Epoch: 237 loss_train: 0.450 acc_train: 0.804 loss_test: 0.554 acc_test: 0.713 AUC = 0.80 time: 0.498s\n",
      "Epoch: 238 loss_train: 0.450 acc_train: 0.801 loss_test: 0.552 acc_test: 0.725 AUC = 0.81 time: 0.498s\n",
      "Epoch: 239 loss_train: 0.463 acc_train: 0.791 loss_test: 0.546 acc_test: 0.725 AUC = 0.81 time: 0.499s\n",
      "Epoch: 240 loss_train: 0.462 acc_train: 0.803 loss_test: 0.540 acc_test: 0.705 AUC = 0.82 time: 0.499s\n",
      "Epoch: 241 loss_train: 0.473 acc_train: 0.789 loss_test: 0.555 acc_test: 0.686 AUC = 0.81 time: 0.498s\n",
      "Epoch: 242 loss_train: 0.468 acc_train: 0.795 loss_test: 0.568 acc_test: 0.719 AUC = 0.80 time: 0.499s\n",
      "Epoch: 243 loss_train: 0.456 acc_train: 0.798 loss_test: 0.542 acc_test: 0.725 AUC = 0.83 time: 0.499s\n",
      "Epoch: 244 loss_train: 0.468 acc_train: 0.796 loss_test: 0.569 acc_test: 0.722 AUC = 0.79 time: 0.499s\n",
      "Epoch: 245 loss_train: 0.470 acc_train: 0.797 loss_test: 0.558 acc_test: 0.726 AUC = 0.81 time: 0.498s\n",
      "Epoch: 246 loss_train: 0.467 acc_train: 0.784 loss_test: 0.543 acc_test: 0.729 AUC = 0.83 time: 0.499s\n",
      "Epoch: 247 loss_train: 0.446 acc_train: 0.806 loss_test: 0.553 acc_test: 0.741 AUC = 0.81 time: 0.499s\n",
      "Epoch: 248 loss_train: 0.450 acc_train: 0.802 loss_test: 0.556 acc_test: 0.719 AUC = 0.80 time: 0.498s\n",
      "Epoch: 249 loss_train: 0.452 acc_train: 0.801 loss_test: 0.517 acc_test: 0.747 AUC = 0.83 time: 0.498s\n",
      "Epoch: 250 loss_train: 0.460 acc_train: 0.804 loss_test: 0.544 acc_test: 0.730 AUC = 0.82 time: 0.498s\n",
      "Epoch: 251 loss_train: 0.458 acc_train: 0.812 loss_test: 0.562 acc_test: 0.714 AUC = 0.81 time: 0.499s\n",
      "Epoch: 252 loss_train: 0.451 acc_train: 0.806 loss_test: 0.545 acc_test: 0.726 AUC = 0.82 time: 0.499s\n",
      "Epoch: 253 loss_train: 0.456 acc_train: 0.796 loss_test: 0.578 acc_test: 0.694 AUC = 0.78 time: 0.499s\n",
      "Epoch: 254 loss_train: 0.447 acc_train: 0.808 loss_test: 0.535 acc_test: 0.735 AUC = 0.82 time: 0.499s\n",
      "Epoch: 255 loss_train: 0.456 acc_train: 0.813 loss_test: 0.556 acc_test: 0.732 AUC = 0.82 time: 0.499s\n",
      "Epoch: 256 loss_train: 0.454 acc_train: 0.793 loss_test: 0.535 acc_test: 0.716 AUC = 0.82 time: 0.498s\n",
      "Epoch: 257 loss_train: 0.462 acc_train: 0.799 loss_test: 0.546 acc_test: 0.714 AUC = 0.82 time: 0.499s\n",
      "Epoch: 258 loss_train: 0.448 acc_train: 0.807 loss_test: 0.543 acc_test: 0.748 AUC = 0.82 time: 0.499s\n",
      "Epoch: 259 loss_train: 0.453 acc_train: 0.803 loss_test: 0.531 acc_test: 0.729 AUC = 0.81 time: 0.498s\n",
      "Epoch: 260 loss_train: 0.451 acc_train: 0.800 loss_test: 0.553 acc_test: 0.716 AUC = 0.81 time: 0.498s\n",
      "Epoch: 261 loss_train: 0.459 acc_train: 0.795 loss_test: 0.522 acc_test: 0.744 AUC = 0.82 time: 0.499s\n",
      "Epoch: 262 loss_train: 0.447 acc_train: 0.801 loss_test: 0.552 acc_test: 0.725 AUC = 0.82 time: 0.498s\n",
      "Epoch: 263 loss_train: 0.437 acc_train: 0.814 loss_test: 0.565 acc_test: 0.722 AUC = 0.79 time: 0.498s\n",
      "Epoch: 264 loss_train: 0.448 acc_train: 0.796 loss_test: 0.550 acc_test: 0.732 AUC = 0.81 time: 0.498s\n",
      "Epoch: 265 loss_train: 0.443 acc_train: 0.802 loss_test: 0.570 acc_test: 0.720 AUC = 0.79 time: 0.499s\n",
      "Epoch: 266 loss_train: 0.462 acc_train: 0.798 loss_test: 0.550 acc_test: 0.728 AUC = 0.82 time: 0.499s\n",
      "Epoch: 267 loss_train: 0.451 acc_train: 0.810 loss_test: 0.562 acc_test: 0.723 AUC = 0.81 time: 0.498s\n",
      "Epoch: 268 loss_train: 0.449 acc_train: 0.795 loss_test: 0.537 acc_test: 0.723 AUC = 0.82 time: 0.498s\n",
      "Epoch: 269 loss_train: 0.445 acc_train: 0.803 loss_test: 0.565 acc_test: 0.714 AUC = 0.80 time: 0.499s\n",
      "Epoch: 270 loss_train: 0.439 acc_train: 0.816 loss_test: 0.535 acc_test: 0.738 AUC = 0.83 time: 0.499s\n",
      "Epoch: 271 loss_train: 0.450 acc_train: 0.797 loss_test: 0.551 acc_test: 0.716 AUC = 0.82 time: 0.498s\n",
      "Epoch: 272 loss_train: 0.448 acc_train: 0.803 loss_test: 0.551 acc_test: 0.716 AUC = 0.81 time: 0.498s\n",
      "Epoch: 273 loss_train: 0.442 acc_train: 0.806 loss_test: 0.573 acc_test: 0.710 AUC = 0.80 time: 0.499s\n",
      "Epoch: 274 loss_train: 0.453 acc_train: 0.808 loss_test: 0.593 acc_test: 0.722 AUC = 0.80 time: 0.499s\n",
      "Epoch: 275 loss_train: 0.449 acc_train: 0.803 loss_test: 0.576 acc_test: 0.716 AUC = 0.81 time: 0.499s\n",
      "Epoch: 276 loss_train: 0.451 acc_train: 0.804 loss_test: 0.552 acc_test: 0.725 AUC = 0.81 time: 0.498s\n",
      "Epoch: 277 loss_train: 0.459 acc_train: 0.804 loss_test: 0.540 acc_test: 0.711 AUC = 0.82 time: 0.498s\n",
      "Epoch: 278 loss_train: 0.446 acc_train: 0.809 loss_test: 0.548 acc_test: 0.723 AUC = 0.82 time: 0.498s\n",
      "Epoch: 279 loss_train: 0.459 acc_train: 0.801 loss_test: 0.530 acc_test: 0.732 AUC = 0.83 time: 0.498s\n",
      "Epoch: 280 loss_train: 0.457 acc_train: 0.809 loss_test: 0.546 acc_test: 0.728 AUC = 0.82 time: 0.498s\n",
      "Epoch: 281 loss_train: 0.458 acc_train: 0.801 loss_test: 0.542 acc_test: 0.713 AUC = 0.82 time: 0.498s\n",
      "Epoch: 282 loss_train: 0.451 acc_train: 0.796 loss_test: 0.544 acc_test: 0.723 AUC = 0.80 time: 0.498s\n",
      "Epoch: 283 loss_train: 0.456 acc_train: 0.792 loss_test: 0.544 acc_test: 0.723 AUC = 0.82 time: 0.498s\n",
      "Epoch: 284 loss_train: 0.452 acc_train: 0.807 loss_test: 0.526 acc_test: 0.738 AUC = 0.83 time: 0.498s\n",
      "Epoch: 285 loss_train: 0.444 acc_train: 0.799 loss_test: 0.535 acc_test: 0.735 AUC = 0.82 time: 0.499s\n",
      "Epoch: 286 loss_train: 0.458 acc_train: 0.804 loss_test: 0.539 acc_test: 0.741 AUC = 0.82 time: 0.499s\n",
      "Epoch: 287 loss_train: 0.451 acc_train: 0.801 loss_test: 0.534 acc_test: 0.736 AUC = 0.82 time: 0.499s\n",
      "Epoch: 288 loss_train: 0.461 acc_train: 0.803 loss_test: 0.522 acc_test: 0.739 AUC = 0.83 time: 0.499s\n",
      "Epoch: 289 loss_train: 0.448 acc_train: 0.802 loss_test: 0.519 acc_test: 0.732 AUC = 0.82 time: 0.499s\n",
      "Epoch: 290 loss_train: 0.451 acc_train: 0.806 loss_test: 0.528 acc_test: 0.745 AUC = 0.82 time: 0.499s\n",
      "Epoch: 291 loss_train: 0.456 acc_train: 0.813 loss_test: 0.527 acc_test: 0.723 AUC = 0.82 time: 0.499s\n",
      "Epoch: 292 loss_train: 0.437 acc_train: 0.823 loss_test: 0.556 acc_test: 0.711 AUC = 0.80 time: 0.499s\n",
      "Epoch: 293 loss_train: 0.454 acc_train: 0.797 loss_test: 0.555 acc_test: 0.722 AUC = 0.81 time: 0.499s\n",
      "Epoch: 294 loss_train: 0.450 acc_train: 0.794 loss_test: 0.510 acc_test: 0.729 AUC = 0.84 time: 0.498s\n",
      "Epoch: 295 loss_train: 0.449 acc_train: 0.806 loss_test: 0.547 acc_test: 0.717 AUC = 0.82 time: 0.499s\n",
      "Epoch: 296 loss_train: 0.440 acc_train: 0.811 loss_test: 0.543 acc_test: 0.733 AUC = 0.82 time: 0.499s\n",
      "Epoch: 297 loss_train: 0.445 acc_train: 0.806 loss_test: 0.548 acc_test: 0.723 AUC = 0.82 time: 0.499s\n",
      "Epoch: 298 loss_train: 0.447 acc_train: 0.801 loss_test: 0.528 acc_test: 0.732 AUC = 0.82 time: 0.498s\n",
      "Epoch: 299 loss_train: 0.441 acc_train: 0.806 loss_test: 0.553 acc_test: 0.726 AUC = 0.82 time: 0.499s\n",
      "Epoch: 300 loss_train: 0.446 acc_train: 0.805 loss_test: 0.546 acc_test: 0.730 AUC = 0.82 time: 0.499s\n",
      "Epoch: 301 loss_train: 0.440 acc_train: 0.815 loss_test: 0.549 acc_test: 0.728 AUC = 0.82 time: 0.499s\n",
      "Epoch: 302 loss_train: 0.449 acc_train: 0.806 loss_test: 0.532 acc_test: 0.726 AUC = 0.82 time: 0.498s\n",
      "Epoch: 303 loss_train: 0.451 acc_train: 0.799 loss_test: 0.532 acc_test: 0.735 AUC = 0.83 time: 0.499s\n",
      "Epoch: 304 loss_train: 0.434 acc_train: 0.812 loss_test: 0.559 acc_test: 0.722 AUC = 0.81 time: 0.498s\n",
      "Epoch: 305 loss_train: 0.447 acc_train: 0.804 loss_test: 0.517 acc_test: 0.729 AUC = 0.83 time: 0.499s\n",
      "Epoch: 306 loss_train: 0.443 acc_train: 0.811 loss_test: 0.539 acc_test: 0.729 AUC = 0.81 time: 0.499s\n",
      "Epoch: 307 loss_train: 0.446 acc_train: 0.803 loss_test: 0.515 acc_test: 0.739 AUC = 0.83 time: 0.498s\n",
      "Epoch: 308 loss_train: 0.435 acc_train: 0.808 loss_test: 0.533 acc_test: 0.723 AUC = 0.82 time: 0.499s\n",
      "Epoch: 309 loss_train: 0.433 acc_train: 0.816 loss_test: 0.535 acc_test: 0.736 AUC = 0.82 time: 0.499s\n",
      "Epoch: 310 loss_train: 0.436 acc_train: 0.816 loss_test: 0.500 acc_test: 0.745 AUC = 0.84 time: 0.498s\n",
      "Epoch: 311 loss_train: 0.438 acc_train: 0.796 loss_test: 0.540 acc_test: 0.732 AUC = 0.83 time: 0.499s\n",
      "Epoch: 312 loss_train: 0.447 acc_train: 0.811 loss_test: 0.532 acc_test: 0.729 AUC = 0.83 time: 0.499s\n",
      "Epoch: 313 loss_train: 0.443 acc_train: 0.797 loss_test: 0.523 acc_test: 0.732 AUC = 0.82 time: 0.499s\n",
      "Epoch: 314 loss_train: 0.442 acc_train: 0.809 loss_test: 0.535 acc_test: 0.733 AUC = 0.82 time: 0.499s\n",
      "Epoch: 315 loss_train: 0.446 acc_train: 0.806 loss_test: 0.563 acc_test: 0.722 AUC = 0.81 time: 0.498s\n",
      "Epoch: 316 loss_train: 0.433 acc_train: 0.810 loss_test: 0.535 acc_test: 0.736 AUC = 0.82 time: 0.499s\n",
      "Epoch: 317 loss_train: 0.430 acc_train: 0.807 loss_test: 0.531 acc_test: 0.725 AUC = 0.83 time: 0.498s\n",
      "Epoch: 318 loss_train: 0.451 acc_train: 0.797 loss_test: 0.523 acc_test: 0.744 AUC = 0.83 time: 0.499s\n",
      "Epoch: 319 loss_train: 0.437 acc_train: 0.803 loss_test: 0.530 acc_test: 0.726 AUC = 0.82 time: 0.498s\n",
      "Epoch: 320 loss_train: 0.435 acc_train: 0.815 loss_test: 0.527 acc_test: 0.742 AUC = 0.83 time: 0.499s\n",
      "Epoch: 321 loss_train: 0.436 acc_train: 0.808 loss_test: 0.513 acc_test: 0.742 AUC = 0.84 time: 0.498s\n",
      "Epoch: 322 loss_train: 0.442 acc_train: 0.808 loss_test: 0.552 acc_test: 0.741 AUC = 0.82 time: 0.498s\n",
      "Epoch: 323 loss_train: 0.434 acc_train: 0.809 loss_test: 0.528 acc_test: 0.738 AUC = 0.83 time: 0.498s\n",
      "Epoch: 324 loss_train: 0.435 acc_train: 0.819 loss_test: 0.507 acc_test: 0.751 AUC = 0.85 time: 0.498s\n",
      "Epoch: 325 loss_train: 0.443 acc_train: 0.809 loss_test: 0.517 acc_test: 0.733 AUC = 0.84 time: 0.498s\n",
      "Epoch: 326 loss_train: 0.433 acc_train: 0.822 loss_test: 0.532 acc_test: 0.738 AUC = 0.82 time: 0.498s\n",
      "Epoch: 327 loss_train: 0.450 acc_train: 0.806 loss_test: 0.546 acc_test: 0.738 AUC = 0.82 time: 0.498s\n",
      "Epoch: 328 loss_train: 0.432 acc_train: 0.820 loss_test: 0.537 acc_test: 0.720 AUC = 0.82 time: 0.498s\n",
      "Epoch: 329 loss_train: 0.435 acc_train: 0.812 loss_test: 0.526 acc_test: 0.741 AUC = 0.83 time: 0.498s\n",
      "Epoch: 330 loss_train: 0.435 acc_train: 0.799 loss_test: 0.519 acc_test: 0.735 AUC = 0.84 time: 0.501s\n",
      "Epoch: 331 loss_train: 0.443 acc_train: 0.816 loss_test: 0.526 acc_test: 0.750 AUC = 0.83 time: 0.498s\n",
      "Epoch: 332 loss_train: 0.431 acc_train: 0.811 loss_test: 0.526 acc_test: 0.736 AUC = 0.82 time: 0.498s\n",
      "Epoch: 333 loss_train: 0.447 acc_train: 0.803 loss_test: 0.571 acc_test: 0.714 AUC = 0.80 time: 0.499s\n",
      "Epoch: 334 loss_train: 0.428 acc_train: 0.811 loss_test: 0.551 acc_test: 0.711 AUC = 0.80 time: 0.498s\n",
      "Epoch: 335 loss_train: 0.451 acc_train: 0.808 loss_test: 0.549 acc_test: 0.720 AUC = 0.82 time: 0.498s\n",
      "Epoch: 336 loss_train: 0.443 acc_train: 0.809 loss_test: 0.551 acc_test: 0.710 AUC = 0.81 time: 0.498s\n",
      "Epoch: 337 loss_train: 0.444 acc_train: 0.805 loss_test: 0.517 acc_test: 0.732 AUC = 0.84 time: 0.498s\n",
      "Epoch: 338 loss_train: 0.438 acc_train: 0.812 loss_test: 0.515 acc_test: 0.730 AUC = 0.83 time: 0.498s\n",
      "Epoch: 339 loss_train: 0.459 acc_train: 0.797 loss_test: 0.514 acc_test: 0.742 AUC = 0.84 time: 0.498s\n",
      "Epoch: 340 loss_train: 0.442 acc_train: 0.814 loss_test: 0.527 acc_test: 0.720 AUC = 0.83 time: 0.498s\n",
      "Epoch: 341 loss_train: 0.449 acc_train: 0.802 loss_test: 0.528 acc_test: 0.741 AUC = 0.83 time: 0.498s\n",
      "Epoch: 342 loss_train: 0.443 acc_train: 0.800 loss_test: 0.501 acc_test: 0.751 AUC = 0.85 time: 0.499s\n",
      "Epoch: 343 loss_train: 0.452 acc_train: 0.806 loss_test: 0.549 acc_test: 0.722 AUC = 0.81 time: 0.498s\n",
      "Epoch: 344 loss_train: 0.436 acc_train: 0.813 loss_test: 0.538 acc_test: 0.730 AUC = 0.82 time: 0.498s\n",
      "Epoch: 345 loss_train: 0.416 acc_train: 0.825 loss_test: 0.542 acc_test: 0.744 AUC = 0.83 time: 0.498s\n",
      "Epoch: 346 loss_train: 0.435 acc_train: 0.808 loss_test: 0.522 acc_test: 0.741 AUC = 0.83 time: 0.498s\n",
      "Epoch: 347 loss_train: 0.438 acc_train: 0.808 loss_test: 0.515 acc_test: 0.742 AUC = 0.84 time: 0.499s\n",
      "Epoch: 348 loss_train: 0.447 acc_train: 0.801 loss_test: 0.537 acc_test: 0.723 AUC = 0.82 time: 0.498s\n",
      "Epoch: 349 loss_train: 0.436 acc_train: 0.816 loss_test: 0.532 acc_test: 0.736 AUC = 0.83 time: 0.498s\n",
      "Epoch: 350 loss_train: 0.429 acc_train: 0.816 loss_test: 0.559 acc_test: 0.735 AUC = 0.81 time: 0.499s\n",
      "Epoch: 351 loss_train: 0.432 acc_train: 0.814 loss_test: 0.537 acc_test: 0.725 AUC = 0.83 time: 0.498s\n",
      "Epoch: 352 loss_train: 0.435 acc_train: 0.813 loss_test: 0.512 acc_test: 0.742 AUC = 0.84 time: 0.499s\n",
      "Epoch: 353 loss_train: 0.432 acc_train: 0.811 loss_test: 0.556 acc_test: 0.717 AUC = 0.82 time: 0.498s\n",
      "Epoch: 354 loss_train: 0.440 acc_train: 0.804 loss_test: 0.524 acc_test: 0.723 AUC = 0.83 time: 0.498s\n",
      "Epoch: 355 loss_train: 0.444 acc_train: 0.810 loss_test: 0.530 acc_test: 0.733 AUC = 0.83 time: 0.498s\n",
      "Epoch: 356 loss_train: 0.439 acc_train: 0.809 loss_test: 0.537 acc_test: 0.728 AUC = 0.82 time: 0.498s\n",
      "Epoch: 357 loss_train: 0.437 acc_train: 0.823 loss_test: 0.549 acc_test: 0.730 AUC = 0.81 time: 0.498s\n",
      "Epoch: 358 loss_train: 0.435 acc_train: 0.813 loss_test: 0.528 acc_test: 0.739 AUC = 0.82 time: 0.498s\n",
      "Epoch: 359 loss_train: 0.439 acc_train: 0.815 loss_test: 0.516 acc_test: 0.735 AUC = 0.83 time: 0.499s\n",
      "Epoch: 360 loss_train: 0.442 acc_train: 0.816 loss_test: 0.554 acc_test: 0.725 AUC = 0.81 time: 0.498s\n",
      "Epoch: 361 loss_train: 0.434 acc_train: 0.811 loss_test: 0.505 acc_test: 0.748 AUC = 0.84 time: 0.498s\n",
      "Epoch: 362 loss_train: 0.439 acc_train: 0.813 loss_test: 0.536 acc_test: 0.714 AUC = 0.82 time: 0.499s\n",
      "Epoch: 363 loss_train: 0.441 acc_train: 0.804 loss_test: 0.501 acc_test: 0.750 AUC = 0.84 time: 0.498s\n",
      "Epoch: 364 loss_train: 0.424 acc_train: 0.816 loss_test: 0.532 acc_test: 0.736 AUC = 0.82 time: 0.498s\n",
      "Epoch: 365 loss_train: 0.431 acc_train: 0.813 loss_test: 0.538 acc_test: 0.716 AUC = 0.82 time: 0.499s\n",
      "Epoch: 366 loss_train: 0.431 acc_train: 0.820 loss_test: 0.507 acc_test: 0.744 AUC = 0.85 time: 0.499s\n",
      "Epoch: 367 loss_train: 0.435 acc_train: 0.817 loss_test: 0.505 acc_test: 0.747 AUC = 0.85 time: 0.498s\n",
      "Epoch: 368 loss_train: 0.438 acc_train: 0.806 loss_test: 0.530 acc_test: 0.735 AUC = 0.84 time: 0.499s\n",
      "Epoch: 369 loss_train: 0.439 acc_train: 0.810 loss_test: 0.535 acc_test: 0.736 AUC = 0.83 time: 0.499s\n",
      "Epoch: 370 loss_train: 0.437 acc_train: 0.817 loss_test: 0.523 acc_test: 0.741 AUC = 0.84 time: 0.499s\n",
      "Epoch: 371 loss_train: 0.428 acc_train: 0.817 loss_test: 0.505 acc_test: 0.736 AUC = 0.84 time: 0.499s\n",
      "Epoch: 372 loss_train: 0.432 acc_train: 0.805 loss_test: 0.526 acc_test: 0.735 AUC = 0.83 time: 0.499s\n",
      "Epoch: 373 loss_train: 0.418 acc_train: 0.820 loss_test: 0.522 acc_test: 0.723 AUC = 0.83 time: 0.499s\n",
      "Epoch: 374 loss_train: 0.429 acc_train: 0.810 loss_test: 0.532 acc_test: 0.728 AUC = 0.83 time: 0.499s\n",
      "Epoch: 375 loss_train: 0.427 acc_train: 0.810 loss_test: 0.523 acc_test: 0.741 AUC = 0.83 time: 0.499s\n",
      "Epoch: 376 loss_train: 0.426 acc_train: 0.811 loss_test: 0.547 acc_test: 0.729 AUC = 0.82 time: 0.499s\n",
      "Epoch: 377 loss_train: 0.432 acc_train: 0.810 loss_test: 0.526 acc_test: 0.716 AUC = 0.83 time: 0.499s\n",
      "Epoch: 378 loss_train: 0.419 acc_train: 0.821 loss_test: 0.510 acc_test: 0.744 AUC = 0.84 time: 0.498s\n",
      "Epoch: 379 loss_train: 0.423 acc_train: 0.818 loss_test: 0.548 acc_test: 0.716 AUC = 0.83 time: 0.499s\n",
      "Epoch: 380 loss_train: 0.429 acc_train: 0.823 loss_test: 0.554 acc_test: 0.729 AUC = 0.81 time: 0.499s\n",
      "Epoch: 381 loss_train: 0.422 acc_train: 0.815 loss_test: 0.541 acc_test: 0.728 AUC = 0.82 time: 0.499s\n",
      "Epoch: 382 loss_train: 0.440 acc_train: 0.813 loss_test: 0.519 acc_test: 0.736 AUC = 0.84 time: 0.498s\n",
      "Epoch: 383 loss_train: 0.424 acc_train: 0.815 loss_test: 0.501 acc_test: 0.732 AUC = 0.85 time: 0.498s\n",
      "Epoch: 384 loss_train: 0.435 acc_train: 0.813 loss_test: 0.507 acc_test: 0.742 AUC = 0.85 time: 0.499s\n",
      "Epoch: 385 loss_train: 0.424 acc_train: 0.822 loss_test: 0.538 acc_test: 0.728 AUC = 0.82 time: 0.498s\n",
      "Epoch: 386 loss_train: 0.430 acc_train: 0.817 loss_test: 0.531 acc_test: 0.739 AUC = 0.84 time: 0.499s\n",
      "Epoch: 387 loss_train: 0.418 acc_train: 0.815 loss_test: 0.524 acc_test: 0.739 AUC = 0.84 time: 0.499s\n",
      "Epoch: 388 loss_train: 0.423 acc_train: 0.823 loss_test: 0.516 acc_test: 0.742 AUC = 0.83 time: 0.499s\n",
      "Epoch: 389 loss_train: 0.437 acc_train: 0.806 loss_test: 0.518 acc_test: 0.747 AUC = 0.84 time: 0.563s\n",
      "Epoch: 390 loss_train: 0.424 acc_train: 0.810 loss_test: 0.522 acc_test: 0.738 AUC = 0.83 time: 0.499s\n",
      "Epoch: 391 loss_train: 0.426 acc_train: 0.814 loss_test: 0.519 acc_test: 0.729 AUC = 0.83 time: 0.498s\n",
      "Epoch: 392 loss_train: 0.433 acc_train: 0.804 loss_test: 0.514 acc_test: 0.742 AUC = 0.84 time: 0.498s\n",
      "Epoch: 393 loss_train: 0.431 acc_train: 0.818 loss_test: 0.514 acc_test: 0.741 AUC = 0.83 time: 0.499s\n",
      "Epoch: 394 loss_train: 0.428 acc_train: 0.808 loss_test: 0.527 acc_test: 0.732 AUC = 0.83 time: 0.498s\n",
      "Epoch: 395 loss_train: 0.424 acc_train: 0.817 loss_test: 0.526 acc_test: 0.725 AUC = 0.83 time: 0.498s\n",
      "Epoch: 396 loss_train: 0.439 acc_train: 0.811 loss_test: 0.561 acc_test: 0.732 AUC = 0.81 time: 0.498s\n",
      "Epoch: 397 loss_train: 0.439 acc_train: 0.806 loss_test: 0.524 acc_test: 0.747 AUC = 0.84 time: 0.498s\n",
      "Epoch: 398 loss_train: 0.411 acc_train: 0.818 loss_test: 0.528 acc_test: 0.729 AUC = 0.83 time: 0.499s\n",
      "Epoch: 399 loss_train: 0.441 acc_train: 0.808 loss_test: 0.515 acc_test: 0.753 AUC = 0.84 time: 0.498s\n",
      "Epoch: 400 loss_train: 0.424 acc_train: 0.818 loss_test: 0.528 acc_test: 0.750 AUC = 0.83 time: 0.498s\n",
      "Epoch: 401 loss_train: 0.433 acc_train: 0.810 loss_test: 0.499 acc_test: 0.741 AUC = 0.85 time: 0.498s\n",
      "Epoch: 402 loss_train: 0.411 acc_train: 0.818 loss_test: 0.531 acc_test: 0.739 AUC = 0.83 time: 0.498s\n",
      "Epoch: 403 loss_train: 0.421 acc_train: 0.818 loss_test: 0.520 acc_test: 0.722 AUC = 0.83 time: 0.498s\n",
      "Epoch: 404 loss_train: 0.430 acc_train: 0.811 loss_test: 0.504 acc_test: 0.756 AUC = 0.85 time: 0.498s\n",
      "Epoch: 405 loss_train: 0.420 acc_train: 0.821 loss_test: 0.564 acc_test: 0.732 AUC = 0.81 time: 0.498s\n",
      "Epoch: 406 loss_train: 0.418 acc_train: 0.815 loss_test: 0.498 acc_test: 0.741 AUC = 0.85 time: 0.498s\n",
      "Epoch: 407 loss_train: 0.413 acc_train: 0.826 loss_test: 0.509 acc_test: 0.745 AUC = 0.84 time: 0.499s\n",
      "Epoch: 408 loss_train: 0.426 acc_train: 0.824 loss_test: 0.530 acc_test: 0.732 AUC = 0.83 time: 0.499s\n",
      "Epoch: 409 loss_train: 0.425 acc_train: 0.815 loss_test: 0.507 acc_test: 0.750 AUC = 0.84 time: 0.501s\n",
      "Epoch: 410 loss_train: 0.425 acc_train: 0.813 loss_test: 0.518 acc_test: 0.738 AUC = 0.84 time: 0.498s\n",
      "Epoch: 411 loss_train: 0.429 acc_train: 0.823 loss_test: 0.510 acc_test: 0.750 AUC = 0.83 time: 0.498s\n",
      "Epoch: 412 loss_train: 0.424 acc_train: 0.813 loss_test: 0.503 acc_test: 0.748 AUC = 0.84 time: 0.498s\n",
      "Epoch: 413 loss_train: 0.408 acc_train: 0.823 loss_test: 0.502 acc_test: 0.735 AUC = 0.85 time: 0.500s\n",
      "Epoch: 414 loss_train: 0.426 acc_train: 0.821 loss_test: 0.548 acc_test: 0.732 AUC = 0.82 time: 0.499s\n",
      "Epoch: 415 loss_train: 0.426 acc_train: 0.815 loss_test: 0.513 acc_test: 0.738 AUC = 0.84 time: 0.499s\n",
      "Epoch: 416 loss_train: 0.424 acc_train: 0.809 loss_test: 0.499 acc_test: 0.744 AUC = 0.84 time: 0.498s\n",
      "Epoch: 417 loss_train: 0.445 acc_train: 0.808 loss_test: 0.506 acc_test: 0.722 AUC = 0.84 time: 0.498s\n",
      "Epoch: 418 loss_train: 0.427 acc_train: 0.809 loss_test: 0.532 acc_test: 0.738 AUC = 0.84 time: 0.498s\n",
      "Epoch: 419 loss_train: 0.417 acc_train: 0.811 loss_test: 0.499 acc_test: 0.748 AUC = 0.84 time: 0.499s\n",
      "Epoch: 420 loss_train: 0.425 acc_train: 0.808 loss_test: 0.513 acc_test: 0.738 AUC = 0.84 time: 0.499s\n",
      "Epoch: 421 loss_train: 0.442 acc_train: 0.812 loss_test: 0.501 acc_test: 0.758 AUC = 0.85 time: 0.499s\n",
      "Epoch: 422 loss_train: 0.410 acc_train: 0.821 loss_test: 0.524 acc_test: 0.750 AUC = 0.84 time: 0.498s\n",
      "Epoch: 423 loss_train: 0.440 acc_train: 0.809 loss_test: 0.529 acc_test: 0.741 AUC = 0.83 time: 0.499s\n",
      "Epoch: 424 loss_train: 0.429 acc_train: 0.809 loss_test: 0.539 acc_test: 0.725 AUC = 0.82 time: 0.498s\n",
      "Epoch: 425 loss_train: 0.419 acc_train: 0.825 loss_test: 0.496 acc_test: 0.739 AUC = 0.85 time: 0.499s\n",
      "Epoch: 426 loss_train: 0.425 acc_train: 0.819 loss_test: 0.512 acc_test: 0.730 AUC = 0.84 time: 0.499s\n",
      "Epoch: 427 loss_train: 0.416 acc_train: 0.816 loss_test: 0.523 acc_test: 0.738 AUC = 0.84 time: 0.499s\n",
      "Epoch: 428 loss_train: 0.415 acc_train: 0.811 loss_test: 0.523 acc_test: 0.735 AUC = 0.84 time: 0.500s\n",
      "Epoch: 429 loss_train: 0.435 acc_train: 0.815 loss_test: 0.514 acc_test: 0.741 AUC = 0.84 time: 0.499s\n",
      "Epoch: 430 loss_train: 0.425 acc_train: 0.818 loss_test: 0.528 acc_test: 0.733 AUC = 0.84 time: 0.498s\n",
      "Epoch: 431 loss_train: 0.415 acc_train: 0.824 loss_test: 0.502 acc_test: 0.747 AUC = 0.84 time: 0.499s\n",
      "Epoch: 432 loss_train: 0.433 acc_train: 0.812 loss_test: 0.522 acc_test: 0.735 AUC = 0.83 time: 0.498s\n",
      "Epoch: 433 loss_train: 0.414 acc_train: 0.820 loss_test: 0.507 acc_test: 0.770 AUC = 0.85 time: 0.499s\n",
      "Epoch: 434 loss_train: 0.427 acc_train: 0.806 loss_test: 0.511 acc_test: 0.747 AUC = 0.84 time: 0.498s\n",
      "Epoch: 435 loss_train: 0.432 acc_train: 0.807 loss_test: 0.500 acc_test: 0.756 AUC = 0.85 time: 0.499s\n",
      "Epoch: 436 loss_train: 0.431 acc_train: 0.807 loss_test: 0.511 acc_test: 0.742 AUC = 0.84 time: 0.499s\n",
      "Epoch: 437 loss_train: 0.440 acc_train: 0.799 loss_test: 0.544 acc_test: 0.728 AUC = 0.82 time: 0.499s\n",
      "Epoch: 438 loss_train: 0.428 acc_train: 0.816 loss_test: 0.519 acc_test: 0.733 AUC = 0.84 time: 0.498s\n",
      "Epoch: 439 loss_train: 0.414 acc_train: 0.824 loss_test: 0.507 acc_test: 0.751 AUC = 0.85 time: 0.498s\n",
      "Epoch: 440 loss_train: 0.418 acc_train: 0.813 loss_test: 0.532 acc_test: 0.733 AUC = 0.84 time: 0.498s\n",
      "Epoch: 441 loss_train: 0.414 acc_train: 0.826 loss_test: 0.521 acc_test: 0.733 AUC = 0.83 time: 0.499s\n",
      "Epoch: 442 loss_train: 0.435 acc_train: 0.806 loss_test: 0.505 acc_test: 0.747 AUC = 0.85 time: 0.498s\n",
      "Epoch: 443 loss_train: 0.432 acc_train: 0.814 loss_test: 0.541 acc_test: 0.728 AUC = 0.82 time: 0.499s\n",
      "Epoch: 444 loss_train: 0.419 acc_train: 0.818 loss_test: 0.538 acc_test: 0.733 AUC = 0.83 time: 0.499s\n",
      "Epoch: 445 loss_train: 0.435 acc_train: 0.807 loss_test: 0.509 acc_test: 0.751 AUC = 0.84 time: 0.499s\n",
      "Epoch: 446 loss_train: 0.412 acc_train: 0.816 loss_test: 0.507 acc_test: 0.739 AUC = 0.85 time: 0.499s\n",
      "Epoch: 447 loss_train: 0.427 acc_train: 0.811 loss_test: 0.521 acc_test: 0.744 AUC = 0.83 time: 0.499s\n",
      "Epoch: 448 loss_train: 0.419 acc_train: 0.815 loss_test: 0.508 acc_test: 0.739 AUC = 0.84 time: 0.499s\n",
      "Epoch: 449 loss_train: 0.420 acc_train: 0.811 loss_test: 0.487 acc_test: 0.742 AUC = 0.87 time: 0.499s\n",
      "Epoch: 450 loss_train: 0.429 acc_train: 0.815 loss_test: 0.503 acc_test: 0.735 AUC = 0.84 time: 0.498s\n",
      "Epoch: 451 loss_train: 0.434 acc_train: 0.822 loss_test: 0.531 acc_test: 0.736 AUC = 0.83 time: 0.498s\n",
      "Epoch: 452 loss_train: 0.420 acc_train: 0.830 loss_test: 0.505 acc_test: 0.751 AUC = 0.85 time: 0.498s\n",
      "Epoch: 453 loss_train: 0.430 acc_train: 0.813 loss_test: 0.527 acc_test: 0.732 AUC = 0.83 time: 0.499s\n",
      "Epoch: 454 loss_train: 0.417 acc_train: 0.818 loss_test: 0.510 acc_test: 0.745 AUC = 0.83 time: 0.498s\n",
      "Epoch: 455 loss_train: 0.412 acc_train: 0.818 loss_test: 0.508 acc_test: 0.738 AUC = 0.85 time: 0.498s\n",
      "Epoch: 456 loss_train: 0.419 acc_train: 0.820 loss_test: 0.527 acc_test: 0.726 AUC = 0.83 time: 0.499s\n",
      "Epoch: 457 loss_train: 0.414 acc_train: 0.821 loss_test: 0.536 acc_test: 0.720 AUC = 0.82 time: 0.499s\n",
      "Epoch: 458 loss_train: 0.414 acc_train: 0.822 loss_test: 0.495 acc_test: 0.747 AUC = 0.85 time: 0.499s\n",
      "Epoch: 459 loss_train: 0.405 acc_train: 0.828 loss_test: 0.494 acc_test: 0.748 AUC = 0.85 time: 0.498s\n",
      "Epoch: 460 loss_train: 0.411 acc_train: 0.821 loss_test: 0.502 acc_test: 0.733 AUC = 0.85 time: 0.498s\n",
      "Epoch: 461 loss_train: 0.410 acc_train: 0.821 loss_test: 0.524 acc_test: 0.741 AUC = 0.83 time: 0.498s\n",
      "Epoch: 462 loss_train: 0.408 acc_train: 0.817 loss_test: 0.504 acc_test: 0.750 AUC = 0.85 time: 0.499s\n",
      "Epoch: 463 loss_train: 0.419 acc_train: 0.810 loss_test: 0.503 acc_test: 0.741 AUC = 0.84 time: 0.498s\n",
      "Epoch: 464 loss_train: 0.428 acc_train: 0.816 loss_test: 0.537 acc_test: 0.745 AUC = 0.82 time: 0.500s\n",
      "Epoch: 465 loss_train: 0.413 acc_train: 0.825 loss_test: 0.518 acc_test: 0.741 AUC = 0.84 time: 0.499s\n",
      "Epoch: 466 loss_train: 0.416 acc_train: 0.819 loss_test: 0.522 acc_test: 0.748 AUC = 0.84 time: 0.499s\n",
      "Epoch: 467 loss_train: 0.446 acc_train: 0.806 loss_test: 0.485 acc_test: 0.754 AUC = 0.85 time: 0.499s\n",
      "Epoch: 468 loss_train: 0.426 acc_train: 0.813 loss_test: 0.502 acc_test: 0.754 AUC = 0.85 time: 0.499s\n",
      "Epoch: 469 loss_train: 0.430 acc_train: 0.823 loss_test: 0.503 acc_test: 0.745 AUC = 0.85 time: 0.499s\n",
      "Epoch: 470 loss_train: 0.410 acc_train: 0.827 loss_test: 0.514 acc_test: 0.745 AUC = 0.84 time: 0.499s\n",
      "Epoch: 471 loss_train: 0.409 acc_train: 0.820 loss_test: 0.506 acc_test: 0.741 AUC = 0.84 time: 0.499s\n",
      "Epoch: 472 loss_train: 0.412 acc_train: 0.826 loss_test: 0.489 acc_test: 0.751 AUC = 0.86 time: 0.498s\n",
      "Epoch: 473 loss_train: 0.419 acc_train: 0.817 loss_test: 0.503 acc_test: 0.753 AUC = 0.85 time: 0.499s\n",
      "Epoch: 474 loss_train: 0.406 acc_train: 0.825 loss_test: 0.517 acc_test: 0.720 AUC = 0.83 time: 0.499s\n",
      "Epoch: 475 loss_train: 0.415 acc_train: 0.825 loss_test: 0.514 acc_test: 0.742 AUC = 0.85 time: 0.499s\n",
      "Epoch: 476 loss_train: 0.418 acc_train: 0.813 loss_test: 0.510 acc_test: 0.770 AUC = 0.85 time: 0.499s\n",
      "Epoch: 477 loss_train: 0.427 acc_train: 0.821 loss_test: 0.513 acc_test: 0.732 AUC = 0.84 time: 0.498s\n",
      "Epoch: 478 loss_train: 0.405 acc_train: 0.819 loss_test: 0.522 acc_test: 0.725 AUC = 0.84 time: 0.498s\n",
      "Epoch: 479 loss_train: 0.405 acc_train: 0.823 loss_test: 0.504 acc_test: 0.745 AUC = 0.84 time: 0.499s\n",
      "Epoch: 480 loss_train: 0.405 acc_train: 0.833 loss_test: 0.507 acc_test: 0.744 AUC = 0.84 time: 0.499s\n",
      "Epoch: 481 loss_train: 0.409 acc_train: 0.820 loss_test: 0.510 acc_test: 0.728 AUC = 0.84 time: 0.498s\n",
      "Epoch: 482 loss_train: 0.416 acc_train: 0.819 loss_test: 0.528 acc_test: 0.736 AUC = 0.83 time: 0.498s\n",
      "Epoch: 483 loss_train: 0.421 acc_train: 0.817 loss_test: 0.485 acc_test: 0.741 AUC = 0.86 time: 0.498s\n",
      "Epoch: 484 loss_train: 0.420 acc_train: 0.820 loss_test: 0.543 acc_test: 0.725 AUC = 0.83 time: 0.499s\n",
      "Epoch: 485 loss_train: 0.401 acc_train: 0.826 loss_test: 0.524 acc_test: 0.736 AUC = 0.84 time: 0.499s\n",
      "Epoch: 486 loss_train: 0.435 acc_train: 0.802 loss_test: 0.485 acc_test: 0.751 AUC = 0.85 time: 0.501s\n",
      "Epoch: 487 loss_train: 0.430 acc_train: 0.814 loss_test: 0.522 acc_test: 0.747 AUC = 0.84 time: 0.499s\n",
      "Epoch: 488 loss_train: 0.430 acc_train: 0.805 loss_test: 0.493 acc_test: 0.757 AUC = 0.86 time: 0.499s\n",
      "Epoch: 489 loss_train: 0.399 acc_train: 0.833 loss_test: 0.506 acc_test: 0.751 AUC = 0.84 time: 0.499s\n",
      "Epoch: 490 loss_train: 0.421 acc_train: 0.821 loss_test: 0.498 acc_test: 0.767 AUC = 0.86 time: 0.499s\n",
      "Epoch: 491 loss_train: 0.419 acc_train: 0.820 loss_test: 0.477 acc_test: 0.750 AUC = 0.86 time: 0.498s\n",
      "Epoch: 492 loss_train: 0.416 acc_train: 0.825 loss_test: 0.510 acc_test: 0.742 AUC = 0.84 time: 0.499s\n",
      "Epoch: 493 loss_train: 0.426 acc_train: 0.810 loss_test: 0.519 acc_test: 0.730 AUC = 0.83 time: 0.498s\n",
      "Epoch: 494 loss_train: 0.406 acc_train: 0.820 loss_test: 0.471 acc_test: 0.763 AUC = 0.86 time: 0.498s\n",
      "Epoch: 495 loss_train: 0.402 acc_train: 0.830 loss_test: 0.517 acc_test: 0.736 AUC = 0.84 time: 0.499s\n",
      "Epoch: 496 loss_train: 0.407 acc_train: 0.832 loss_test: 0.491 acc_test: 0.750 AUC = 0.85 time: 0.498s\n",
      "Epoch: 497 loss_train: 0.431 acc_train: 0.818 loss_test: 0.512 acc_test: 0.751 AUC = 0.84 time: 0.498s\n",
      "Epoch: 498 loss_train: 0.427 acc_train: 0.813 loss_test: 0.519 acc_test: 0.761 AUC = 0.84 time: 0.498s\n",
      "Epoch: 499 loss_train: 0.415 acc_train: 0.825 loss_test: 0.537 acc_test: 0.719 AUC = 0.83 time: 0.499s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 314.6223s\n",
      "Loading 448th epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-a4368b6b2731>:179: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(sv_path +'{}.pkl'.format(best_epoch)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.4425 accuracy= 0.7688 AUC = 0.89 False Alarm Rate = 0.3019 F1 Score = 0.6975\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import time\n",
    "import json\n",
    "from sklearn import metrics\n",
    "#from load import accuracy, load_multi_data\n",
    "#from models import FusionGAT3\n",
    "\n",
    "\n",
    "seed = 72\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output, l1_loss = model(features, adj_list_tt)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train]) + l1_loss*lambda_l1\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test]) + l1_loss*lambda_l1\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    y_prob = torch.exp(output[:, 1]).cpu().detach().numpy()\n",
    "\n",
    "    auc_score = roc_auc_score(labels[idx_test].cpu().detach().numpy(), y_prob[idx_test.cpu().detach().numpy()])\n",
    "    print('Epoch: {:03d}'.format(epoch+1),\n",
    "          'loss_train: {:.3f}'.format(loss_train.data.item()),\n",
    "          'acc_train: {:.3f}'.format(acc_train.data.item()),\n",
    "          'loss_test: {:.3f}'.format(loss_test.data.item()),\n",
    "          'acc_test: {:.3f}'.format(acc_test.data.item()),\n",
    "          \"AUC = {:.2f}\".format(auc_score),\n",
    "          'time: {:.3f}s'.format(time.time() - t))\n",
    "\n",
    "    return loss_test.data.item(), acc_test, auc_score\n",
    "\n",
    "def compute_test(model):\n",
    "    model.eval()\n",
    "    output, l1_loss = model(features, adj_list_tt)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test]) + l1_loss*lambda_l1\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    y_prob = torch.exp(output[:, 1]).cpu().detach().numpy()\n",
    "\n",
    "    auc_score = roc_auc_score(labels[idx_test].cpu().detach().numpy(), y_prob[idx_test.cpu().detach().numpy()])\n",
    "    # compute false alarm rate and f1 score\n",
    "    y_pred = np.array([0 if p < 0.5 else 1 for p in y_prob])\n",
    "    false_alarm_rate = np.mean(y_pred[idx_test.cpu().detach().numpy()] == 1)\n",
    "    f1score = f1_score(labels[idx_test].cpu().detach().numpy(), y_pred[idx_test.cpu().detach().numpy()])\n",
    "    precisionn, recalll, _ = precision_recall_curve(labels[idx_test].cpu().detach().numpy(), y_prob[idx_test.cpu().detach().numpy()])\n",
    "\n",
    "    pr_auc = metrics.auc(recalll, precisionn)\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.data.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.data.item()),\n",
    "          \"AUC = {:.2f}\".format(auc_score),\n",
    "          \"False Alarm Rate = {:.4f}\".format(false_alarm_rate),\n",
    "          \"F1 Score = {:.4f}\".format(f1score))\n",
    "    return loss_test.data.item(), acc_test.data.item(), auc_score, false_alarm_rate, f1score, y_pred, pr_auc\n",
    "\n",
    "\n",
    "#parameter setting\n",
    "#\n",
    "\n",
    "cuda = True\n",
    "epochs = 500\n",
    "#lrs = [0.0005, 0.001]\n",
    "#weight_decays = [5e-5, 5e-4]\n",
    "#dropouts = [0.3, 0.5]\n",
    "#hidden_dims1 = [64]\n",
    "#hidden_dims2 = [64]\n",
    "#fusion1_dims = [4]\n",
    "#nb_heads = [8, 12]\n",
    "#alpha = 0.2 #Alpha for the leaky_relu\n",
    "#lambda_l1s = [0.0001, 0.001, 0.01]\n",
    "#patience = 50\n",
    "\n",
    "lrs = [0.0005]\n",
    "weight_decays = [0.00005]\n",
    "dropouts = [0.3]\n",
    "hidden_dims1 = [16]\n",
    "hidden_dims2 = [16]\n",
    "fusion1_dims = [4]\n",
    "nb_heads = [8]\n",
    "alpha = 0.2 #Alpha for the leaky_relu\n",
    "lambda_l1s = [0.0001, 0.001, 0.01]\n",
    "patience = 50\n",
    "\n",
    "\n",
    "# Load data\n",
    "sv_path = \"\"\n",
    "folder = \"# Insert data folder pathway here\"\n",
    "att_name = \"# Insert main interaction attribute dataset name here\"\n",
    "edge_list_name = [\"# Insert all network edge list dataset names here\"]\n",
    "\n",
    "adj_list, features, labels, idx_train, idx_test = load_multi_data(folder, att_name, edge_list_name)\n",
    "\n",
    "adj_list_tensor = torch.stack(adj_list)\n",
    "features, adj_list_tt, labels = Variable(features), adj_list_tensor, Variable(labels)\n",
    "\n",
    "count = 0\n",
    "for lr in lrs:\n",
    "    for dropout in dropouts:\n",
    "        for hid1 in hidden_dims1:\n",
    "            for hid2 in hidden_dims2:\n",
    "                for fusion1_dim in fusion1_dims:\n",
    "                    for nb_head in nb_heads:\n",
    "                        for weight_decay in weight_decays:\n",
    "                            for lambda_l1 in lambda_l1s:\n",
    "\n",
    "                                # Model and optimizer\n",
    "                                model = FusionGAT3(nfeat=features.shape[1],\n",
    "                                            nhid1=hid1,\n",
    "                                            nhid2=hid2,\n",
    "                                            fusion1_dim = fusion1_dim,\n",
    "                                            nclass=int(labels.max()) + 1,\n",
    "                                            dropout=dropout,\n",
    "                                            alpha=alpha,\n",
    "                                            adj_list= adj_list_tt,\n",
    "                                            nheads = nb_head)\n",
    "\n",
    "                                optimizer = optim.Adam(model.parameters(),\n",
    "                                                    lr=lr,\n",
    "                                                    weight_decay=weight_decay)\n",
    "\n",
    "                                if cuda:\n",
    "                                    model.cuda()\n",
    "                                    features = features.cuda()\n",
    "                                    adj_list_tt = adj_list_tt.cuda()\n",
    "                                    labels = labels.cuda()\n",
    "                                    idx_train = torch.tensor(idx_train).cuda()\n",
    "                                    idx_test = torch.tensor(idx_test).cuda()\n",
    "\n",
    "\n",
    "                                # Train model\n",
    "                                t_total = time.time()\n",
    "                                bad_counter = 0\n",
    "                                best_auc = -1\n",
    "                                best_epoch = 0\n",
    "                                for epoch in range(epochs):\n",
    "                                    loss, acc, auc = train(epoch)\n",
    "\n",
    "                                    torch.save(model.state_dict(), sv_path + '{}.pkl'.format(epoch))\n",
    "                                    if auc > best_auc:\n",
    "                                        best_auc = auc\n",
    "                                        best_epoch = epoch\n",
    "                                        bad_counter = 0\n",
    "                                    else:\n",
    "                                        bad_counter += 1\n",
    "\n",
    "                                    if bad_counter == patience:\n",
    "                                        break\n",
    "\n",
    "                                files = glob.glob(sv_path +'*.pkl')\n",
    "                                for file in files:\n",
    "                                    filename = file.split('/')[-1]\n",
    "                                    epoch_nb = int(filename.split('.')[0])\n",
    "                                    if epoch_nb != best_epoch:\n",
    "                                        os.remove(file)\n",
    "\n",
    "                                print(\"Optimization Finished!\")\n",
    "                                print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "                                # Restore best model\n",
    "                                print('Loading {}th epoch'.format(best_epoch))\n",
    "                                model.load_state_dict(torch.load(sv_path +'{}.pkl'.format(best_epoch)))\n",
    "\n",
    "                                # Testing\n",
    "                                test_loss, test_acc, auc, far, f1, y_pred, pr_auc= compute_test(model)\n",
    "                                hyper_para = {}\n",
    "                                hyper_para[\"lr\"] = lr\n",
    "                                hyper_para[\"weight_decay\"] = weight_decay\n",
    "                                hyper_para[\"dropout\"] = dropout\n",
    "                                hyper_para[\"hidden_dim1\"] = hid1\n",
    "                                hyper_para[\"hidden_dim2\"] = hid2\n",
    "                                hyper_para[\"lambda\"] = lambda_l1\n",
    "                                hyper_para[\"fusion1_dim\"] = fusion1_dim\n",
    "                                hyper_para[\"nb_heads\"] = nb_head\n",
    "                                hyper_para[\"alpha\"] = alpha\n",
    "                                hyper_para[\"loss\"] = test_loss\n",
    "                                hyper_para[\"accuracy\"] = test_acc\n",
    "                                hyper_para[\"auc\"] = auc\n",
    "                                hyper_para[\"pr_auc\"] = pr_auc\n",
    "                                hyper_para[\"false_alarm_rate\"] = far\n",
    "                                hyper_para[\"f1_score\"] = f1\n",
    "                                with open(sv_path +\"hyperpara.json\", \"a+\") as fp:\n",
    "                                    fp.write('\\n')\n",
    "                                    json.dump(hyper_para, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYMtxiC6SVna"
   },
   "source": [
    "# Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLF-s-ZJwquB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import os\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def normalize_features(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    probs = torch.exp(output)\n",
    "    preds = torch.argmax(probs, dim = 1)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def load_multi_data(folder, att_file, edge_list_name):\n",
    "    print(\"loading data\")\n",
    "    att = pd.read_csv(folder + att_file)\n",
    "    edge_list = []\n",
    "    for name in edge_list_name:\n",
    "        edge_list.append(pd.read_csv(folder + name))\n",
    "\n",
    "    #get y and x\n",
    "    labels = np.array(att[\"# Insert outcome variable here\"])\n",
    "    features = sp.csr_matrix(att[[\"# Insert feature variables here\"]])\n",
    "    #features = normalize_features(features)\n",
    "\n",
    "    #get adj mat\n",
    "    adj_list = []\n",
    "    for edge in edge_list:\n",
    "        #get row col idx for adj matrix\n",
    "        row_idx = []\n",
    "        col_idx = []\n",
    "        for i in range(edge.shape[0]):\n",
    "            id_from = edge.iloc[i,0]\n",
    "            id_to = edge.iloc[i,1]\n",
    "            row_id = att.index[att[\"uid\"] == id_from]\n",
    "            row_idx.append(row_id[0])\n",
    "            col_id = att.index[att[\"uid\"] == id_to]\n",
    "            col_idx.append(col_id[0])\n",
    "\n",
    "        if edge.shape[1] == 2:\n",
    "            adj = sp.coo_matrix((np.ones(edge.shape[0]), (row_idx, col_idx)), shape=(att.shape[0], att.shape[0]), dtype=np.float32)\n",
    "\n",
    "        elif edge.shape[1] == 3:\n",
    "            adj = sp.coo_matrix((np.array(edge.iloc[:,2]), (row_idx, col_idx)), shape=(att.shape[0], att.shape[0]), dtype=np.float32)\n",
    "\n",
    "        #make adj symmetric\n",
    "        adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "        #normaliza adj\n",
    "        adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "        adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "        adj_list.append(adj)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    dim = len(labels)\n",
    "    idx_train = range(1585)\n",
    "    idx_test = range(1585, dim)\n",
    "\n",
    "    features, adj_list, labels = Variable(features), torch.stack(adj_list), Variable(labels)\n",
    "\n",
    "\n",
    "    data = MyDataset(adj_list, features, labels, idx_train, idx_test)\n",
    "    return data\n",
    "\n",
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, adj_ls, node_features, labels, idx_train, idx_test):\n",
    "        self.adj_ls = adj_ls\n",
    "        self.features = node_features\n",
    "        self.train_mask = idx_train\n",
    "        self.test_mask = idx_test\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        node_features = self.features[index]\n",
    "        labels = self.labels[index]\n",
    "        return self.adj_ls, node_features, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0VckMh_wqxB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"Dense version of GAT.\"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = nn.Linear(nhid * nheads, nclass)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x))\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class FusionGAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, adj_list, nheads):\n",
    "        super(FusionGAT, self).__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.nheads = nheads\n",
    "        self.adj_list = adj_list\n",
    "\n",
    "        # Define list of GAT layers for each adjacency matrix\n",
    "        self.attentions = nn.ModuleList()\n",
    "        for i in range(len(adj_list)):\n",
    "            att_list = nn.ModuleList([GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
    "            self.attentions.append(att_list)\n",
    "            for k, attention in enumerate(self.attentions):\n",
    "                self.add_module('adj{}, attention_{}'.format(i, k), attention)\n",
    "\n",
    "        # Define linear layer for integration with L1 regularization\n",
    "        self.integration_att = nn.Linear(nhid * nheads, nclass)\n",
    "        self.fusion = nn.Linear(nclass * len(adj_list), nclass)\n",
    "        self.l1_reg = nn.L1Loss(reduction='mean')\n",
    "\n",
    "    def forward(self, x, adj_list):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        # Compute output for each adjacency matrix using GAT layers\n",
    "        output_list = []\n",
    "        for i, adj in enumerate(adj_list):\n",
    "            x_i = torch.cat([att(x, adj) for att in self.attentions[i]], dim=1)\n",
    "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
    "            x_i = F.elu(self.integration_att(x_i))\n",
    "            output_list.append(x_i)\n",
    "        output = torch.cat(output_list, dim=1)\n",
    "\n",
    "        # Apply linear layer for integration with L1 regularization\n",
    "        output = F.dropout(output, self.dropout, training=self.training)\n",
    "        output = self.fusion(output.view(output.size(0), -1))\n",
    "        l1_loss = self.l1_reg(self.fusion.weight, torch.zeros_like(self.fusion.weight))\n",
    "        return F.log_softmax(output, dim=1), l1_loss\n",
    "\n",
    "\n",
    "\n",
    "class FusionGAT2(nn.Module):\n",
    "    def __init__(self, nfeat, nhid1, nhid2, nclass, dropout, alpha, adj_list, nheads):\n",
    "        super(FusionGAT2, self).__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.nheads = nheads\n",
    "        self.adj_list = adj_list\n",
    "\n",
    "        # Define list of GAT layers for each adjacency matrix in attention 1\n",
    "        self.attentions1 = nn.ModuleList()\n",
    "        for i in range(len(adj_list)):\n",
    "            att_list = nn.ModuleList([GraphAttentionLayer(nfeat, nhid1, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
    "            self.attentions1.append(att_list)\n",
    "            for k, attention in enumerate(self.attentions1):\n",
    "                self.add_module('adj{}, attention_layer1_{}'.format(i, k), attention)\n",
    "\n",
    "        # Define linear layer for integration of multihead attention1\n",
    "        self.integration_att1 = nn.Linear(nhid1 * nheads, nhid1)\n",
    "\n",
    "        self.attentions2 = nn.ModuleList()\n",
    "        for i in range(len(adj_list)):\n",
    "            att_list2 = nn.ModuleList([GraphAttentionLayer(nhid1, nhid2, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
    "            self.attentions2.append(att_list2)\n",
    "            for k, attention in enumerate(self.attentions2):\n",
    "                self.add_module('adj{}, attention_layer2_{}'.format(i, k), attention)\n",
    "\n",
    "        # Define linear layer for integration of multihead attention2\n",
    "        self.integration_att2 = nn.Linear(nhid2 * nheads, nclass)\n",
    "\n",
    "        #fusion layer with l1 penalty\n",
    "        self.fusion_att = nn.Linear(nclass * len(adj_list), nclass)\n",
    "        self.l1_reg = nn.L1Loss(reduction='mean')\n",
    "\n",
    "    def forward(self, x, adj_list):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        # Compute output for each adjacency matrix using GAT layers\n",
    "        output_list = []\n",
    "        for i, adj in enumerate(adj_list):\n",
    "            #attention layer 1\n",
    "            x_i = torch.cat([att(x, adj) for att in self.attentions1[i]], dim=1)\n",
    "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
    "            x_i = F.elu(self.integration_att1(x_i))\n",
    "\n",
    "            #attention layer 2\n",
    "            x_i = torch.cat([att(x_i, adj) for att in self.attentions2[i]], dim=1)\n",
    "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
    "            x_i = F.elu(self.integration_att2(x_i))\n",
    "            output_list.append(x_i)\n",
    "\n",
    "        output = torch.cat(output_list, dim=1)\n",
    "        output = F.dropout(output, self.dropout, training=self.training)\n",
    "        output = self.fusion_att(output.view(output.size(0), -1))\n",
    "        l1_loss = self.l1_reg(self.fusion_att.weight, torch.zeros_like(self.fusion_att.weight))\n",
    "        return F.log_softmax(output, dim=1), l1_loss\n",
    "\n",
    "\n",
    "\n",
    "class FusionGAT3(nn.Module):\n",
    "    def __init__(self, nfeat, nhid1, nhid2, fusion1_dim, nclass, dropout, alpha, adj_list, nheads):\n",
    "        super(FusionGAT3, self).__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.nheads = nheads\n",
    "        self.adj_list = adj_list\n",
    "\n",
    "        # Define list of GAT layers for each adjacency matrix\n",
    "        #att 1\n",
    "        self.attentions1 = nn.ModuleList()\n",
    "        for i in range(len(adj_list)):\n",
    "            att_list = nn.ModuleList([GraphAttentionLayer(nfeat, nhid1, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
    "            self.attentions1.append(att_list)\n",
    "            for k, attention in enumerate(self.attentions1):\n",
    "                self.add_module('adj{}, attention_layer1_{}'.format(i, k), attention)\n",
    "\n",
    "        # fusion1\n",
    "        self.integration_att1 = nn.Linear(nhid1 * nheads, fusion1_dim)\n",
    "        self.fusion_att1 = nn.Linear(fusion1_dim * len(adj_list), fusion1_dim)\n",
    "        self.l1_reg1 = nn.L1Loss(reduction='mean')\n",
    "\n",
    "        #att 2\n",
    "        self.attentions2 = nn.ModuleList()\n",
    "        for i in range(len(adj_list)):\n",
    "            att_list = nn.ModuleList([GraphAttentionLayer(fusion1_dim, nhid2, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
    "            self.attentions2.append(att_list)\n",
    "            for k, attention in enumerate(self.attentions2):\n",
    "                self.add_module('adj{}, attention_layer2_{}'.format(i, k), attention)\n",
    "\n",
    "        #fusion2\n",
    "        self.integration_att2 = nn.Linear(nhid2 * nheads, nclass)\n",
    "        self.fusion_att2 = nn.Linear(nclass * len(adj_list), nclass)\n",
    "        self.l1_reg2 = nn.L1Loss(reduction='mean')\n",
    "\n",
    "\n",
    "    def forward(self, x, adj_list):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "        # Compute output for each adjacency matrix using GAT layers\n",
    "        output_list = []\n",
    "        for i, adj in enumerate(adj_list):\n",
    "            x_i = x\n",
    "            x_i = torch.cat([att(x, adj) for att in self.attentions1[i]], dim=1)\n",
    "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
    "            x_i = F.elu(self.integration_att1(x_i))\n",
    "            output_list.append(x_i)\n",
    "        output = torch.cat(output_list, dim=1)\n",
    "\n",
    "        # Apply linear layer for integration with L1 regularization\n",
    "        output = F.dropout(output, self.dropout, training=self.training)\n",
    "        output = self.fusion_att1(output.view(output.size(0), -1))\n",
    "        l1_loss1 = self.l1_reg1(self.fusion_att1.weight, torch.zeros_like(self.fusion_att1.weight))\n",
    "\n",
    "\n",
    "        output_list2 = []\n",
    "        for i, adj in enumerate(adj_list):\n",
    "            x_i = torch.cat([att(output, adj) for att in self.attentions2[i]], dim=1)\n",
    "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
    "            x_i = F.elu(self.integration_att2(x_i))\n",
    "            output_list2.append(x_i)\n",
    "        output2 = torch.cat(output_list2, dim=1)\n",
    "\n",
    "        # Apply linear layer for integration with L1 regularization\n",
    "        output2 = F.dropout(output2, self.dropout, training=self.training)\n",
    "        output2 = self.fusion_att2(output2.view(output.size(0), -1))\n",
    "        l1_loss2 = self.l1_reg2(self.fusion_att2.weight, torch.zeros_like(self.fusion_att2.weight))\n",
    "\n",
    "        return F.log_softmax(output2, dim=1), l1_loss1 + l1_loss2\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4v4QkNXwqzp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "import math\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        Wh = torch.mm(h, self.W) # h.shape: (N, in_features), Wh.shape: (N, out_features)\n",
    "        e = self._prepare_attentional_mechanism_input(Wh)\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, Wh)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        # Wh.shape (N, out_feature)\n",
    "        # self.a.shape (2 * out_feature, 1)\n",
    "        # Wh1&2.shape (N, 1)\n",
    "        # e.shape (N, N)\n",
    "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])\n",
    "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])\n",
    "        # broadcast add\n",
    "        e = Wh1 + Wh2.T\n",
    "        return self.leakyrelu(e)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16256,
     "status": "ok",
     "timestamp": 1731092615858,
     "user": {
      "displayName": "Baode Gao",
      "userId": "00825733222134474059"
     },
     "user_tz": 360
    },
    "id": "MuIhyIpawvtt",
    "outputId": "70868989-7ec5-423a-b793-01fabed6d84c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-1e0519ce22f9>:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(sv_path+ model_name))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.explain.algorithm.utils import clear_masks, set_masks\n",
    "from torch_geometric.explain.config import ExplanationType, ModelMode, ModelTaskLevel\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "from torch_geometric.explain import Explainer, GNNExplainer, PGExplainer\n",
    "\n",
    "from torch_geometric.explain.config import ModelMode, ModelReturnType\n",
    "from torch_geometric.utils import get_embeddings\n",
    "\n",
    "seed = 72\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Parameter settings\n",
    "epochs = 500\n",
    "lr = 0.0005\n",
    "weight_decay = 5e-5\n",
    "dropout = 0.3\n",
    "hidden_dims1 = 16\n",
    "hidden_dims2 = 16\n",
    "fusion1_dim = 4\n",
    "nb_head = 8\n",
    "alpha = 0.2  # Alpha for the leaky_relu\n",
    "lambda_l1 =  0.0001\n",
    "patience = 50\n",
    "\n",
    "# Load data\n",
    "sv_path = \"\"\n",
    "folder = \"# Insert data folder pathway here\"\n",
    "att_name = \"# Insert main interaction attribute dataset name here\"\n",
    "edge_list_name = [\"# Insert all network edge list dataset names here\"]\n",
    "\n",
    "data = load_multi_data(folder, att_name, edge_list_name)\n",
    "idx_train, idx_test = data.train_mask, data.test_mask\n",
    "\n",
    "model = FusionGAT3(\n",
    "    nfeat=data.features.shape[1],\n",
    "    nhid1=hidden_dims1,\n",
    "    nhid2=hidden_dims2,\n",
    "    fusion1_dim=fusion1_dim,\n",
    "    nclass=int(data.labels.max()) + 1,\n",
    "    dropout=dropout,\n",
    "    alpha=alpha,\n",
    "    adj_list=data.adj_ls,\n",
    "    nheads=nb_head\n",
    ")\n",
    "\n",
    "\n",
    "model_name = \"428.pkl\"\n",
    "\n",
    "# Restore best model\n",
    "print('Loading model')\n",
    "model.load_state_dict(torch.load(sv_path+ model_name))\n",
    "\n",
    "\n",
    "\n",
    "from torch_geometric.explain import Explainer, GNNExplainer\n",
    "\n",
    "class ModifiedExplainer(Explainer):\n",
    "    def get_target(self, prediction):\n",
    "        # Modify the get_target method here\n",
    "        # You can access the prediction variable directly\n",
    "\n",
    "        # Perform necessary changes or customizations\n",
    "        if self.model_config.mode == ModelMode.binary_classification:\n",
    "            # TODO: Allow customization of the thresholds used below.\n",
    "            if self.model_config.return_type == ModelReturnType.raw:\n",
    "                return (prediction > 0).long().view(-1)\n",
    "            if self.model_config.return_type == ModelReturnType.probs:\n",
    "                return (prediction > 0.5).long().view(-1)\n",
    "            assert False\n",
    "\n",
    "        if self.model_config.mode == ModelMode.multiclass_classification:\n",
    "            return prediction[0].argmax(dim=-1)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "\n",
    "class ModifiedGNNExplainer(GNNExplainer):\n",
    "    def _train(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        x: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        *,\n",
    "        target: Tensor,\n",
    "        index: Optional[Union[int, Tensor]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self._initialize_masks(x, edge_index)\n",
    "\n",
    "        parameters = []\n",
    "        if self.node_mask is not None:\n",
    "            parameters.append(self.node_mask)\n",
    "        if self.edge_mask is not None:\n",
    "            set_masks(model, self.edge_mask, edge_index, apply_sigmoid=True)\n",
    "            parameters.append(self.edge_mask)\n",
    "\n",
    "        optimizer = torch.optim.Adam(parameters, lr=self.lr)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            h = x if self.node_mask is None else x * self.node_mask.sigmoid()\n",
    "            y_hat, y = model(h, edge_index, **kwargs)[0], target\n",
    "\n",
    "            if index is not None:\n",
    "                y_hat, y = y_hat[index], y[index]\n",
    "\n",
    "            loss = self._loss(y_hat, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # In the first iteration, we collect the nodes and edges that are\n",
    "            # involved into making the prediction. These are all the nodes and\n",
    "            # edges with gradient != 0 (without regularization applied).\n",
    "            if i == 0 and self.node_mask is not None:\n",
    "                if self.node_mask.grad is None:\n",
    "                    raise ValueError(\"Could not compute gradients for node \"\n",
    "                                     \"features. Please make sure that node \"\n",
    "                                     \"features are used inside the model or \"\n",
    "                                     \"disable it via `node_mask_type=None`.\")\n",
    "                self.hard_node_mask = self.node_mask.grad != 0.0\n",
    "            if i == 0 and self.edge_mask is not None:\n",
    "                if self.edge_mask.grad is None:\n",
    "                    raise ValueError(\"Could not compute gradients for edges. \"\n",
    "                                     \"Please make sure that edges are used \"\n",
    "                                     \"via message passing inside the model or \"\n",
    "                                     \"disable it via `edge_mask_type=None`.\")\n",
    "                self.hard_edge_mask = self.edge_mask.grad != 0.0\n",
    "\n",
    "\n",
    "# class ModifiedPGExplainer(GNNExplainer):\n",
    "#     def train(\n",
    "#         self,\n",
    "#         epoch: int,\n",
    "#         model: torch.nn.Module,\n",
    "#         x: Tensor,\n",
    "#         edge_index: Tensor,\n",
    "#         *,\n",
    "#         target: Tensor,\n",
    "#         index: Optional[Union[int, Tensor]] = None,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "\n",
    "#         if isinstance(x, dict) or isinstance(edge_index, dict):\n",
    "#             raise ValueError(f\"Heterogeneous graphs not yet supported in \"\n",
    "#                              f\"'{self.__class__.__name__}'\")\n",
    "\n",
    "#         if self.model_config.task_level == ModelTaskLevel.node:\n",
    "#             if index is None:\n",
    "#                 raise ValueError(f\"The 'index' argument needs to be provided \"\n",
    "#                                  f\"in '{self.__class__.__name__}' for \"\n",
    "#                                  f\"node-level explanations\")\n",
    "#             if isinstance(index, Tensor) and index.numel() > 1:\n",
    "#                 raise ValueError(f\"Only scalars are supported for the 'index' \"\n",
    "#                                  f\"argument in '{self.__class__.__name__}'\")\n",
    "\n",
    "#         z = get_embeddings(model, x, edge_index, **kwargs)[-1]\n",
    "\n",
    "#         self.optimizer.zero_grad()\n",
    "#         temperature = self._get_temperature(epoch)\n",
    "\n",
    "#         inputs = self._get_inputs(z, edge_index, index)\n",
    "#         logits = self.mlp(inputs).view(-1)\n",
    "#         edge_mask = self._concrete_sample(logits, temperature)\n",
    "#         set_masks(model, edge_mask, edge_index, apply_sigmoid=True)\n",
    "\n",
    "#         if self.model_config.task_level == ModelTaskLevel.node:\n",
    "#             _, hard_edge_mask = self._get_hard_masks(model, index, edge_index,\n",
    "#                                                      num_nodes=x.size(0))\n",
    "#             edge_mask = edge_mask[hard_edge_mask]\n",
    "\n",
    "#         y_hat, y = model(x, edge_index, **kwargs)[0], target\n",
    "\n",
    "#         if index is not None:\n",
    "#             y_hat, y = y_hat[index], y[index]\n",
    "\n",
    "#         loss = self._loss(y_hat, y, edge_mask)\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "\n",
    "#         clear_masks(model)\n",
    "#         self._curr_epoch = epoch\n",
    "\n",
    "#         return float(loss)\n",
    "\n",
    "\n",
    "##add explainer\n",
    "explainer = ModifiedExplainer(\n",
    "    model=model,\n",
    "    algorithm=ModifiedGNNExplainer(epochs=200),\n",
    "    explanation_type=\"model\",\n",
    "    node_mask_type='attributes',\n",
    "    model_config=dict(\n",
    "        mode='multiclass_classification',\n",
    "        task_level='node',\n",
    "        return_type='log_probs',\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 685586,
     "status": "ok",
     "timestamp": 1731093312341,
     "user": {
      "displayName": "Baode Gao",
      "userId": "00825733222134474059"
     },
     "user_tz": 360
    },
    "id": "-NfqrdBdw8la",
    "outputId": "3017d5c4-4da7-45f8-e14a-4779a0ca58ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated explanations in ['node_mask']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#target_label = data.labels\n",
    "explanation = explainer(data.features, data.adj_ls)#, target = target_label )\n",
    "print(f'Generated explanations in {explanation.available_explanations}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0byaHCovwvwP"
   },
   "outputs": [],
   "source": [
    "node_mask = explanation.node_mask\n",
    "np.save(sv_path+f\"node_mask_all.npy\", node_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1013,
     "status": "ok",
     "timestamp": 1731093696564,
     "user": {
      "displayName": "Baode Gao",
      "userId": "00825733222134474059"
     },
     "user_tz": 360
    },
    "id": "ryMYhQX7wvya",
    "outputId": "3568178d-2bfc-4bf3-85d7-71aa72e91655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance plot has been saved to 'feature_importance_5top428.png'\n"
     ]
    }
   ],
   "source": [
    "path = f'feature_importance_5top428.png'\n",
    "feat_label = [\"# Insert feature variables here\"]\n",
    "explanation.visualize_feature_importance(sv_path+path, feat_labels = feat_label, top_k=5)\n",
    "print(f\"Feature importance plot has been saved to '{path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-zakPPC8HND"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1ZiGJ_qNnTYh27de6To0G8x_I9r-mJcyk",
     "timestamp": 1733359708563
    },
    {
     "file_id": "1rRmYJcx_k7xG8N3tUVOLL-ZdlULoHVOD",
     "timestamp": 1732420927221
    },
    {
     "file_id": "1xqX9Lk65-CnAxIgLqBrlPocyckiLTGRC",
     "timestamp": 1731090055269
    },
    {
     "file_id": "1eIvxdd_Ls5LTemTSh7FFwUMzbBD4wuIZ",
     "timestamp": 1730145641572
    },
    {
     "file_id": "1SX0uaXILrGX1M4_l5qhfa33MyZUFKuBq",
     "timestamp": 1730144771525
    },
    {
     "file_id": "1CNiT3l78XqrBEcjBRorVetEcAMsfE2A3",
     "timestamp": 1727032185782
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLF-s-ZJwquB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import os\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def normalize_features(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    probs = torch.exp(output)\n",
    "    preds = torch.argmax(probs, dim = 1)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "\n",
    "def load_multi_data(folder, att_file, edge_list_name):\n",
    "    att = pd.read_csv(folder + att_file)\n",
    "    edge_list = []\n",
    "    for name in edge_list_name:\n",
    "        edge_list.append(pd.read_csv(folder + name))\n",
    "\n",
    "    #get y and x\n",
    "    labels = np.array(att[\"# Insert outcome variable here\"])\n",
    "    features = sp.csr_matrix(att[[\"# Insert feature variables here\"]])\n",
    "    #features = normalize_features(features)\n",
    "\n",
    "    #get adj mat\n",
    "    adj_list = []\n",
    "    for edge in edge_list:\n",
    "        #get row col idx for adj matrix\n",
    "        row_idx = []\n",
    "        col_idx = []\n",
    "        for i in range(edge.shape[0]):\n",
    "            id_from = edge.iloc[i,0]\n",
    "            id_to = edge.iloc[i,1]\n",
    "            # uid as unique identifier\n",
    "            if sum(att[\"uid\"] == id_from)>0:\n",
    "              row_id = att.index[att[\"uid\"] == id_from]\n",
    "              row_idx.append(row_id[0])\n",
    "\n",
    "            if sum(att[\"uid\"] == id_to)>0:\n",
    "              col_id = att.index[att[\"uid\"] == id_to]\n",
    "              col_idx.append(col_id[0])\n",
    "\n",
    "\n",
    "        adj = sp.coo_matrix((np.ones(edge.shape[0]), (row_idx, col_idx)), shape=(att.shape[0], att.shape[0]), dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "        #make adj symmetric\n",
    "        adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "        #normaliza adj\n",
    "        adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "        adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "        adj_list.append(adj)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    dim = len(labels)\n",
    "    idx_train = range(1915)\n",
    "    idx_test = range(1915, dim)\n",
    "\n",
    "    features, adj_list, labels = Variable(features), torch.stack(adj_list), Variable(labels)\n",
    "\n",
    "\n",
    "    data = MyDataset(adj_list, features, labels, idx_train, idx_test)\n",
    "    return data\n",
    "\n",
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, adj_ls, node_features, labels, idx_train, idx_test):\n",
    "        self.adj_ls = adj_ls\n",
    "        self.features = node_features\n",
    "        self.train_mask = idx_train\n",
    "        self.test_mask = idx_test\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        node_features = self.features[index]\n",
    "        labels = self.labels[index]\n",
    "        return self.adj_ls, node_features, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0VckMh_wqxB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"Dense version of GAT.\"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = nn.Linear(nhid * nheads, nclass)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x))\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class FusionGAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, adj_list, nheads):\n",
    "        super(FusionGAT, self).__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.nheads = nheads\n",
    "        self.adj_list = adj_list\n",
    "\n",
    "        # Define list of GAT layers for each adjacency matrix\n",
    "        self.attentions = nn.ModuleList()\n",
    "        for i in range(len(adj_list)):\n",
    "            att_list = nn.ModuleList([GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
    "            self.attentions.append(att_list)\n",
    "            for k, attention in enumerate(self.attentions):\n",
    "                self.add_module('adj{}, attention_{}'.format(i, k), attention)\n",
    "\n",
    "        # Define linear layer for integration with L1 regularization\n",
    "        self.integration_att = nn.Linear(nhid * nheads, nclass)\n",
    "        self.fusion = nn.Linear(nclass * len(adj_list), nclass)\n",
    "        self.l1_reg = nn.L1Loss(reduction='mean')\n",
    "\n",
    "    def forward(self, x, adj_list):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        # Compute output for each adjacency matrix using GAT layers\n",
    "        output_list = []\n",
    "        for i, adj in enumerate(adj_list):\n",
    "            x_i = torch.cat([att(x, adj) for att in self.attentions[i]], dim=1)\n",
    "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
    "            x_i = F.elu(self.integration_att(x_i))\n",
    "            output_list.append(x_i)\n",
    "        output = torch.cat(output_list, dim=1)\n",
    "\n",
    "        # Apply linear layer for integration with L1 regularization\n",
    "        output = F.dropout(output, self.dropout, training=self.training)\n",
    "        output = self.fusion(output.view(output.size(0), -1))\n",
    "        l1_loss = self.l1_reg(self.fusion.weight, torch.zeros_like(self.fusion.weight))\n",
    "        return F.log_softmax(output, dim=1), l1_loss\n",
    "\n",
    "\n",
    "\n",
    "class FusionGAT2(nn.Module):\n",
    "    def __init__(self, nfeat, nhid1, nhid2, nclass, dropout, alpha, adj_list, nheads):\n",
    "        super(FusionGAT2, self).__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.nheads = nheads\n",
    "        self.adj_list = adj_list\n",
    "\n",
    "        # Define list of GAT layers for each adjacency matrix in attention 1\n",
    "        self.attentions1 = nn.ModuleList()\n",
    "        for i in range(len(adj_list)):\n",
    "            att_list = nn.ModuleList([GraphAttentionLayer(nfeat, nhid1, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
    "            self.attentions1.append(att_list)\n",
    "            for k, attention in enumerate(self.attentions1):\n",
    "                self.add_module('adj{}, attention_layer1_{}'.format(i, k), attention)\n",
    "\n",
    "        # Define linear layer for integration of multihead attention1\n",
    "        self.integration_att1 = nn.Linear(nhid1 * nheads, nhid1)\n",
    "\n",
    "        self.attentions2 = nn.ModuleList()\n",
    "        for i in range(len(adj_list)):\n",
    "            att_list2 = nn.ModuleList([GraphAttentionLayer(nhid1, nhid2, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
    "            self.attentions2.append(att_list2)\n",
    "            for k, attention in enumerate(self.attentions2):\n",
    "                self.add_module('adj{}, attention_layer2_{}'.format(i, k), attention)\n",
    "\n",
    "        # Define linear layer for integration of multihead attention2\n",
    "        self.integration_att2 = nn.Linear(nhid2 * nheads, nclass)\n",
    "\n",
    "        #fusion layer with l1 penalty\n",
    "        self.fusion_att = nn.Linear(nclass * len(adj_list), nclass)\n",
    "        self.l1_reg = nn.L1Loss(reduction='mean')\n",
    "\n",
    "    def forward(self, x, adj_list):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        # Compute output for each adjacency matrix using GAT layers\n",
    "        output_list = []\n",
    "        for i, adj in enumerate(adj_list):\n",
    "            #attention layer 1\n",
    "            x_i = torch.cat([att(x, adj) for att in self.attentions1[i]], dim=1)\n",
    "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
    "            x_i = F.elu(self.integration_att1(x_i))\n",
    "\n",
    "            #attention layer 2\n",
    "            x_i = torch.cat([att(x_i, adj) for att in self.attentions2[i]], dim=1)\n",
    "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
    "            x_i = F.elu(self.integration_att2(x_i))\n",
    "            output_list.append(x_i)\n",
    "\n",
    "        output = torch.cat(output_list, dim=1)\n",
    "        output = F.dropout(output, self.dropout, training=self.training)\n",
    "        output = self.fusion_att(output.view(output.size(0), -1))\n",
    "        l1_loss = self.l1_reg(self.fusion_att.weight, torch.zeros_like(self.fusion_att.weight))\n",
    "        return F.log_softmax(output, dim=1), l1_loss\n",
    "\n",
    "\n",
    "\n",
    "class FusionGAT3(nn.Module):\n",
    "    def __init__(self, nfeat, nhid1, nhid2, fusion1_dim, nclass, dropout, alpha, adj_list, nheads):\n",
    "        super(FusionGAT3, self).__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.nheads = nheads\n",
    "        self.adj_list = adj_list\n",
    "\n",
    "        # Define list of GAT layers for each adjacency matrix\n",
    "        #att 1\n",
    "        self.attentions1 = nn.ModuleList()\n",
    "        for i in range(len(adj_list)):\n",
    "            att_list = nn.ModuleList([GraphAttentionLayer(nfeat, nhid1, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
    "            self.attentions1.append(att_list)\n",
    "            for k, attention in enumerate(self.attentions1):\n",
    "                self.add_module('adj{}, attention_layer1_{}'.format(i, k), attention)\n",
    "\n",
    "        # fusion1\n",
    "        self.integration_att1 = nn.Linear(nhid1 * nheads, fusion1_dim)\n",
    "        self.fusion_att1 = nn.Linear(fusion1_dim * len(adj_list), fusion1_dim)\n",
    "        self.l1_reg1 = nn.L1Loss(reduction='mean')\n",
    "\n",
    "        #att 2\n",
    "        self.attentions2 = nn.ModuleList()\n",
    "        for i in range(len(adj_list)):\n",
    "            att_list = nn.ModuleList([GraphAttentionLayer(fusion1_dim, nhid2, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
    "            self.attentions2.append(att_list)\n",
    "            for k, attention in enumerate(self.attentions2):\n",
    "                self.add_module('adj{}, attention_layer2_{}'.format(i, k), attention)\n",
    "\n",
    "        #fusion2\n",
    "        self.integration_att2 = nn.Linear(nhid2 * nheads, nclass)\n",
    "        self.fusion_att2 = nn.Linear(nclass * len(adj_list), nclass)\n",
    "        self.l1_reg2 = nn.L1Loss(reduction='mean')\n",
    "\n",
    "\n",
    "    def forward(self, x, adj_list):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "        # Compute output for each adjacency matrix using GAT layers\n",
    "        output_list = []\n",
    "        for i, adj in enumerate(adj_list):\n",
    "            x_i = x\n",
    "            x_i = torch.cat([att(x, adj) for att in self.attentions1[i]], dim=1)\n",
    "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
    "            x_i = F.elu(self.integration_att1(x_i))\n",
    "            output_list.append(x_i)\n",
    "        output = torch.cat(output_list, dim=1)\n",
    "\n",
    "        # Apply linear layer for integration with L1 regularization\n",
    "        output = F.dropout(output, self.dropout, training=self.training)\n",
    "        output = self.fusion_att1(output.view(output.size(0), -1))\n",
    "        l1_loss1 = self.l1_reg1(self.fusion_att1.weight, torch.zeros_like(self.fusion_att1.weight))\n",
    "\n",
    "\n",
    "        output_list2 = []\n",
    "        for i, adj in enumerate(adj_list):\n",
    "            x_i = torch.cat([att(output, adj) for att in self.attentions2[i]], dim=1)\n",
    "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
    "            x_i = F.elu(self.integration_att2(x_i))\n",
    "            output_list2.append(x_i)\n",
    "        output2 = torch.cat(output_list2, dim=1)\n",
    "\n",
    "        # Apply linear layer for integration with L1 regularization\n",
    "        output2 = F.dropout(output2, self.dropout, training=self.training)\n",
    "        output2 = self.fusion_att2(output2.view(output.size(0), -1))\n",
    "        l1_loss2 = self.l1_reg2(self.fusion_att2.weight, torch.zeros_like(self.fusion_att2.weight))\n",
    "\n",
    "        return F.log_softmax(output2, dim=1), l1_loss1 + l1_loss2\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4v4QkNXwqzp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "import math\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        Wh = torch.mm(h, self.W) # h.shape: (N, in_features), Wh.shape: (N, out_features)\n",
    "        e = self._prepare_attentional_mechanism_input(Wh)\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, Wh)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        # Wh.shape (N, out_feature)\n",
    "        # self.a.shape (2 * out_feature, 1)\n",
    "        # Wh1&2.shape (N, 1)\n",
    "        # e.shape (N, N)\n",
    "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])\n",
    "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])\n",
    "        # broadcast add\n",
    "        e = Wh1 + Wh2.T\n",
    "        return self.leakyrelu(e)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MuIhyIpawvtt",
    "outputId": "0836cd4d-eee8-44e8-a1b1-f158af45fac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch_geometric\n",
    "from torch_geometric.explain.algorithm.utils import clear_masks, set_masks\n",
    "from torch_geometric.explain.config import ExplanationType, ModelMode, ModelTaskLevel\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "from torch_geometric.explain import Explainer, GNNExplainer, PGExplainer\n",
    "\n",
    "from torch_geometric.explain.config import ModelMode, ModelReturnType\n",
    "from torch_geometric.utils import get_embeddings\n",
    "\n",
    "seed = 72\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Parameter settings\n",
    "epochs = 500\n",
    "lr = 0.0005\n",
    "weight_decay = 5e-5\n",
    "dropout = 0.3\n",
    "hidden_dims1 = 64\n",
    "hidden_dims2 = 64\n",
    "fusion1_dim = 4\n",
    "nb_head = 12\n",
    "alpha = 0.2  # Alpha for the leaky_relu\n",
    "lambda_l1 =  0.0001\n",
    "patience = 50\n",
    "\n",
    "\n",
    "sv_path = \"\"\n",
    "folder = \"# Insert data folder pathway here\"\n",
    "att_name = \"# Insert node attribute dataset name here\"\n",
    "\n",
    "edge_list_name = [\"# Insert sequence edge list dataset name here\", \n",
    "                  \"# Insert cl edge list dataset name here\"]\n",
    "\n",
    "data = load_multi_data(folder, att_name, edge_list_name)\n",
    "idx_train, idx_test = data.train_mask, data.test_mask\n",
    "\n",
    "model = FusionGAT3(\n",
    "    nfeat=data.features.shape[1],\n",
    "    nhid1=hidden_dims1,\n",
    "    nhid2=hidden_dims2,\n",
    "    fusion1_dim=fusion1_dim,\n",
    "    nclass=int(data.labels.max()) + 1,\n",
    "    dropout=dropout,\n",
    "    alpha=alpha,\n",
    "    adj_list=data.adj_ls,\n",
    "    nheads=nb_head\n",
    ")\n",
    "\n",
    "\n",
    "model_name = \"216.pkl\"\n",
    "\n",
    "# Restore best model\n",
    "print('Loading model')\n",
    "model.load_state_dict(torch.load(sv_path+ model_name))\n",
    "\n",
    "\n",
    "\n",
    "from torch_geometric.explain import Explainer, GNNExplainer\n",
    "\n",
    "class ModifiedExplainer(Explainer):\n",
    "    def get_target(self, prediction):\n",
    "        # Modify the get_target method here\n",
    "        # You can access the prediction variable directly\n",
    "\n",
    "        # Perform necessary changes or customizations\n",
    "        if self.model_config.mode == ModelMode.binary_classification:\n",
    "            # TODO: Allow customization of the thresholds used below.\n",
    "            if self.model_config.return_type == ModelReturnType.raw:\n",
    "                return (prediction > 0).long().view(-1)\n",
    "            if self.model_config.return_type == ModelReturnType.probs:\n",
    "                return (prediction > 0.5).long().view(-1)\n",
    "            assert False\n",
    "\n",
    "        if self.model_config.mode == ModelMode.multiclass_classification:\n",
    "            return prediction[0].argmax(dim=-1)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "\n",
    "class ModifiedGNNExplainer(GNNExplainer):\n",
    "    def _train(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        x: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        *,\n",
    "        target: Tensor,\n",
    "        index: Optional[Union[int, Tensor]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self._initialize_masks(x, edge_index)\n",
    "\n",
    "        parameters = []\n",
    "        if self.node_mask is not None:\n",
    "            parameters.append(self.node_mask)\n",
    "        if self.edge_mask is not None:\n",
    "            set_masks(model, self.edge_mask, edge_index, apply_sigmoid=True)\n",
    "            parameters.append(self.edge_mask)\n",
    "\n",
    "        optimizer = torch.optim.Adam(parameters, lr=self.lr)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            h = x if self.node_mask is None else x * self.node_mask.sigmoid()\n",
    "            y_hat, y = model(h, edge_index, **kwargs)[0], target\n",
    "\n",
    "            if index is not None:\n",
    "                y_hat, y = y_hat[index], y[index]\n",
    "\n",
    "            loss = self._loss(y_hat, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # In the first iteration, we collect the nodes and edges that are\n",
    "            # involved into making the prediction. These are all the nodes and\n",
    "            # edges with gradient != 0 (without regularization applied).\n",
    "            if i == 0 and self.node_mask is not None:\n",
    "                if self.node_mask.grad is None:\n",
    "                    raise ValueError(\"Could not compute gradients for node \"\n",
    "                                     \"features. Please make sure that node \"\n",
    "                                     \"features are used inside the model or \"\n",
    "                                     \"disable it via `node_mask_type=None`.\")\n",
    "                self.hard_node_mask = self.node_mask.grad != 0.0\n",
    "            if i == 0 and self.edge_mask is not None:\n",
    "                if self.edge_mask.grad is None:\n",
    "                    raise ValueError(\"Could not compute gradients for edges. \"\n",
    "                                     \"Please make sure that edges are used \"\n",
    "                                     \"via message passing inside the model or \"\n",
    "                                     \"disable it via `edge_mask_type=None`.\")\n",
    "                self.hard_edge_mask = self.edge_mask.grad != 0.0\n",
    "\n",
    "\n",
    "\n",
    "##add explainer\n",
    "explainer = ModifiedExplainer(\n",
    "    model=model,\n",
    "    algorithm=ModifiedGNNExplainer(epochs=200),\n",
    "    explanation_type=\"model\",\n",
    "    node_mask_type='attributes',\n",
    "    model_config=dict(\n",
    "        mode='multiclass_classification',\n",
    "        task_level='node',\n",
    "        return_type='log_probs',\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-NfqrdBdw8la",
    "outputId": "8e53b4e8-16f2-48df-9c18-3c0570069d01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated explanations in ['node_mask']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#target_label = data.labels\n",
    "explanation = explainer(data.features, data.adj_ls)#, target = target_label )\n",
    "print(f'Generated explanations in {explanation.available_explanations}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0byaHCovwvwP"
   },
   "outputs": [],
   "source": [
    "node_mask = explanation.node_mask\n",
    "np.save(sv_path+f\"node_mask_all.npy\", node_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ryMYhQX7wvya",
    "outputId": "9a797f40-f05c-4e8b-8b4d-e7896ff46301"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance plot has been saved to 'feature_importance_top22.png'\n"
     ]
    }
   ],
   "source": [
    "path = f'feature_importance_top22.png'\n",
    "feat_label = ['# Insert feature variables here']\n",
    "explanation.visualize_feature_importance(sv_path+path, feat_labels = feat_label, top_k=22)\n",
    "print(f\"Feature importance plot has been saved to '{path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-zakPPC8HND"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "br7MnyBAfVce"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "import math\n",
        "\n",
        "class GraphAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
        "    Corresponds to:\n",
        "    - Equation (1): Linear projection of input features\n",
        "    - Equation (2): Attention score between nodes i and j using shared weights\n",
        "    - Equation (3): Aggregation via softmax-normalized attention weights\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.alpha = alpha\n",
        "        self.concat = concat\n",
        "\n",
        "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
        "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, h, adj):\n",
        "        Wh = torch.mm(h, self.W) # h.shape: (N, in_features), Wh.shape: (N, out_features)\n",
        "        e = self._prepare_attentional_mechanism_input(Wh)\n",
        "\n",
        "        zero_vec = -9e15*torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "        h_prime = torch.matmul(attention, Wh)\n",
        "\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            return h_prime\n",
        "\n",
        "    def _prepare_attentional_mechanism_input(self, Wh):\n",
        "        # Wh.shape (N, out_feature)\n",
        "        # self.a.shape (2 * out_feature, 1)\n",
        "        # Wh1&2.shape (N, 1)\n",
        "        # e.shape (N, N)\n",
        "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])\n",
        "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])\n",
        "        # broadcast add\n",
        "        e = Wh1 + Wh2.T\n",
        "        return self.leakyrelu(e)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
        "\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qpSIrx7f_73"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import os\n",
        "\n",
        "def normalize_features(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "def normalize_adj(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
        "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
        "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    probs = torch.exp(output)\n",
        "    preds = torch.argmax(probs, dim = 1)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "def load_multi_data(folder, att_file, edge_list_name):\n",
        "    att = pd.read_csv(folder + att_file)\n",
        "    edge_list = []\n",
        "    for name in edge_list_name:\n",
        "        edge_list.append(pd.read_csv(folder + name))\n",
        "\n",
        "    #get y and x\n",
        "    labels = np.array(att[\"SARSCoV2\"])\n",
        "    # b,c,d are age groups\n",
        "    features = sp.csr_matrix(att[[\n",
        "       'race', 'birth_sex', 'hiv_aids_age_yrs',\n",
        "       'trans_categ', 'clusterSize',\n",
        "       'cd4_group', 'sqnetwork_exp', 'eigen',\n",
        "       'clustering_coef', 'degree', 'log10vl',\n",
        "       #'priority_clusters',# need numrical\n",
        "       'dyad',\n",
        "       'ncovid',\n",
        "       'pcd0', 'pcd1', 'pcd2',  'page0', 'page1',\n",
        "      'ptrans1', 'ptrans2', 'ptrans3', 'ptrans5']])\n",
        "    #features = normalize_features(features)\n",
        "\n",
        "    #get adj mat\n",
        "    adj_list = []\n",
        "    for edge in edge_list:\n",
        "        #get row col idx for adj matrix\n",
        "        row_idx = []\n",
        "        col_idx = []\n",
        "        for i in range(edge.shape[0]):\n",
        "            id_from = edge.iloc[i,0]\n",
        "            id_to = edge.iloc[i,1]\n",
        "            if sum(att[\"uid\"] == id_from)>0:\n",
        "              row_id = att.index[att[\"uid\"] == id_from]\n",
        "              row_idx.append(row_id[0])\n",
        "\n",
        "            if sum(att[\"uid\"] == id_to)>0:\n",
        "              col_id = att.index[att[\"uid\"] == id_to]\n",
        "              col_idx.append(col_id[0])\n",
        "\n",
        "        # Construct a weighted adjacency matrix using constant weights (value = 1.0)\n",
        "        # This is referred to as 'weighted coding' as it enables non-binary edge representation,\n",
        "        # even though all weights are initially the same.\n",
        "        adj = sp.coo_matrix((np.ones(edge.shape[0]), (row_idx, col_idx)), shape=(att.shape[0], att.shape[0]), dtype=np.float32)\n",
        "\n",
        "        #Applies symmetric normalization D^{-1/2}AD^{-1/2} for stability in GNN training\n",
        "        adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "        #normaliza adj\n",
        "        adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "        adj = torch.FloatTensor(np.array(adj.todense()))\n",
        "        adj_list.append(adj)\n",
        "\n",
        "    # Note: The adj_list contains the following adjacency matrices in order:\n",
        "    # adj_list[0] = P (personal contact network)\n",
        "    # adj_list[1] = H (household co-residency network)\n",
        "    # adj_list[2] = C (co-location network)\n",
        "    # adj_list[3-9] = S_k sublayers (site-type-specific co-location layers)\n",
        "\n",
        "\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(labels)\n",
        "\n",
        "    dim = len(labels)\n",
        "    idx_train = range(1915)\n",
        "    idx_test = range(1915, dim)\n",
        "\n",
        "    return adj_list, features, labels, idx_train, idx_test\n",
        "\n",
        "def load_data(folder, att_file, edge_name):\n",
        "    att = pd.read_csv(folder + att_file)\n",
        "    edge_list = pd.read_csv(folder + edge_name)\n",
        "\n",
        "    #get y and x\n",
        "    labels = np.array(att[\"SARSCoV2\"])\n",
        "    # b,c,d are age groups\n",
        "\n",
        "    features = sp.csr_matrix(att[[\n",
        "       'race', 'birth_sex', 'hiv_aids_age_yrs',\n",
        "       'trans_categ', 'clusterSize',\n",
        "       'cd4_group', 'sqnetwork_exp', 'eigen',\n",
        "       'clustering_coef', 'degree', 'log10vl',\n",
        "       #'priority_clusters',# need numrical\n",
        "       'dyad',\n",
        "       'ncovid',\n",
        "       'pcd0', 'pcd1', 'pcd2',  'page0', 'page1',\n",
        "      'ptrans1', 'ptrans2', 'ptrans3', 'ptrans5']])\n",
        "    #features = normalize_features(features)\n",
        "\n",
        "    #get adj mat\n",
        "    row_idx = []\n",
        "    col_idx = []\n",
        "    for i in range(edge_list.shape[0]):\n",
        "        id_from = edge_list.iloc[i,0]\n",
        "        id_to = edge_list.iloc[i,1]\n",
        "        if sum(att[\"uid\"] == id_from)>0:\n",
        "          row_id = att.index[att[\"uid\"] == id_from]\n",
        "          row_idx.append(row_id[0])\n",
        "\n",
        "        if sum(att[\"uid\"] == id_to)>0:\n",
        "          col_id = att.index[att[\"uid\"] == id_to]\n",
        "          col_idx.append(col_id[0])\n",
        "\n",
        "\n",
        "    adj = sp.coo_matrix((np.ones(edge_list.shape[0]), (row_idx, col_idx)), shape=(att.shape[0], att.shape[0]), dtype=np.float32)\n",
        "\n",
        "    # === Symmetrize the adjacency matrix (A = A ∪ A^T) ===\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "    # === Apply row normalization to address differences in network density ===\n",
        "    # This step corresponds to normalization of P, H, and C layers as described in the paper.\n",
        "    # These layers are treated as weighted graphs and normalized to stabilize training\n",
        "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
        "\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(labels)\n",
        "\n",
        "    dim = len(labels)\n",
        "    idx_train = range(1915)\n",
        "    idx_test = range(1915, dim)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5auJI3dgKKO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
        "        \"\"\"Dense version of GAT.\"\"\"\n",
        "        super(GAT, self).__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
        "        for i, attention in enumerate(self.attentions):\n",
        "            self.add_module('attention_{}'.format(i), attention)\n",
        "\n",
        "        self.out_att = nn.Linear(nhid * nheads, nclass)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        ## Equation (4): Concatenate K multi-head outputs and apply linear projection W^O\n",
        "        x = F.elu(self.out_att(x))\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "class FusionGAT(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, adj_list, nheads):\n",
        "        super(FusionGAT, self).__init__()\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.nheads = nheads\n",
        "        self.adj_list = adj_list\n",
        "\n",
        "        # Define list of GAT layers for each adjacency matrix\n",
        "        self.attentions = nn.ModuleList()\n",
        "        for i in range(len(adj_list)):\n",
        "            att_list = nn.ModuleList([GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
        "            self.attentions.append(att_list)\n",
        "            for k, attention in enumerate(self.attentions):\n",
        "                self.add_module('adj{}, attention_{}'.format(i, k), attention)\n",
        "\n",
        "        # Define linear layer for integration with L1 regularization\n",
        "        self.integration_att = nn.Linear(nhid * nheads, nclass)\n",
        "        self.fusion = nn.Linear(nclass * len(adj_list), nclass)\n",
        "        self.l1_reg = nn.L1Loss(reduction='mean')\n",
        "\n",
        "    def forward(self, x, adj_list):\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        # Compute output for each adjacency matrix using GAT layers\n",
        "        output_list = []\n",
        "        for i, adj in enumerate(adj_list):          \n",
        "            x_i = torch.cat([att(x, adj) for att in self.attentions[i]], dim=1)\n",
        "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
        "            x_i = F.elu(self.integration_att(x_i))\n",
        "            output_list.append(x_i)\n",
        "        output = torch.cat(output_list, dim=1)\n",
        "\n",
        "        # Apply linear layer for integration with L1 regularization\n",
        "        output = F.dropout(output, self.dropout, training=self.training)\n",
        "        output = self.fusion(output.view(output.size(0), -1))\n",
        "        l1_loss = self.l1_reg(self.fusion.weight, torch.zeros_like(self.fusion.weight))\n",
        "        return F.log_softmax(output, dim=1), l1_loss\n",
        "\n",
        "\n",
        "\n",
        "class FusionGAT2(nn.Module):\n",
        "    def __init__(self, nfeat, nhid1, nhid2, nclass, dropout, alpha, adj_list, nheads):\n",
        "        super(FusionGAT2, self).__init__()\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.nheads = nheads\n",
        "        self.adj_list = adj_list\n",
        "\n",
        "        # Define list of GAT layers for each adjacency matrix in attention 1\n",
        "        self.attentions1 = nn.ModuleList()\n",
        "        for i in range(len(adj_list)):\n",
        "            att_list = nn.ModuleList([GraphAttentionLayer(nfeat, nhid1, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
        "            self.attentions1.append(att_list)\n",
        "            for k, attention in enumerate(self.attentions1):\n",
        "                self.add_module('adj{}, attention_layer1_{}'.format(i, k), attention)\n",
        "\n",
        "        # Define linear layer for integration of multihead attention1\n",
        "        self.integration_att1 = nn.Linear(nhid1 * nheads, nhid1)\n",
        "\n",
        "        self.attentions2 = nn.ModuleList()\n",
        "        for i in range(len(adj_list)):\n",
        "            att_list2 = nn.ModuleList([GraphAttentionLayer(nhid1, nhid2, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
        "            self.attentions2.append(att_list2)\n",
        "            for k, attention in enumerate(self.attentions2):\n",
        "                self.add_module('adj{}, attention_layer2_{}'.format(i, k), attention)\n",
        "\n",
        "        # Define linear layer for integration of multihead attention2\n",
        "        self.integration_att2 = nn.Linear(nhid2 * nheads, nclass)\n",
        "\n",
        "        # Fusion layer: dense (fully connected) layer integrating concatenated GAT outputs from all layer with L1 Penalty\n",
        "        self.fusion_att = nn.Linear(nclass * len(adj_list), nclass)\n",
        "        # L1 regularization encourages sparsity by penalizing non-zero weights in fusion layers\n",
        "        self.l1_reg = nn.L1Loss(reduction='mean')\n",
        "\n",
        "    def forward(self, x, adj_list):\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        # Compute output for each adjacency matrix using GAT layers\n",
        "        output_list = []\n",
        "        for i, adj in enumerate(adj_list):\n",
        "            #attention layer 1\n",
        "            x_i = torch.cat([att(x, adj) for att in self.attentions1[i]], dim=1)\n",
        "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
        "            x_i = F.elu(self.integration_att1(x_i))\n",
        "\n",
        "            # === Equation (7): Fusion of H_concat^(1) → H_fused^(1) ===\n",
        "            # This is done implicitly after all x_i from layers are collected into output_list\n",
        "\n",
        "            # === Equation (8): Apply GAT_2 to fused representation H_fused^(1) ===\n",
        "            #attention layer 2\n",
        "            x_i = torch.cat([att(x_i, adj) for att in self.attentions2[i]], dim=1)\n",
        "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
        "            x_i = F.elu(self.integration_att2(x_i))\n",
        "            output_list.append(x_i)\n",
        "\n",
        "        output = torch.cat(output_list, dim=1)\n",
        "        output = F.dropout(output, self.dropout, training=self.training)\n",
        "        output = self.fusion_att(output.view(output.size(0), -1))\n",
        "        l1_loss = self.l1_reg(self.fusion_att.weight, torch.zeros_like(self.fusion_att.weight))\n",
        "        return F.log_softmax(output, dim=1), l1_loss\n",
        "\n",
        "\n",
        "\n",
        "class FusionGAT3(nn.Module):\n",
        "    def __init__(self, nfeat, nhid1, nhid2, fusion1_dim, nclass, dropout, alpha, adj_list, nheads):\n",
        "        super(FusionGAT3, self).__init__()\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.nheads = nheads\n",
        "        self.adj_list = adj_list\n",
        "\n",
        "        # Define list of GAT layers for each adjacency matrix\n",
        "        #att 1\n",
        "        self.attentions1 = nn.ModuleList()\n",
        "        for i in range(len(adj_list)):\n",
        "            att_list = nn.ModuleList([GraphAttentionLayer(nfeat, nhid1, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
        "            self.attentions1.append(att_list)\n",
        "            for k, attention in enumerate(self.attentions1):\n",
        "                self.add_module('adj{}, attention_layer1_{}'.format(i, k), attention)\n",
        "\n",
        "        # fusion1\n",
        "        self.integration_att1 = nn.Linear(nhid1 * nheads, fusion1_dim)\n",
        "        self.fusion_att1 = nn.Linear(fusion1_dim * len(adj_list), fusion1_dim)\n",
        "        self.l1_reg1 = nn.L1Loss(reduction='mean')\n",
        "\n",
        "        #att 2\n",
        "        self.attentions2 = nn.ModuleList()\n",
        "        for i in range(len(adj_list)):\n",
        "            att_list = nn.ModuleList([GraphAttentionLayer(fusion1_dim, nhid2, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
        "            self.attentions2.append(att_list)\n",
        "            for k, attention in enumerate(self.attentions2):\n",
        "                self.add_module('adj{}, attention_layer2_{}'.format(i, k), attention)\n",
        "\n",
        "        #fusion2\n",
        "        self.integration_att2 = nn.Linear(nhid2 * nheads, nclass)\n",
        "        self.fusion_att2 = nn.Linear(nclass * len(adj_list), nclass)\n",
        "        self.l1_reg2 = nn.L1Loss(reduction='mean')\n",
        "\n",
        "\n",
        "    def forward(self, x, adj_list):\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "\n",
        "        # Compute output for each adjacency matrix using GAT layers\n",
        "        output_list = []\n",
        "        for i, adj in enumerate(adj_list):\n",
        "            # Equation (6): Apply multi-head GAT (Layer 1) independently on each adjacency matrix A ∈ {P, H, C, S^(k)}\n",
        "            # Each attention head computes attention coefficients α_ij over neighbors and aggregates using W^(1)\n",
        "            # Resulting head outputs are concatenated into H_A^(1)\n",
        "            x_i = x\n",
        "            x_i = torch.cat([att(x, adj) for att in self.attentions1[i]], dim=1)\n",
        "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
        "            x_i = F.elu(self.integration_att1(x_i))\n",
        "            output_list.append(x_i)\n",
        "        # Equation (5): Concatenate embeddings from L layers into unified vector c_i    \n",
        "        output = torch.cat(output_list, dim=1)\n",
        "\n",
        "        # Apply linear layer for integration with L1 regularization\n",
        "        # === Equation (9): First L1-regularized fusion layer ===\n",
        "        # H_fused^(1) = ReLU(Linear_fuse1(H_concat^(1))) with L1 penalty on W_fuse1\n",
        "        output = F.dropout(output, self.dropout, training=self.training)\n",
        "        output = self.fusion_att1(output.view(output.size(0), -1))\n",
        "        l1_loss1 = self.l1_reg1(self.fusion_att1.weight, torch.zeros_like(self.fusion_att1.weight))\n",
        "\n",
        "        # === Equation (10): GAT_2 per layer, using H_fused^(1) as input ===\n",
        "        # H_A^(2) = GAT_2(H_fused^(1), A)\n",
        "        output_list2 = []\n",
        "        for i, adj in enumerate(adj_list):\n",
        "            x_i = torch.cat([att(output, adj) for att in self.attentions2[i]], dim=1)\n",
        "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
        "            x_i = F.elu(self.integration_att2(x_i))\n",
        "            output_list2.append(x_i)\n",
        "        output2 = torch.cat(output_list2, dim=1)\n",
        "\n",
        "        # Apply linear layer for integration with L1 regularization\n",
        "        # === Equation (11): Second L1-regularized fusion layer ===\n",
        "        # H_fused^(2) = ReLU(Linear_fuse2(H_concat^(2))) with L1 penalty on W_fuse2\n",
        "        output2 = F.dropout(output2, self.dropout, training=self.training)\n",
        "        output2 = self.fusion_att2(output2.view(output.size(0), -1))\n",
        "        l1_loss2 = self.l1_reg2(self.fusion_att2.weight, torch.zeros_like(self.fusion_att2.weight))\n",
        "\n",
        "        return F.log_softmax(output2, dim=1), l1_loss1 + l1_loss2\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    # Standard 2-layer GCN model implementation based on Kipf & Welling (2016)\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, adj)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH-pVdeaS0Yf"
      },
      "source": [
        "# GCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmct_H8lgNms",
        "outputId": "46184e14-f63b-4080-dadd-a972c1c18aab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001 loss_train: 1.336 acc_train: 0.919 loss_test: 1.487 acc_test: 0.918 AUC = 0.47 time: 1.009s\n",
            "Epoch: 002 loss_train: 1.250 acc_train: 0.919 loss_test: 1.392 acc_test: 0.916 AUC = 0.45 time: 0.006s\n",
            "Epoch: 003 loss_train: 1.221 acc_train: 0.916 loss_test: 1.296 acc_test: 0.916 AUC = 0.43 time: 0.005s\n",
            "Epoch: 004 loss_train: 1.078 acc_train: 0.912 loss_test: 1.218 acc_test: 0.909 AUC = 0.46 time: 0.006s\n",
            "Epoch: 005 loss_train: 1.030 acc_train: 0.909 loss_test: 1.113 acc_test: 0.904 AUC = 0.43 time: 0.004s\n",
            "Epoch: 006 loss_train: 0.949 acc_train: 0.907 loss_test: 0.991 acc_test: 0.917 AUC = 0.50 time: 0.004s\n",
            "Epoch: 007 loss_train: 0.798 acc_train: 0.905 loss_test: 0.933 acc_test: 0.899 AUC = 0.49 time: 0.004s\n",
            "Epoch: 008 loss_train: 0.731 acc_train: 0.889 loss_test: 0.949 acc_test: 0.877 AUC = 0.44 time: 0.004s\n",
            "Epoch: 009 loss_train: 0.738 acc_train: 0.886 loss_test: 0.738 acc_test: 0.885 AUC = 0.52 time: 0.006s\n",
            "Epoch: 010 loss_train: 0.708 acc_train: 0.844 loss_test: 0.773 acc_test: 0.844 AUC = 0.49 time: 0.006s\n",
            "Epoch: 011 loss_train: 0.739 acc_train: 0.856 loss_test: 0.802 acc_test: 0.851 AUC = 0.45 time: 0.004s\n",
            "Epoch: 012 loss_train: 0.737 acc_train: 0.817 loss_test: 0.822 acc_test: 0.818 AUC = 0.46 time: 0.004s\n",
            "Epoch: 013 loss_train: 0.701 acc_train: 0.805 loss_test: 0.861 acc_test: 0.773 AUC = 0.50 time: 0.004s\n",
            "Epoch: 014 loss_train: 0.821 acc_train: 0.762 loss_test: 0.779 acc_test: 0.762 AUC = 0.53 time: 0.005s\n",
            "Epoch: 015 loss_train: 0.769 acc_train: 0.767 loss_test: 0.632 acc_test: 0.807 AUC = 0.57 time: 0.004s\n",
            "Epoch: 016 loss_train: 0.719 acc_train: 0.781 loss_test: 0.883 acc_test: 0.761 AUC = 0.53 time: 0.004s\n",
            "Epoch: 017 loss_train: 0.717 acc_train: 0.809 loss_test: 0.729 acc_test: 0.798 AUC = 0.54 time: 0.004s\n",
            "Epoch: 018 loss_train: 0.631 acc_train: 0.844 loss_test: 0.778 acc_test: 0.835 AUC = 0.44 time: 0.004s\n",
            "Epoch: 019 loss_train: 0.665 acc_train: 0.827 loss_test: 0.795 acc_test: 0.844 AUC = 0.48 time: 0.004s\n",
            "Epoch: 020 loss_train: 0.714 acc_train: 0.837 loss_test: 0.632 acc_test: 0.857 AUC = 0.55 time: 0.004s\n",
            "Epoch: 021 loss_train: 0.620 acc_train: 0.872 loss_test: 0.704 acc_test: 0.872 AUC = 0.50 time: 0.004s\n",
            "Epoch: 022 loss_train: 0.664 acc_train: 0.885 loss_test: 0.721 acc_test: 0.883 AUC = 0.52 time: 0.004s\n",
            "Epoch: 023 loss_train: 0.678 acc_train: 0.868 loss_test: 0.855 acc_test: 0.859 AUC = 0.49 time: 0.004s\n",
            "Epoch: 024 loss_train: 0.592 acc_train: 0.890 loss_test: 0.718 acc_test: 0.884 AUC = 0.51 time: 0.004s\n",
            "Epoch: 025 loss_train: 0.636 acc_train: 0.893 loss_test: 0.723 acc_test: 0.899 AUC = 0.49 time: 0.004s\n",
            "Epoch: 026 loss_train: 0.626 acc_train: 0.900 loss_test: 0.685 acc_test: 0.887 AUC = 0.54 time: 0.004s\n",
            "Epoch: 027 loss_train: 0.673 acc_train: 0.898 loss_test: 0.737 acc_test: 0.893 AUC = 0.47 time: 0.005s\n",
            "Epoch: 028 loss_train: 0.619 acc_train: 0.894 loss_test: 0.693 acc_test: 0.905 AUC = 0.51 time: 0.004s\n",
            "Epoch: 029 loss_train: 0.618 acc_train: 0.889 loss_test: 0.771 acc_test: 0.884 AUC = 0.50 time: 0.004s\n",
            "Epoch: 030 loss_train: 0.571 acc_train: 0.892 loss_test: 0.729 acc_test: 0.878 AUC = 0.49 time: 0.004s\n",
            "Epoch: 031 loss_train: 0.570 acc_train: 0.887 loss_test: 0.677 acc_test: 0.887 AUC = 0.53 time: 0.004s\n",
            "Epoch: 032 loss_train: 0.588 acc_train: 0.876 loss_test: 0.695 acc_test: 0.867 AUC = 0.53 time: 0.004s\n",
            "Epoch: 033 loss_train: 0.567 acc_train: 0.883 loss_test: 0.694 acc_test: 0.884 AUC = 0.46 time: 0.004s\n",
            "Epoch: 034 loss_train: 0.567 acc_train: 0.857 loss_test: 0.699 acc_test: 0.862 AUC = 0.49 time: 0.004s\n",
            "Epoch: 035 loss_train: 0.565 acc_train: 0.870 loss_test: 0.642 acc_test: 0.880 AUC = 0.49 time: 0.004s\n",
            "Epoch: 036 loss_train: 0.597 acc_train: 0.868 loss_test: 0.569 acc_test: 0.873 AUC = 0.57 time: 0.004s\n",
            "Epoch: 037 loss_train: 0.628 acc_train: 0.859 loss_test: 0.601 acc_test: 0.880 AUC = 0.53 time: 0.005s\n",
            "Epoch: 038 loss_train: 0.515 acc_train: 0.873 loss_test: 0.646 acc_test: 0.865 AUC = 0.51 time: 0.004s\n",
            "Epoch: 039 loss_train: 0.528 acc_train: 0.863 loss_test: 0.641 acc_test: 0.843 AUC = 0.55 time: 0.005s\n",
            "Epoch: 040 loss_train: 0.616 acc_train: 0.836 loss_test: 0.589 acc_test: 0.846 AUC = 0.58 time: 0.005s\n",
            "Epoch: 041 loss_train: 0.523 acc_train: 0.857 loss_test: 0.550 acc_test: 0.872 AUC = 0.58 time: 0.005s\n",
            "Epoch: 042 loss_train: 0.549 acc_train: 0.862 loss_test: 0.604 acc_test: 0.871 AUC = 0.52 time: 0.004s\n",
            "Epoch: 043 loss_train: 0.552 acc_train: 0.864 loss_test: 0.705 acc_test: 0.888 AUC = 0.43 time: 0.004s\n",
            "Epoch: 044 loss_train: 0.494 acc_train: 0.880 loss_test: 0.623 acc_test: 0.874 AUC = 0.51 time: 0.004s\n",
            "Epoch: 045 loss_train: 0.552 acc_train: 0.884 loss_test: 0.661 acc_test: 0.872 AUC = 0.56 time: 0.004s\n",
            "Epoch: 046 loss_train: 0.549 acc_train: 0.875 loss_test: 0.631 acc_test: 0.894 AUC = 0.50 time: 0.004s\n",
            "Epoch: 047 loss_train: 0.525 acc_train: 0.895 loss_test: 0.591 acc_test: 0.890 AUC = 0.51 time: 0.004s\n",
            "Epoch: 048 loss_train: 0.519 acc_train: 0.885 loss_test: 0.587 acc_test: 0.889 AUC = 0.55 time: 0.004s\n",
            "Epoch: 049 loss_train: 0.510 acc_train: 0.887 loss_test: 0.600 acc_test: 0.888 AUC = 0.54 time: 0.004s\n",
            "Epoch: 050 loss_train: 0.526 acc_train: 0.877 loss_test: 0.628 acc_test: 0.883 AUC = 0.54 time: 0.004s\n",
            "Epoch: 051 loss_train: 0.519 acc_train: 0.880 loss_test: 0.556 acc_test: 0.882 AUC = 0.53 time: 0.004s\n",
            "Epoch: 052 loss_train: 0.511 acc_train: 0.882 loss_test: 0.545 acc_test: 0.901 AUC = 0.53 time: 0.004s\n",
            "Epoch: 053 loss_train: 0.466 acc_train: 0.889 loss_test: 0.519 acc_test: 0.891 AUC = 0.56 time: 0.004s\n",
            "Epoch: 054 loss_train: 0.495 acc_train: 0.872 loss_test: 0.569 acc_test: 0.885 AUC = 0.55 time: 0.004s\n",
            "Epoch: 055 loss_train: 0.504 acc_train: 0.868 loss_test: 0.591 acc_test: 0.890 AUC = 0.55 time: 0.004s\n",
            "Epoch: 056 loss_train: 0.514 acc_train: 0.862 loss_test: 0.533 acc_test: 0.876 AUC = 0.56 time: 0.004s\n",
            "Epoch: 057 loss_train: 0.492 acc_train: 0.884 loss_test: 0.567 acc_test: 0.887 AUC = 0.52 time: 0.005s\n",
            "Epoch: 058 loss_train: 0.516 acc_train: 0.871 loss_test: 0.535 acc_test: 0.882 AUC = 0.55 time: 0.004s\n",
            "Epoch: 059 loss_train: 0.463 acc_train: 0.869 loss_test: 0.520 acc_test: 0.883 AUC = 0.55 time: 0.004s\n",
            "Epoch: 060 loss_train: 0.499 acc_train: 0.870 loss_test: 0.571 acc_test: 0.883 AUC = 0.54 time: 0.004s\n",
            "Epoch: 061 loss_train: 0.457 acc_train: 0.865 loss_test: 0.547 acc_test: 0.887 AUC = 0.51 time: 0.004s\n",
            "Epoch: 062 loss_train: 0.481 acc_train: 0.855 loss_test: 0.592 acc_test: 0.867 AUC = 0.51 time: 0.004s\n",
            "Epoch: 063 loss_train: 0.484 acc_train: 0.889 loss_test: 0.507 acc_test: 0.877 AUC = 0.59 time: 0.004s\n",
            "Epoch: 064 loss_train: 0.456 acc_train: 0.883 loss_test: 0.546 acc_test: 0.876 AUC = 0.54 time: 0.004s\n",
            "Epoch: 065 loss_train: 0.408 acc_train: 0.885 loss_test: 0.500 acc_test: 0.878 AUC = 0.58 time: 0.004s\n",
            "Epoch: 066 loss_train: 0.466 acc_train: 0.879 loss_test: 0.520 acc_test: 0.878 AUC = 0.54 time: 0.004s\n",
            "Epoch: 067 loss_train: 0.444 acc_train: 0.882 loss_test: 0.519 acc_test: 0.874 AUC = 0.56 time: 0.004s\n",
            "Epoch: 068 loss_train: 0.453 acc_train: 0.891 loss_test: 0.520 acc_test: 0.889 AUC = 0.54 time: 0.004s\n",
            "Epoch: 069 loss_train: 0.453 acc_train: 0.892 loss_test: 0.536 acc_test: 0.904 AUC = 0.52 time: 0.005s\n",
            "Epoch: 070 loss_train: 0.424 acc_train: 0.885 loss_test: 0.514 acc_test: 0.884 AUC = 0.55 time: 0.004s\n",
            "Epoch: 071 loss_train: 0.443 acc_train: 0.878 loss_test: 0.510 acc_test: 0.880 AUC = 0.56 time: 0.004s\n",
            "Epoch: 072 loss_train: 0.416 acc_train: 0.879 loss_test: 0.456 acc_test: 0.885 AUC = 0.58 time: 0.004s\n",
            "Epoch: 073 loss_train: 0.418 acc_train: 0.872 loss_test: 0.503 acc_test: 0.880 AUC = 0.57 time: 0.004s\n",
            "Epoch: 074 loss_train: 0.400 acc_train: 0.890 loss_test: 0.502 acc_test: 0.880 AUC = 0.54 time: 0.004s\n",
            "Epoch: 075 loss_train: 0.440 acc_train: 0.868 loss_test: 0.450 acc_test: 0.889 AUC = 0.61 time: 0.004s\n",
            "Epoch: 076 loss_train: 0.398 acc_train: 0.877 loss_test: 0.464 acc_test: 0.872 AUC = 0.60 time: 0.004s\n",
            "Epoch: 077 loss_train: 0.394 acc_train: 0.887 loss_test: 0.457 acc_test: 0.889 AUC = 0.60 time: 0.004s\n",
            "Epoch: 078 loss_train: 0.403 acc_train: 0.879 loss_test: 0.457 acc_test: 0.894 AUC = 0.59 time: 0.004s\n",
            "Epoch: 079 loss_train: 0.397 acc_train: 0.890 loss_test: 0.470 acc_test: 0.898 AUC = 0.55 time: 0.004s\n",
            "Epoch: 080 loss_train: 0.421 acc_train: 0.891 loss_test: 0.497 acc_test: 0.894 AUC = 0.57 time: 0.004s\n",
            "Epoch: 081 loss_train: 0.404 acc_train: 0.884 loss_test: 0.443 acc_test: 0.900 AUC = 0.60 time: 0.005s\n",
            "Epoch: 082 loss_train: 0.404 acc_train: 0.901 loss_test: 0.474 acc_test: 0.891 AUC = 0.58 time: 0.004s\n",
            "Epoch: 083 loss_train: 0.401 acc_train: 0.895 loss_test: 0.444 acc_test: 0.911 AUC = 0.58 time: 0.004s\n",
            "Epoch: 084 loss_train: 0.410 acc_train: 0.890 loss_test: 0.462 acc_test: 0.895 AUC = 0.54 time: 0.004s\n",
            "Epoch: 085 loss_train: 0.385 acc_train: 0.892 loss_test: 0.490 acc_test: 0.900 AUC = 0.55 time: 0.004s\n",
            "Epoch: 086 loss_train: 0.416 acc_train: 0.893 loss_test: 0.450 acc_test: 0.896 AUC = 0.55 time: 0.004s\n",
            "Epoch: 087 loss_train: 0.411 acc_train: 0.886 loss_test: 0.419 acc_test: 0.905 AUC = 0.62 time: 0.004s\n",
            "Epoch: 088 loss_train: 0.398 acc_train: 0.901 loss_test: 0.415 acc_test: 0.906 AUC = 0.61 time: 0.004s\n",
            "Epoch: 089 loss_train: 0.403 acc_train: 0.893 loss_test: 0.421 acc_test: 0.885 AUC = 0.61 time: 0.004s\n",
            "Epoch: 090 loss_train: 0.403 acc_train: 0.884 loss_test: 0.440 acc_test: 0.896 AUC = 0.58 time: 0.004s\n",
            "Epoch: 091 loss_train: 0.386 acc_train: 0.891 loss_test: 0.382 acc_test: 0.896 AUC = 0.63 time: 0.004s\n",
            "Epoch: 092 loss_train: 0.399 acc_train: 0.888 loss_test: 0.417 acc_test: 0.882 AUC = 0.60 time: 0.004s\n",
            "Epoch: 093 loss_train: 0.401 acc_train: 0.890 loss_test: 0.391 acc_test: 0.887 AUC = 0.63 time: 0.004s\n",
            "Epoch: 094 loss_train: 0.380 acc_train: 0.875 loss_test: 0.422 acc_test: 0.895 AUC = 0.59 time: 0.004s\n",
            "Epoch: 095 loss_train: 0.390 acc_train: 0.880 loss_test: 0.423 acc_test: 0.883 AUC = 0.58 time: 0.004s\n",
            "Epoch: 096 loss_train: 0.393 acc_train: 0.885 loss_test: 0.456 acc_test: 0.896 AUC = 0.55 time: 0.004s\n",
            "Epoch: 097 loss_train: 0.378 acc_train: 0.892 loss_test: 0.395 acc_test: 0.896 AUC = 0.61 time: 0.004s\n",
            "Epoch: 098 loss_train: 0.348 acc_train: 0.906 loss_test: 0.414 acc_test: 0.917 AUC = 0.61 time: 0.004s\n",
            "Epoch: 099 loss_train: 0.359 acc_train: 0.898 loss_test: 0.402 acc_test: 0.911 AUC = 0.61 time: 0.004s\n",
            "Epoch: 100 loss_train: 0.360 acc_train: 0.887 loss_test: 0.430 acc_test: 0.884 AUC = 0.60 time: 0.005s\n",
            "Epoch: 101 loss_train: 0.373 acc_train: 0.897 loss_test: 0.397 acc_test: 0.906 AUC = 0.62 time: 0.004s\n",
            "Epoch: 102 loss_train: 0.360 acc_train: 0.897 loss_test: 0.393 acc_test: 0.902 AUC = 0.63 time: 0.004s\n",
            "Epoch: 103 loss_train: 0.374 acc_train: 0.901 loss_test: 0.407 acc_test: 0.890 AUC = 0.62 time: 0.004s\n",
            "Epoch: 104 loss_train: 0.342 acc_train: 0.902 loss_test: 0.357 acc_test: 0.902 AUC = 0.67 time: 0.004s\n",
            "Epoch: 105 loss_train: 0.363 acc_train: 0.895 loss_test: 0.385 acc_test: 0.906 AUC = 0.63 time: 0.004s\n",
            "Epoch: 106 loss_train: 0.327 acc_train: 0.898 loss_test: 0.417 acc_test: 0.909 AUC = 0.56 time: 0.006s\n",
            "Epoch: 107 loss_train: 0.348 acc_train: 0.896 loss_test: 0.426 acc_test: 0.896 AUC = 0.58 time: 0.005s\n",
            "Epoch: 108 loss_train: 0.334 acc_train: 0.907 loss_test: 0.364 acc_test: 0.913 AUC = 0.64 time: 0.006s\n",
            "Epoch: 109 loss_train: 0.336 acc_train: 0.897 loss_test: 0.341 acc_test: 0.906 AUC = 0.65 time: 0.005s\n",
            "Epoch: 110 loss_train: 0.343 acc_train: 0.892 loss_test: 0.378 acc_test: 0.901 AUC = 0.61 time: 0.004s\n",
            "Epoch: 111 loss_train: 0.345 acc_train: 0.892 loss_test: 0.395 acc_test: 0.905 AUC = 0.60 time: 0.004s\n",
            "Epoch: 112 loss_train: 0.352 acc_train: 0.896 loss_test: 0.364 acc_test: 0.904 AUC = 0.64 time: 0.004s\n",
            "Epoch: 113 loss_train: 0.332 acc_train: 0.899 loss_test: 0.362 acc_test: 0.893 AUC = 0.65 time: 0.005s\n",
            "Epoch: 114 loss_train: 0.346 acc_train: 0.890 loss_test: 0.346 acc_test: 0.912 AUC = 0.64 time: 0.004s\n",
            "Epoch: 115 loss_train: 0.353 acc_train: 0.883 loss_test: 0.338 acc_test: 0.906 AUC = 0.66 time: 0.005s\n",
            "Epoch: 116 loss_train: 0.335 acc_train: 0.896 loss_test: 0.369 acc_test: 0.900 AUC = 0.64 time: 0.004s\n",
            "Epoch: 117 loss_train: 0.331 acc_train: 0.898 loss_test: 0.338 acc_test: 0.917 AUC = 0.67 time: 0.005s\n",
            "Epoch: 118 loss_train: 0.340 acc_train: 0.900 loss_test: 0.333 acc_test: 0.904 AUC = 0.66 time: 0.005s\n",
            "Epoch: 119 loss_train: 0.325 acc_train: 0.896 loss_test: 0.369 acc_test: 0.902 AUC = 0.61 time: 0.006s\n",
            "Epoch: 120 loss_train: 0.331 acc_train: 0.905 loss_test: 0.344 acc_test: 0.917 AUC = 0.65 time: 0.004s\n",
            "Epoch: 121 loss_train: 0.324 acc_train: 0.899 loss_test: 0.349 acc_test: 0.901 AUC = 0.64 time: 0.004s\n",
            "Epoch: 122 loss_train: 0.334 acc_train: 0.893 loss_test: 0.354 acc_test: 0.894 AUC = 0.66 time: 0.004s\n",
            "Epoch: 123 loss_train: 0.346 acc_train: 0.884 loss_test: 0.346 acc_test: 0.907 AUC = 0.66 time: 0.004s\n",
            "Epoch: 124 loss_train: 0.317 acc_train: 0.903 loss_test: 0.331 acc_test: 0.915 AUC = 0.67 time: 0.005s\n",
            "Epoch: 125 loss_train: 0.337 acc_train: 0.899 loss_test: 0.366 acc_test: 0.902 AUC = 0.63 time: 0.004s\n",
            "Epoch: 126 loss_train: 0.297 acc_train: 0.908 loss_test: 0.370 acc_test: 0.911 AUC = 0.60 time: 0.005s\n",
            "Epoch: 127 loss_train: 0.321 acc_train: 0.907 loss_test: 0.355 acc_test: 0.912 AUC = 0.62 time: 0.004s\n",
            "Epoch: 128 loss_train: 0.332 acc_train: 0.894 loss_test: 0.369 acc_test: 0.905 AUC = 0.62 time: 0.005s\n",
            "Epoch: 129 loss_train: 0.332 acc_train: 0.904 loss_test: 0.332 acc_test: 0.917 AUC = 0.67 time: 0.004s\n",
            "Epoch: 130 loss_train: 0.310 acc_train: 0.904 loss_test: 0.371 acc_test: 0.911 AUC = 0.61 time: 0.004s\n",
            "Epoch: 131 loss_train: 0.302 acc_train: 0.906 loss_test: 0.335 acc_test: 0.906 AUC = 0.66 time: 0.004s\n",
            "Epoch: 132 loss_train: 0.324 acc_train: 0.903 loss_test: 0.365 acc_test: 0.913 AUC = 0.60 time: 0.004s\n",
            "Epoch: 133 loss_train: 0.301 acc_train: 0.907 loss_test: 0.331 acc_test: 0.901 AUC = 0.66 time: 0.004s\n",
            "Epoch: 134 loss_train: 0.309 acc_train: 0.904 loss_test: 0.377 acc_test: 0.904 AUC = 0.61 time: 0.004s\n",
            "Epoch: 135 loss_train: 0.307 acc_train: 0.902 loss_test: 0.314 acc_test: 0.918 AUC = 0.67 time: 0.004s\n",
            "Epoch: 136 loss_train: 0.308 acc_train: 0.902 loss_test: 0.323 acc_test: 0.902 AUC = 0.66 time: 0.005s\n",
            "Epoch: 137 loss_train: 0.294 acc_train: 0.902 loss_test: 0.330 acc_test: 0.902 AUC = 0.66 time: 0.004s\n",
            "Epoch: 138 loss_train: 0.290 acc_train: 0.903 loss_test: 0.348 acc_test: 0.910 AUC = 0.64 time: 0.005s\n",
            "Epoch: 139 loss_train: 0.281 acc_train: 0.905 loss_test: 0.324 acc_test: 0.915 AUC = 0.66 time: 0.004s\n",
            "Epoch: 140 loss_train: 0.304 acc_train: 0.899 loss_test: 0.329 acc_test: 0.898 AUC = 0.68 time: 0.004s\n",
            "Epoch: 141 loss_train: 0.303 acc_train: 0.901 loss_test: 0.342 acc_test: 0.904 AUC = 0.65 time: 0.004s\n",
            "Epoch: 142 loss_train: 0.301 acc_train: 0.905 loss_test: 0.321 acc_test: 0.912 AUC = 0.66 time: 0.004s\n",
            "Epoch: 143 loss_train: 0.291 acc_train: 0.912 loss_test: 0.295 acc_test: 0.912 AUC = 0.71 time: 0.004s\n",
            "Epoch: 144 loss_train: 0.289 acc_train: 0.900 loss_test: 0.319 acc_test: 0.905 AUC = 0.68 time: 0.004s\n",
            "Epoch: 145 loss_train: 0.296 acc_train: 0.904 loss_test: 0.319 acc_test: 0.911 AUC = 0.67 time: 0.005s\n",
            "Epoch: 146 loss_train: 0.297 acc_train: 0.901 loss_test: 0.333 acc_test: 0.900 AUC = 0.64 time: 0.004s\n",
            "Epoch: 147 loss_train: 0.284 acc_train: 0.908 loss_test: 0.292 acc_test: 0.916 AUC = 0.69 time: 0.004s\n",
            "Epoch: 148 loss_train: 0.293 acc_train: 0.910 loss_test: 0.320 acc_test: 0.917 AUC = 0.65 time: 0.004s\n",
            "Epoch: 149 loss_train: 0.306 acc_train: 0.907 loss_test: 0.301 acc_test: 0.910 AUC = 0.71 time: 0.004s\n",
            "Epoch: 150 loss_train: 0.293 acc_train: 0.900 loss_test: 0.335 acc_test: 0.898 AUC = 0.64 time: 0.005s\n",
            "Epoch: 151 loss_train: 0.300 acc_train: 0.905 loss_test: 0.299 acc_test: 0.918 AUC = 0.69 time: 0.006s\n",
            "Epoch: 152 loss_train: 0.292 acc_train: 0.904 loss_test: 0.314 acc_test: 0.907 AUC = 0.67 time: 0.004s\n",
            "Epoch: 153 loss_train: 0.269 acc_train: 0.911 loss_test: 0.325 acc_test: 0.907 AUC = 0.68 time: 0.005s\n",
            "Epoch: 154 loss_train: 0.287 acc_train: 0.906 loss_test: 0.316 acc_test: 0.912 AUC = 0.65 time: 0.004s\n",
            "Epoch: 155 loss_train: 0.287 acc_train: 0.911 loss_test: 0.295 acc_test: 0.915 AUC = 0.69 time: 0.004s\n",
            "Epoch: 156 loss_train: 0.294 acc_train: 0.901 loss_test: 0.306 acc_test: 0.917 AUC = 0.68 time: 0.004s\n",
            "Epoch: 157 loss_train: 0.272 acc_train: 0.911 loss_test: 0.306 acc_test: 0.920 AUC = 0.66 time: 0.005s\n",
            "Epoch: 158 loss_train: 0.276 acc_train: 0.909 loss_test: 0.308 acc_test: 0.916 AUC = 0.67 time: 0.004s\n",
            "Epoch: 159 loss_train: 0.292 acc_train: 0.905 loss_test: 0.287 acc_test: 0.913 AUC = 0.71 time: 0.004s\n",
            "Epoch: 160 loss_train: 0.283 acc_train: 0.908 loss_test: 0.281 acc_test: 0.912 AUC = 0.73 time: 0.004s\n",
            "Epoch: 161 loss_train: 0.273 acc_train: 0.911 loss_test: 0.289 acc_test: 0.917 AUC = 0.71 time: 0.004s\n",
            "Epoch: 162 loss_train: 0.280 acc_train: 0.902 loss_test: 0.297 acc_test: 0.907 AUC = 0.69 time: 0.004s\n",
            "Epoch: 163 loss_train: 0.261 acc_train: 0.912 loss_test: 0.291 acc_test: 0.916 AUC = 0.70 time: 0.004s\n",
            "Epoch: 164 loss_train: 0.261 acc_train: 0.911 loss_test: 0.305 acc_test: 0.915 AUC = 0.66 time: 0.004s\n",
            "Epoch: 165 loss_train: 0.265 acc_train: 0.914 loss_test: 0.281 acc_test: 0.915 AUC = 0.71 time: 0.004s\n",
            "Epoch: 166 loss_train: 0.275 acc_train: 0.909 loss_test: 0.287 acc_test: 0.915 AUC = 0.71 time: 0.004s\n",
            "Epoch: 167 loss_train: 0.267 acc_train: 0.914 loss_test: 0.283 acc_test: 0.912 AUC = 0.71 time: 0.004s\n",
            "Epoch: 168 loss_train: 0.279 acc_train: 0.906 loss_test: 0.282 acc_test: 0.921 AUC = 0.70 time: 0.004s\n",
            "Epoch: 169 loss_train: 0.265 acc_train: 0.911 loss_test: 0.291 acc_test: 0.912 AUC = 0.69 time: 0.004s\n",
            "Epoch: 170 loss_train: 0.269 acc_train: 0.909 loss_test: 0.278 acc_test: 0.909 AUC = 0.73 time: 0.004s\n",
            "Epoch: 171 loss_train: 0.273 acc_train: 0.909 loss_test: 0.278 acc_test: 0.917 AUC = 0.72 time: 0.004s\n",
            "Epoch: 172 loss_train: 0.271 acc_train: 0.903 loss_test: 0.277 acc_test: 0.913 AUC = 0.72 time: 0.004s\n",
            "Epoch: 173 loss_train: 0.263 acc_train: 0.910 loss_test: 0.266 acc_test: 0.917 AUC = 0.75 time: 0.004s\n",
            "Epoch: 174 loss_train: 0.258 acc_train: 0.907 loss_test: 0.276 acc_test: 0.909 AUC = 0.72 time: 0.004s\n",
            "Epoch: 175 loss_train: 0.261 acc_train: 0.912 loss_test: 0.286 acc_test: 0.917 AUC = 0.70 time: 0.004s\n",
            "Epoch: 176 loss_train: 0.260 acc_train: 0.915 loss_test: 0.279 acc_test: 0.916 AUC = 0.71 time: 0.005s\n",
            "Epoch: 177 loss_train: 0.256 acc_train: 0.913 loss_test: 0.260 acc_test: 0.922 AUC = 0.74 time: 0.004s\n",
            "Epoch: 178 loss_train: 0.260 acc_train: 0.910 loss_test: 0.273 acc_test: 0.917 AUC = 0.73 time: 0.004s\n",
            "Epoch: 179 loss_train: 0.278 acc_train: 0.909 loss_test: 0.281 acc_test: 0.911 AUC = 0.71 time: 0.004s\n",
            "Epoch: 180 loss_train: 0.253 acc_train: 0.913 loss_test: 0.255 acc_test: 0.917 AUC = 0.76 time: 0.004s\n",
            "Epoch: 181 loss_train: 0.257 acc_train: 0.912 loss_test: 0.295 acc_test: 0.913 AUC = 0.70 time: 0.004s\n",
            "Epoch: 182 loss_train: 0.261 acc_train: 0.911 loss_test: 0.280 acc_test: 0.918 AUC = 0.72 time: 0.004s\n",
            "Epoch: 183 loss_train: 0.252 acc_train: 0.912 loss_test: 0.274 acc_test: 0.915 AUC = 0.73 time: 0.004s\n",
            "Epoch: 184 loss_train: 0.251 acc_train: 0.921 loss_test: 0.257 acc_test: 0.913 AUC = 0.77 time: 0.004s\n",
            "Epoch: 185 loss_train: 0.259 acc_train: 0.906 loss_test: 0.271 acc_test: 0.911 AUC = 0.75 time: 0.004s\n",
            "Epoch: 186 loss_train: 0.251 acc_train: 0.913 loss_test: 0.284 acc_test: 0.917 AUC = 0.69 time: 0.006s\n",
            "Epoch: 187 loss_train: 0.253 acc_train: 0.910 loss_test: 0.283 acc_test: 0.909 AUC = 0.72 time: 0.004s\n",
            "Epoch: 188 loss_train: 0.245 acc_train: 0.913 loss_test: 0.262 acc_test: 0.921 AUC = 0.75 time: 0.004s\n",
            "Epoch: 189 loss_train: 0.257 acc_train: 0.909 loss_test: 0.273 acc_test: 0.909 AUC = 0.75 time: 0.006s\n",
            "Epoch: 190 loss_train: 0.261 acc_train: 0.912 loss_test: 0.278 acc_test: 0.920 AUC = 0.73 time: 0.004s\n",
            "Epoch: 191 loss_train: 0.250 acc_train: 0.911 loss_test: 0.267 acc_test: 0.920 AUC = 0.74 time: 0.004s\n",
            "Epoch: 192 loss_train: 0.256 acc_train: 0.912 loss_test: 0.262 acc_test: 0.909 AUC = 0.77 time: 0.004s\n",
            "Epoch: 193 loss_train: 0.264 acc_train: 0.903 loss_test: 0.259 acc_test: 0.905 AUC = 0.77 time: 0.004s\n",
            "Epoch: 194 loss_train: 0.257 acc_train: 0.913 loss_test: 0.257 acc_test: 0.920 AUC = 0.77 time: 0.004s\n",
            "Epoch: 195 loss_train: 0.259 acc_train: 0.913 loss_test: 0.259 acc_test: 0.922 AUC = 0.74 time: 0.005s\n",
            "Epoch: 196 loss_train: 0.246 acc_train: 0.913 loss_test: 0.272 acc_test: 0.909 AUC = 0.74 time: 0.006s\n",
            "Epoch: 197 loss_train: 0.271 acc_train: 0.909 loss_test: 0.280 acc_test: 0.911 AUC = 0.72 time: 0.004s\n",
            "Epoch: 198 loss_train: 0.260 acc_train: 0.914 loss_test: 0.266 acc_test: 0.918 AUC = 0.74 time: 0.004s\n",
            "Epoch: 199 loss_train: 0.261 acc_train: 0.911 loss_test: 0.280 acc_test: 0.910 AUC = 0.74 time: 0.004s\n",
            "Epoch: 200 loss_train: 0.250 acc_train: 0.909 loss_test: 0.258 acc_test: 0.912 AUC = 0.77 time: 0.004s\n",
            "Epoch: 201 loss_train: 0.252 acc_train: 0.919 loss_test: 0.269 acc_test: 0.917 AUC = 0.73 time: 0.004s\n",
            "Epoch: 202 loss_train: 0.252 acc_train: 0.911 loss_test: 0.260 acc_test: 0.920 AUC = 0.75 time: 0.006s\n",
            "Epoch: 203 loss_train: 0.255 acc_train: 0.915 loss_test: 0.270 acc_test: 0.917 AUC = 0.74 time: 0.006s\n",
            "Epoch: 204 loss_train: 0.251 acc_train: 0.915 loss_test: 0.258 acc_test: 0.917 AUC = 0.76 time: 0.005s\n",
            "Epoch: 205 loss_train: 0.246 acc_train: 0.914 loss_test: 0.251 acc_test: 0.920 AUC = 0.78 time: 0.005s\n",
            "Epoch: 206 loss_train: 0.252 acc_train: 0.913 loss_test: 0.266 acc_test: 0.917 AUC = 0.74 time: 0.004s\n",
            "Epoch: 207 loss_train: 0.244 acc_train: 0.916 loss_test: 0.257 acc_test: 0.918 AUC = 0.76 time: 0.004s\n",
            "Epoch: 208 loss_train: 0.254 acc_train: 0.916 loss_test: 0.257 acc_test: 0.915 AUC = 0.76 time: 0.004s\n",
            "Epoch: 209 loss_train: 0.245 acc_train: 0.916 loss_test: 0.264 acc_test: 0.921 AUC = 0.75 time: 0.004s\n",
            "Epoch: 210 loss_train: 0.244 acc_train: 0.916 loss_test: 0.250 acc_test: 0.921 AUC = 0.78 time: 0.004s\n",
            "Epoch: 211 loss_train: 0.247 acc_train: 0.919 loss_test: 0.261 acc_test: 0.920 AUC = 0.75 time: 0.004s\n",
            "Epoch: 212 loss_train: 0.241 acc_train: 0.913 loss_test: 0.267 acc_test: 0.912 AUC = 0.74 time: 0.004s\n",
            "Epoch: 213 loss_train: 0.247 acc_train: 0.914 loss_test: 0.254 acc_test: 0.917 AUC = 0.76 time: 0.004s\n",
            "Epoch: 214 loss_train: 0.238 acc_train: 0.912 loss_test: 0.255 acc_test: 0.916 AUC = 0.76 time: 0.005s\n",
            "Epoch: 215 loss_train: 0.241 acc_train: 0.912 loss_test: 0.260 acc_test: 0.917 AUC = 0.75 time: 0.006s\n",
            "Epoch: 216 loss_train: 0.244 acc_train: 0.907 loss_test: 0.262 acc_test: 0.909 AUC = 0.76 time: 0.004s\n",
            "Epoch: 217 loss_train: 0.241 acc_train: 0.912 loss_test: 0.268 acc_test: 0.917 AUC = 0.74 time: 0.004s\n",
            "Epoch: 218 loss_train: 0.244 acc_train: 0.914 loss_test: 0.259 acc_test: 0.920 AUC = 0.76 time: 0.004s\n",
            "Epoch: 219 loss_train: 0.250 acc_train: 0.917 loss_test: 0.252 acc_test: 0.918 AUC = 0.78 time: 0.005s\n",
            "Epoch: 220 loss_train: 0.242 acc_train: 0.917 loss_test: 0.249 acc_test: 0.920 AUC = 0.77 time: 0.004s\n",
            "Epoch: 221 loss_train: 0.244 acc_train: 0.909 loss_test: 0.256 acc_test: 0.909 AUC = 0.77 time: 0.004s\n",
            "Epoch: 222 loss_train: 0.236 acc_train: 0.919 loss_test: 0.249 acc_test: 0.918 AUC = 0.79 time: 0.004s\n",
            "Epoch: 223 loss_train: 0.242 acc_train: 0.918 loss_test: 0.257 acc_test: 0.922 AUC = 0.77 time: 0.004s\n",
            "Epoch: 224 loss_train: 0.241 acc_train: 0.916 loss_test: 0.246 acc_test: 0.920 AUC = 0.78 time: 0.004s\n",
            "Epoch: 225 loss_train: 0.236 acc_train: 0.910 loss_test: 0.249 acc_test: 0.918 AUC = 0.79 time: 0.004s\n",
            "Epoch: 226 loss_train: 0.244 acc_train: 0.911 loss_test: 0.252 acc_test: 0.910 AUC = 0.78 time: 0.004s\n",
            "Epoch: 227 loss_train: 0.236 acc_train: 0.915 loss_test: 0.247 acc_test: 0.920 AUC = 0.78 time: 0.004s\n",
            "Epoch: 228 loss_train: 0.243 acc_train: 0.916 loss_test: 0.256 acc_test: 0.916 AUC = 0.76 time: 0.005s\n",
            "Epoch: 229 loss_train: 0.232 acc_train: 0.917 loss_test: 0.254 acc_test: 0.921 AUC = 0.76 time: 0.004s\n",
            "Epoch: 230 loss_train: 0.237 acc_train: 0.916 loss_test: 0.247 acc_test: 0.922 AUC = 0.78 time: 0.004s\n",
            "Epoch: 231 loss_train: 0.251 acc_train: 0.910 loss_test: 0.254 acc_test: 0.913 AUC = 0.77 time: 0.004s\n",
            "Epoch: 232 loss_train: 0.234 acc_train: 0.921 loss_test: 0.243 acc_test: 0.920 AUC = 0.80 time: 0.004s\n",
            "Epoch: 233 loss_train: 0.247 acc_train: 0.917 loss_test: 0.254 acc_test: 0.920 AUC = 0.77 time: 0.004s\n",
            "Epoch: 234 loss_train: 0.240 acc_train: 0.917 loss_test: 0.247 acc_test: 0.918 AUC = 0.77 time: 0.004s\n",
            "Epoch: 235 loss_train: 0.239 acc_train: 0.916 loss_test: 0.261 acc_test: 0.917 AUC = 0.75 time: 0.004s\n",
            "Epoch: 236 loss_train: 0.239 acc_train: 0.912 loss_test: 0.246 acc_test: 0.922 AUC = 0.78 time: 0.005s\n",
            "Epoch: 237 loss_train: 0.239 acc_train: 0.913 loss_test: 0.248 acc_test: 0.918 AUC = 0.78 time: 0.004s\n",
            "Epoch: 238 loss_train: 0.232 acc_train: 0.917 loss_test: 0.249 acc_test: 0.922 AUC = 0.78 time: 0.005s\n",
            "Epoch: 239 loss_train: 0.245 acc_train: 0.917 loss_test: 0.255 acc_test: 0.920 AUC = 0.77 time: 0.005s\n",
            "Epoch: 240 loss_train: 0.237 acc_train: 0.911 loss_test: 0.256 acc_test: 0.913 AUC = 0.77 time: 0.004s\n",
            "Epoch: 241 loss_train: 0.237 acc_train: 0.914 loss_test: 0.252 acc_test: 0.920 AUC = 0.78 time: 0.004s\n",
            "Epoch: 242 loss_train: 0.233 acc_train: 0.918 loss_test: 0.241 acc_test: 0.917 AUC = 0.80 time: 0.005s\n",
            "Epoch: 243 loss_train: 0.232 acc_train: 0.916 loss_test: 0.234 acc_test: 0.922 AUC = 0.81 time: 0.004s\n",
            "Epoch: 244 loss_train: 0.232 acc_train: 0.920 loss_test: 0.252 acc_test: 0.920 AUC = 0.77 time: 0.004s\n",
            "Epoch: 245 loss_train: 0.233 acc_train: 0.916 loss_test: 0.238 acc_test: 0.917 AUC = 0.81 time: 0.004s\n",
            "Epoch: 246 loss_train: 0.240 acc_train: 0.914 loss_test: 0.239 acc_test: 0.916 AUC = 0.80 time: 0.004s\n",
            "Epoch: 247 loss_train: 0.232 acc_train: 0.917 loss_test: 0.255 acc_test: 0.921 AUC = 0.76 time: 0.004s\n",
            "Epoch: 248 loss_train: 0.232 acc_train: 0.911 loss_test: 0.249 acc_test: 0.913 AUC = 0.78 time: 0.004s\n",
            "Epoch: 249 loss_train: 0.231 acc_train: 0.919 loss_test: 0.247 acc_test: 0.920 AUC = 0.78 time: 0.004s\n",
            "Epoch: 250 loss_train: 0.233 acc_train: 0.917 loss_test: 0.258 acc_test: 0.916 AUC = 0.76 time: 0.004s\n",
            "Epoch: 251 loss_train: 0.238 acc_train: 0.916 loss_test: 0.245 acc_test: 0.923 AUC = 0.78 time: 0.004s\n",
            "Epoch: 252 loss_train: 0.239 acc_train: 0.914 loss_test: 0.235 acc_test: 0.920 AUC = 0.81 time: 0.004s\n",
            "Epoch: 253 loss_train: 0.230 acc_train: 0.922 loss_test: 0.254 acc_test: 0.917 AUC = 0.78 time: 0.004s\n",
            "Epoch: 254 loss_train: 0.231 acc_train: 0.914 loss_test: 0.246 acc_test: 0.922 AUC = 0.79 time: 0.004s\n",
            "Epoch: 255 loss_train: 0.228 acc_train: 0.919 loss_test: 0.242 acc_test: 0.920 AUC = 0.80 time: 0.004s\n",
            "Epoch: 256 loss_train: 0.235 acc_train: 0.919 loss_test: 0.236 acc_test: 0.922 AUC = 0.81 time: 0.004s\n",
            "Epoch: 257 loss_train: 0.229 acc_train: 0.917 loss_test: 0.242 acc_test: 0.917 AUC = 0.79 time: 0.004s\n",
            "Epoch: 258 loss_train: 0.233 acc_train: 0.918 loss_test: 0.241 acc_test: 0.921 AUC = 0.79 time: 0.004s\n",
            "Epoch: 259 loss_train: 0.227 acc_train: 0.915 loss_test: 0.233 acc_test: 0.918 AUC = 0.81 time: 0.004s\n",
            "Epoch: 260 loss_train: 0.227 acc_train: 0.917 loss_test: 0.247 acc_test: 0.920 AUC = 0.79 time: 0.004s\n",
            "Epoch: 261 loss_train: 0.232 acc_train: 0.916 loss_test: 0.247 acc_test: 0.920 AUC = 0.78 time: 0.007s\n",
            "Epoch: 262 loss_train: 0.230 acc_train: 0.917 loss_test: 0.229 acc_test: 0.921 AUC = 0.82 time: 0.005s\n",
            "Epoch: 263 loss_train: 0.233 acc_train: 0.917 loss_test: 0.231 acc_test: 0.917 AUC = 0.82 time: 0.004s\n",
            "Epoch: 264 loss_train: 0.230 acc_train: 0.919 loss_test: 0.249 acc_test: 0.923 AUC = 0.78 time: 0.005s\n",
            "Epoch: 265 loss_train: 0.232 acc_train: 0.914 loss_test: 0.242 acc_test: 0.920 AUC = 0.80 time: 0.004s\n",
            "Epoch: 266 loss_train: 0.227 acc_train: 0.917 loss_test: 0.238 acc_test: 0.913 AUC = 0.82 time: 0.005s\n",
            "Epoch: 267 loss_train: 0.228 acc_train: 0.911 loss_test: 0.240 acc_test: 0.918 AUC = 0.80 time: 0.004s\n",
            "Epoch: 268 loss_train: 0.218 acc_train: 0.922 loss_test: 0.241 acc_test: 0.917 AUC = 0.80 time: 0.004s\n",
            "Epoch: 269 loss_train: 0.235 acc_train: 0.918 loss_test: 0.251 acc_test: 0.918 AUC = 0.77 time: 0.004s\n",
            "Epoch: 270 loss_train: 0.228 acc_train: 0.917 loss_test: 0.241 acc_test: 0.921 AUC = 0.80 time: 0.004s\n",
            "Epoch: 271 loss_train: 0.227 acc_train: 0.918 loss_test: 0.248 acc_test: 0.920 AUC = 0.78 time: 0.004s\n",
            "Epoch: 272 loss_train: 0.227 acc_train: 0.916 loss_test: 0.249 acc_test: 0.918 AUC = 0.78 time: 0.004s\n",
            "Epoch: 273 loss_train: 0.231 acc_train: 0.917 loss_test: 0.241 acc_test: 0.923 AUC = 0.79 time: 0.004s\n",
            "Epoch: 274 loss_train: 0.226 acc_train: 0.917 loss_test: 0.239 acc_test: 0.920 AUC = 0.80 time: 0.004s\n",
            "Epoch: 275 loss_train: 0.233 acc_train: 0.915 loss_test: 0.235 acc_test: 0.922 AUC = 0.80 time: 0.004s\n",
            "Epoch: 276 loss_train: 0.230 acc_train: 0.918 loss_test: 0.244 acc_test: 0.922 AUC = 0.79 time: 0.004s\n",
            "Epoch: 277 loss_train: 0.232 acc_train: 0.919 loss_test: 0.246 acc_test: 0.918 AUC = 0.79 time: 0.004s\n",
            "Epoch: 278 loss_train: 0.225 acc_train: 0.916 loss_test: 0.231 acc_test: 0.921 AUC = 0.82 time: 0.004s\n",
            "Epoch: 279 loss_train: 0.229 acc_train: 0.918 loss_test: 0.241 acc_test: 0.918 AUC = 0.80 time: 0.006s\n",
            "Epoch: 280 loss_train: 0.229 acc_train: 0.915 loss_test: 0.225 acc_test: 0.924 AUC = 0.83 time: 0.004s\n",
            "Epoch: 281 loss_train: 0.220 acc_train: 0.917 loss_test: 0.229 acc_test: 0.922 AUC = 0.82 time: 0.004s\n",
            "Epoch: 282 loss_train: 0.231 acc_train: 0.916 loss_test: 0.231 acc_test: 0.921 AUC = 0.81 time: 0.005s\n",
            "Epoch: 283 loss_train: 0.230 acc_train: 0.917 loss_test: 0.237 acc_test: 0.921 AUC = 0.80 time: 0.004s\n",
            "Epoch: 284 loss_train: 0.224 acc_train: 0.919 loss_test: 0.239 acc_test: 0.918 AUC = 0.80 time: 0.004s\n",
            "Epoch: 285 loss_train: 0.226 acc_train: 0.916 loss_test: 0.234 acc_test: 0.920 AUC = 0.81 time: 0.004s\n",
            "Epoch: 286 loss_train: 0.221 acc_train: 0.919 loss_test: 0.248 acc_test: 0.918 AUC = 0.79 time: 0.004s\n",
            "Epoch: 287 loss_train: 0.226 acc_train: 0.920 loss_test: 0.240 acc_test: 0.921 AUC = 0.79 time: 0.004s\n",
            "Epoch: 288 loss_train: 0.224 acc_train: 0.918 loss_test: 0.234 acc_test: 0.921 AUC = 0.81 time: 0.004s\n",
            "Epoch: 289 loss_train: 0.225 acc_train: 0.920 loss_test: 0.228 acc_test: 0.918 AUC = 0.82 time: 0.004s\n",
            "Epoch: 290 loss_train: 0.226 acc_train: 0.917 loss_test: 0.237 acc_test: 0.923 AUC = 0.81 time: 0.004s\n",
            "Epoch: 291 loss_train: 0.226 acc_train: 0.919 loss_test: 0.239 acc_test: 0.921 AUC = 0.79 time: 0.004s\n",
            "Epoch: 292 loss_train: 0.224 acc_train: 0.917 loss_test: 0.229 acc_test: 0.921 AUC = 0.82 time: 0.004s\n",
            "Epoch: 293 loss_train: 0.228 acc_train: 0.910 loss_test: 0.238 acc_test: 0.922 AUC = 0.80 time: 0.004s\n",
            "Epoch: 294 loss_train: 0.225 acc_train: 0.920 loss_test: 0.236 acc_test: 0.922 AUC = 0.80 time: 0.004s\n",
            "Epoch: 295 loss_train: 0.227 acc_train: 0.918 loss_test: 0.239 acc_test: 0.922 AUC = 0.80 time: 0.004s\n",
            "Epoch: 296 loss_train: 0.219 acc_train: 0.916 loss_test: 0.244 acc_test: 0.923 AUC = 0.79 time: 0.004s\n",
            "Epoch: 297 loss_train: 0.217 acc_train: 0.922 loss_test: 0.240 acc_test: 0.922 AUC = 0.80 time: 0.004s\n",
            "Epoch: 298 loss_train: 0.225 acc_train: 0.919 loss_test: 0.242 acc_test: 0.920 AUC = 0.79 time: 0.004s\n",
            "Epoch: 299 loss_train: 0.220 acc_train: 0.918 loss_test: 0.235 acc_test: 0.920 AUC = 0.81 time: 0.004s\n",
            "Epoch: 300 loss_train: 0.226 acc_train: 0.914 loss_test: 0.238 acc_test: 0.921 AUC = 0.80 time: 0.005s\n",
            "Epoch: 301 loss_train: 0.222 acc_train: 0.916 loss_test: 0.235 acc_test: 0.923 AUC = 0.81 time: 0.004s\n",
            "Epoch: 302 loss_train: 0.224 acc_train: 0.921 loss_test: 0.222 acc_test: 0.922 AUC = 0.83 time: 0.004s\n",
            "Epoch: 303 loss_train: 0.221 acc_train: 0.916 loss_test: 0.244 acc_test: 0.915 AUC = 0.79 time: 0.004s\n",
            "Epoch: 304 loss_train: 0.226 acc_train: 0.915 loss_test: 0.230 acc_test: 0.922 AUC = 0.81 time: 0.004s\n",
            "Epoch: 305 loss_train: 0.219 acc_train: 0.920 loss_test: 0.234 acc_test: 0.920 AUC = 0.81 time: 0.004s\n",
            "Epoch: 306 loss_train: 0.223 acc_train: 0.920 loss_test: 0.242 acc_test: 0.921 AUC = 0.80 time: 0.004s\n",
            "Epoch: 307 loss_train: 0.223 acc_train: 0.918 loss_test: 0.224 acc_test: 0.921 AUC = 0.83 time: 0.004s\n",
            "Epoch: 308 loss_train: 0.220 acc_train: 0.917 loss_test: 0.232 acc_test: 0.918 AUC = 0.82 time: 0.004s\n",
            "Epoch: 309 loss_train: 0.218 acc_train: 0.918 loss_test: 0.232 acc_test: 0.921 AUC = 0.82 time: 0.004s\n",
            "Epoch: 310 loss_train: 0.218 acc_train: 0.919 loss_test: 0.228 acc_test: 0.922 AUC = 0.83 time: 0.004s\n",
            "Epoch: 311 loss_train: 0.216 acc_train: 0.919 loss_test: 0.242 acc_test: 0.918 AUC = 0.79 time: 0.004s\n",
            "Epoch: 312 loss_train: 0.226 acc_train: 0.919 loss_test: 0.223 acc_test: 0.922 AUC = 0.83 time: 0.004s\n",
            "Epoch: 313 loss_train: 0.221 acc_train: 0.918 loss_test: 0.232 acc_test: 0.923 AUC = 0.81 time: 0.005s\n",
            "Epoch: 314 loss_train: 0.222 acc_train: 0.920 loss_test: 0.233 acc_test: 0.918 AUC = 0.82 time: 0.004s\n",
            "Epoch: 315 loss_train: 0.221 acc_train: 0.919 loss_test: 0.227 acc_test: 0.922 AUC = 0.83 time: 0.004s\n",
            "Epoch: 316 loss_train: 0.219 acc_train: 0.922 loss_test: 0.232 acc_test: 0.920 AUC = 0.82 time: 0.005s\n",
            "Epoch: 317 loss_train: 0.225 acc_train: 0.917 loss_test: 0.222 acc_test: 0.921 AUC = 0.84 time: 0.005s\n",
            "Epoch: 318 loss_train: 0.219 acc_train: 0.917 loss_test: 0.228 acc_test: 0.922 AUC = 0.83 time: 0.004s\n",
            "Epoch: 319 loss_train: 0.220 acc_train: 0.921 loss_test: 0.229 acc_test: 0.922 AUC = 0.82 time: 0.005s\n",
            "Epoch: 320 loss_train: 0.220 acc_train: 0.920 loss_test: 0.234 acc_test: 0.921 AUC = 0.81 time: 0.006s\n",
            "Epoch: 321 loss_train: 0.221 acc_train: 0.919 loss_test: 0.243 acc_test: 0.918 AUC = 0.80 time: 0.004s\n",
            "Epoch: 322 loss_train: 0.220 acc_train: 0.920 loss_test: 0.231 acc_test: 0.920 AUC = 0.82 time: 0.004s\n",
            "Epoch: 323 loss_train: 0.222 acc_train: 0.921 loss_test: 0.233 acc_test: 0.922 AUC = 0.81 time: 0.004s\n",
            "Epoch: 324 loss_train: 0.218 acc_train: 0.915 loss_test: 0.233 acc_test: 0.924 AUC = 0.81 time: 0.005s\n",
            "Epoch: 325 loss_train: 0.224 acc_train: 0.919 loss_test: 0.225 acc_test: 0.918 AUC = 0.83 time: 0.004s\n",
            "Epoch: 326 loss_train: 0.222 acc_train: 0.920 loss_test: 0.233 acc_test: 0.915 AUC = 0.83 time: 0.004s\n",
            "Epoch: 327 loss_train: 0.221 acc_train: 0.919 loss_test: 0.225 acc_test: 0.924 AUC = 0.83 time: 0.004s\n",
            "Epoch: 328 loss_train: 0.222 acc_train: 0.915 loss_test: 0.238 acc_test: 0.917 AUC = 0.82 time: 0.004s\n",
            "Epoch: 329 loss_train: 0.219 acc_train: 0.917 loss_test: 0.227 acc_test: 0.924 AUC = 0.83 time: 0.004s\n",
            "Epoch: 330 loss_train: 0.222 acc_train: 0.916 loss_test: 0.231 acc_test: 0.920 AUC = 0.82 time: 0.004s\n",
            "Epoch: 331 loss_train: 0.219 acc_train: 0.917 loss_test: 0.233 acc_test: 0.916 AUC = 0.81 time: 0.004s\n",
            "Epoch: 332 loss_train: 0.220 acc_train: 0.915 loss_test: 0.231 acc_test: 0.918 AUC = 0.82 time: 0.004s\n",
            "Epoch: 333 loss_train: 0.219 acc_train: 0.919 loss_test: 0.223 acc_test: 0.924 AUC = 0.83 time: 0.005s\n",
            "Epoch: 334 loss_train: 0.218 acc_train: 0.920 loss_test: 0.230 acc_test: 0.920 AUC = 0.82 time: 0.005s\n",
            "Epoch: 335 loss_train: 0.219 acc_train: 0.916 loss_test: 0.230 acc_test: 0.918 AUC = 0.82 time: 0.004s\n",
            "Epoch: 336 loss_train: 0.220 acc_train: 0.920 loss_test: 0.229 acc_test: 0.916 AUC = 0.83 time: 0.004s\n",
            "Epoch: 337 loss_train: 0.216 acc_train: 0.920 loss_test: 0.228 acc_test: 0.921 AUC = 0.82 time: 0.004s\n",
            "Epoch: 338 loss_train: 0.215 acc_train: 0.920 loss_test: 0.229 acc_test: 0.924 AUC = 0.82 time: 0.008s\n",
            "Epoch: 339 loss_train: 0.221 acc_train: 0.918 loss_test: 0.229 acc_test: 0.918 AUC = 0.82 time: 0.007s\n",
            "Epoch: 340 loss_train: 0.221 acc_train: 0.917 loss_test: 0.234 acc_test: 0.922 AUC = 0.81 time: 0.004s\n",
            "Epoch: 341 loss_train: 0.220 acc_train: 0.919 loss_test: 0.233 acc_test: 0.918 AUC = 0.82 time: 0.004s\n",
            "Epoch: 342 loss_train: 0.219 acc_train: 0.920 loss_test: 0.235 acc_test: 0.920 AUC = 0.81 time: 0.004s\n",
            "Epoch: 343 loss_train: 0.220 acc_train: 0.920 loss_test: 0.227 acc_test: 0.922 AUC = 0.82 time: 0.004s\n",
            "Epoch: 344 loss_train: 0.218 acc_train: 0.919 loss_test: 0.226 acc_test: 0.926 AUC = 0.82 time: 0.004s\n",
            "Epoch: 345 loss_train: 0.218 acc_train: 0.920 loss_test: 0.233 acc_test: 0.920 AUC = 0.82 time: 0.006s\n",
            "Epoch: 346 loss_train: 0.218 acc_train: 0.918 loss_test: 0.239 acc_test: 0.921 AUC = 0.81 time: 0.005s\n",
            "Epoch: 347 loss_train: 0.215 acc_train: 0.919 loss_test: 0.234 acc_test: 0.922 AUC = 0.81 time: 0.004s\n",
            "Epoch: 348 loss_train: 0.218 acc_train: 0.917 loss_test: 0.230 acc_test: 0.923 AUC = 0.82 time: 0.004s\n",
            "Epoch: 349 loss_train: 0.217 acc_train: 0.919 loss_test: 0.233 acc_test: 0.920 AUC = 0.82 time: 0.004s\n",
            "Epoch: 350 loss_train: 0.220 acc_train: 0.920 loss_test: 0.231 acc_test: 0.920 AUC = 0.82 time: 0.006s\n",
            "Epoch: 351 loss_train: 0.219 acc_train: 0.920 loss_test: 0.232 acc_test: 0.917 AUC = 0.82 time: 0.004s\n",
            "Epoch: 352 loss_train: 0.220 acc_train: 0.921 loss_test: 0.230 acc_test: 0.924 AUC = 0.82 time: 0.004s\n",
            "Epoch: 353 loss_train: 0.217 acc_train: 0.921 loss_test: 0.230 acc_test: 0.920 AUC = 0.82 time: 0.005s\n",
            "Epoch: 354 loss_train: 0.217 acc_train: 0.920 loss_test: 0.221 acc_test: 0.922 AUC = 0.84 time: 0.004s\n",
            "Epoch: 355 loss_train: 0.212 acc_train: 0.919 loss_test: 0.229 acc_test: 0.918 AUC = 0.82 time: 0.005s\n",
            "Epoch: 356 loss_train: 0.220 acc_train: 0.918 loss_test: 0.231 acc_test: 0.921 AUC = 0.82 time: 0.004s\n",
            "Epoch: 357 loss_train: 0.218 acc_train: 0.917 loss_test: 0.232 acc_test: 0.921 AUC = 0.81 time: 0.004s\n",
            "Epoch: 358 loss_train: 0.218 acc_train: 0.920 loss_test: 0.230 acc_test: 0.920 AUC = 0.82 time: 0.007s\n",
            "Epoch: 359 loss_train: 0.220 acc_train: 0.917 loss_test: 0.225 acc_test: 0.922 AUC = 0.83 time: 0.004s\n",
            "Epoch: 360 loss_train: 0.217 acc_train: 0.920 loss_test: 0.228 acc_test: 0.922 AUC = 0.82 time: 0.004s\n",
            "Epoch: 361 loss_train: 0.219 acc_train: 0.920 loss_test: 0.230 acc_test: 0.921 AUC = 0.82 time: 0.004s\n",
            "Epoch: 362 loss_train: 0.216 acc_train: 0.919 loss_test: 0.239 acc_test: 0.918 AUC = 0.81 time: 0.004s\n",
            "Epoch: 363 loss_train: 0.217 acc_train: 0.920 loss_test: 0.238 acc_test: 0.918 AUC = 0.80 time: 0.004s\n",
            "Epoch: 364 loss_train: 0.216 acc_train: 0.917 loss_test: 0.219 acc_test: 0.922 AUC = 0.84 time: 0.004s\n",
            "Epoch: 365 loss_train: 0.218 acc_train: 0.919 loss_test: 0.219 acc_test: 0.922 AUC = 0.84 time: 0.004s\n",
            "Epoch: 366 loss_train: 0.218 acc_train: 0.917 loss_test: 0.226 acc_test: 0.924 AUC = 0.82 time: 0.004s\n",
            "Epoch: 367 loss_train: 0.216 acc_train: 0.920 loss_test: 0.226 acc_test: 0.920 AUC = 0.83 time: 0.004s\n",
            "Epoch: 368 loss_train: 0.221 acc_train: 0.920 loss_test: 0.225 acc_test: 0.922 AUC = 0.83 time: 0.004s\n",
            "Epoch: 369 loss_train: 0.215 acc_train: 0.920 loss_test: 0.228 acc_test: 0.921 AUC = 0.83 time: 0.004s\n",
            "Epoch: 370 loss_train: 0.217 acc_train: 0.921 loss_test: 0.223 acc_test: 0.922 AUC = 0.83 time: 0.004s\n",
            "Epoch: 371 loss_train: 0.219 acc_train: 0.919 loss_test: 0.229 acc_test: 0.915 AUC = 0.82 time: 0.004s\n",
            "Epoch: 372 loss_train: 0.218 acc_train: 0.919 loss_test: 0.226 acc_test: 0.920 AUC = 0.83 time: 0.004s\n",
            "Epoch: 373 loss_train: 0.215 acc_train: 0.920 loss_test: 0.230 acc_test: 0.922 AUC = 0.82 time: 0.004s\n",
            "Epoch: 374 loss_train: 0.216 acc_train: 0.920 loss_test: 0.230 acc_test: 0.917 AUC = 0.82 time: 0.005s\n",
            "Epoch: 375 loss_train: 0.213 acc_train: 0.918 loss_test: 0.226 acc_test: 0.921 AUC = 0.83 time: 0.005s\n",
            "Epoch: 376 loss_train: 0.224 acc_train: 0.919 loss_test: 0.223 acc_test: 0.923 AUC = 0.84 time: 0.004s\n",
            "Epoch: 377 loss_train: 0.216 acc_train: 0.917 loss_test: 0.227 acc_test: 0.921 AUC = 0.83 time: 0.004s\n",
            "Epoch: 378 loss_train: 0.215 acc_train: 0.920 loss_test: 0.227 acc_test: 0.917 AUC = 0.83 time: 0.004s\n",
            "Epoch: 379 loss_train: 0.216 acc_train: 0.921 loss_test: 0.227 acc_test: 0.922 AUC = 0.83 time: 0.004s\n",
            "Epoch: 380 loss_train: 0.212 acc_train: 0.918 loss_test: 0.222 acc_test: 0.922 AUC = 0.84 time: 0.004s\n",
            "Epoch: 381 loss_train: 0.216 acc_train: 0.920 loss_test: 0.225 acc_test: 0.923 AUC = 0.83 time: 0.004s\n",
            "Epoch: 382 loss_train: 0.217 acc_train: 0.919 loss_test: 0.230 acc_test: 0.920 AUC = 0.82 time: 0.004s\n",
            "Epoch: 383 loss_train: 0.214 acc_train: 0.922 loss_test: 0.225 acc_test: 0.918 AUC = 0.83 time: 0.004s\n",
            "Epoch: 384 loss_train: 0.215 acc_train: 0.916 loss_test: 0.221 acc_test: 0.922 AUC = 0.84 time: 0.004s\n",
            "Epoch: 385 loss_train: 0.215 acc_train: 0.920 loss_test: 0.222 acc_test: 0.921 AUC = 0.84 time: 0.004s\n",
            "Epoch: 386 loss_train: 0.214 acc_train: 0.920 loss_test: 0.224 acc_test: 0.922 AUC = 0.83 time: 0.004s\n",
            "Epoch: 387 loss_train: 0.214 acc_train: 0.917 loss_test: 0.228 acc_test: 0.924 AUC = 0.83 time: 0.004s\n",
            "Epoch: 388 loss_train: 0.212 acc_train: 0.921 loss_test: 0.227 acc_test: 0.915 AUC = 0.84 time: 0.004s\n",
            "Epoch: 389 loss_train: 0.214 acc_train: 0.918 loss_test: 0.226 acc_test: 0.922 AUC = 0.83 time: 0.005s\n",
            "Epoch: 390 loss_train: 0.217 acc_train: 0.917 loss_test: 0.221 acc_test: 0.921 AUC = 0.84 time: 0.004s\n",
            "Epoch: 391 loss_train: 0.213 acc_train: 0.922 loss_test: 0.224 acc_test: 0.921 AUC = 0.83 time: 0.005s\n",
            "Epoch: 392 loss_train: 0.212 acc_train: 0.921 loss_test: 0.230 acc_test: 0.921 AUC = 0.82 time: 0.005s\n",
            "Epoch: 393 loss_train: 0.213 acc_train: 0.921 loss_test: 0.229 acc_test: 0.920 AUC = 0.82 time: 0.004s\n",
            "Epoch: 394 loss_train: 0.217 acc_train: 0.918 loss_test: 0.219 acc_test: 0.923 AUC = 0.84 time: 0.004s\n",
            "Epoch: 395 loss_train: 0.214 acc_train: 0.920 loss_test: 0.228 acc_test: 0.920 AUC = 0.83 time: 0.004s\n",
            "Epoch: 396 loss_train: 0.216 acc_train: 0.920 loss_test: 0.227 acc_test: 0.923 AUC = 0.82 time: 0.004s\n",
            "Epoch: 397 loss_train: 0.214 acc_train: 0.920 loss_test: 0.225 acc_test: 0.921 AUC = 0.83 time: 0.004s\n",
            "Epoch: 398 loss_train: 0.217 acc_train: 0.916 loss_test: 0.227 acc_test: 0.921 AUC = 0.83 time: 0.004s\n",
            "Epoch: 399 loss_train: 0.216 acc_train: 0.917 loss_test: 0.230 acc_test: 0.921 AUC = 0.82 time: 0.004s\n",
            "Epoch: 400 loss_train: 0.215 acc_train: 0.917 loss_test: 0.223 acc_test: 0.921 AUC = 0.84 time: 0.004s\n",
            "Epoch: 401 loss_train: 0.213 acc_train: 0.922 loss_test: 0.229 acc_test: 0.920 AUC = 0.82 time: 0.004s\n",
            "Epoch: 402 loss_train: 0.214 acc_train: 0.920 loss_test: 0.223 acc_test: 0.921 AUC = 0.84 time: 0.005s\n",
            "Epoch: 403 loss_train: 0.208 acc_train: 0.919 loss_test: 0.220 acc_test: 0.923 AUC = 0.84 time: 0.004s\n",
            "Epoch: 404 loss_train: 0.216 acc_train: 0.919 loss_test: 0.222 acc_test: 0.924 AUC = 0.83 time: 0.004s\n",
            "Epoch: 405 loss_train: 0.215 acc_train: 0.920 loss_test: 0.223 acc_test: 0.921 AUC = 0.84 time: 0.004s\n",
            "Epoch: 406 loss_train: 0.212 acc_train: 0.920 loss_test: 0.222 acc_test: 0.921 AUC = 0.83 time: 0.004s\n",
            "Epoch: 407 loss_train: 0.211 acc_train: 0.921 loss_test: 0.224 acc_test: 0.920 AUC = 0.83 time: 0.005s\n",
            "Epoch: 408 loss_train: 0.213 acc_train: 0.920 loss_test: 0.225 acc_test: 0.921 AUC = 0.83 time: 0.004s\n",
            "Epoch: 409 loss_train: 0.211 acc_train: 0.919 loss_test: 0.226 acc_test: 0.922 AUC = 0.83 time: 0.005s\n",
            "Epoch: 410 loss_train: 0.212 acc_train: 0.921 loss_test: 0.231 acc_test: 0.922 AUC = 0.82 time: 0.004s\n",
            "Epoch: 411 loss_train: 0.215 acc_train: 0.920 loss_test: 0.219 acc_test: 0.921 AUC = 0.84 time: 0.005s\n",
            "Epoch: 412 loss_train: 0.213 acc_train: 0.921 loss_test: 0.223 acc_test: 0.922 AUC = 0.83 time: 0.005s\n",
            "Epoch: 413 loss_train: 0.213 acc_train: 0.920 loss_test: 0.220 acc_test: 0.920 AUC = 0.84 time: 0.005s\n",
            "Epoch: 414 loss_train: 0.212 acc_train: 0.921 loss_test: 0.223 acc_test: 0.921 AUC = 0.84 time: 0.004s\n",
            "Epoch: 415 loss_train: 0.216 acc_train: 0.920 loss_test: 0.225 acc_test: 0.921 AUC = 0.83 time: 0.005s\n",
            "Epoch: 416 loss_train: 0.214 acc_train: 0.916 loss_test: 0.218 acc_test: 0.922 AUC = 0.84 time: 0.005s\n",
            "Epoch: 417 loss_train: 0.217 acc_train: 0.919 loss_test: 0.218 acc_test: 0.923 AUC = 0.84 time: 0.004s\n",
            "Epoch: 418 loss_train: 0.213 acc_train: 0.920 loss_test: 0.221 acc_test: 0.923 AUC = 0.84 time: 0.004s\n",
            "Epoch: 419 loss_train: 0.212 acc_train: 0.921 loss_test: 0.216 acc_test: 0.920 AUC = 0.85 time: 0.004s\n",
            "Epoch: 420 loss_train: 0.215 acc_train: 0.919 loss_test: 0.222 acc_test: 0.924 AUC = 0.83 time: 0.004s\n",
            "Epoch: 421 loss_train: 0.212 acc_train: 0.918 loss_test: 0.218 acc_test: 0.920 AUC = 0.84 time: 0.004s\n",
            "Epoch: 422 loss_train: 0.212 acc_train: 0.920 loss_test: 0.217 acc_test: 0.922 AUC = 0.84 time: 0.004s\n",
            "Epoch: 423 loss_train: 0.214 acc_train: 0.912 loss_test: 0.221 acc_test: 0.920 AUC = 0.84 time: 0.004s\n",
            "Epoch: 424 loss_train: 0.211 acc_train: 0.920 loss_test: 0.222 acc_test: 0.922 AUC = 0.83 time: 0.004s\n",
            "Epoch: 425 loss_train: 0.207 acc_train: 0.922 loss_test: 0.225 acc_test: 0.920 AUC = 0.83 time: 0.004s\n",
            "Epoch: 426 loss_train: 0.215 acc_train: 0.920 loss_test: 0.226 acc_test: 0.921 AUC = 0.83 time: 0.005s\n",
            "Epoch: 427 loss_train: 0.217 acc_train: 0.920 loss_test: 0.223 acc_test: 0.922 AUC = 0.84 time: 0.004s\n",
            "Epoch: 428 loss_train: 0.210 acc_train: 0.921 loss_test: 0.230 acc_test: 0.921 AUC = 0.83 time: 0.006s\n",
            "Epoch: 429 loss_train: 0.212 acc_train: 0.920 loss_test: 0.225 acc_test: 0.922 AUC = 0.83 time: 0.004s\n",
            "Epoch: 430 loss_train: 0.211 acc_train: 0.921 loss_test: 0.226 acc_test: 0.920 AUC = 0.83 time: 0.004s\n",
            "Epoch: 431 loss_train: 0.209 acc_train: 0.921 loss_test: 0.224 acc_test: 0.920 AUC = 0.83 time: 0.004s\n",
            "Epoch: 432 loss_train: 0.210 acc_train: 0.921 loss_test: 0.228 acc_test: 0.921 AUC = 0.82 time: 0.004s\n",
            "Epoch: 433 loss_train: 0.213 acc_train: 0.917 loss_test: 0.214 acc_test: 0.920 AUC = 0.85 time: 0.011s\n",
            "Epoch: 434 loss_train: 0.214 acc_train: 0.918 loss_test: 0.213 acc_test: 0.924 AUC = 0.85 time: 0.007s\n",
            "Epoch: 435 loss_train: 0.214 acc_train: 0.920 loss_test: 0.213 acc_test: 0.924 AUC = 0.85 time: 0.006s\n",
            "Epoch: 436 loss_train: 0.209 acc_train: 0.919 loss_test: 0.223 acc_test: 0.922 AUC = 0.83 time: 0.004s\n",
            "Epoch: 437 loss_train: 0.209 acc_train: 0.919 loss_test: 0.228 acc_test: 0.916 AUC = 0.83 time: 0.005s\n",
            "Epoch: 438 loss_train: 0.211 acc_train: 0.920 loss_test: 0.229 acc_test: 0.920 AUC = 0.82 time: 0.004s\n",
            "Epoch: 439 loss_train: 0.213 acc_train: 0.921 loss_test: 0.226 acc_test: 0.918 AUC = 0.83 time: 0.004s\n",
            "Epoch: 440 loss_train: 0.209 acc_train: 0.917 loss_test: 0.228 acc_test: 0.920 AUC = 0.83 time: 0.004s\n",
            "Epoch: 441 loss_train: 0.211 acc_train: 0.920 loss_test: 0.228 acc_test: 0.923 AUC = 0.83 time: 0.004s\n",
            "Epoch: 442 loss_train: 0.214 acc_train: 0.918 loss_test: 0.219 acc_test: 0.923 AUC = 0.84 time: 0.004s\n",
            "Epoch: 443 loss_train: 0.213 acc_train: 0.918 loss_test: 0.218 acc_test: 0.923 AUC = 0.84 time: 0.004s\n",
            "Epoch: 444 loss_train: 0.214 acc_train: 0.918 loss_test: 0.222 acc_test: 0.920 AUC = 0.84 time: 0.004s\n",
            "Epoch: 445 loss_train: 0.211 acc_train: 0.921 loss_test: 0.224 acc_test: 0.918 AUC = 0.83 time: 0.004s\n",
            "Epoch: 446 loss_train: 0.212 acc_train: 0.920 loss_test: 0.219 acc_test: 0.926 AUC = 0.84 time: 0.005s\n",
            "Epoch: 447 loss_train: 0.214 acc_train: 0.919 loss_test: 0.217 acc_test: 0.923 AUC = 0.84 time: 0.004s\n",
            "Epoch: 448 loss_train: 0.209 acc_train: 0.919 loss_test: 0.216 acc_test: 0.924 AUC = 0.85 time: 0.004s\n",
            "Epoch: 449 loss_train: 0.210 acc_train: 0.921 loss_test: 0.221 acc_test: 0.920 AUC = 0.84 time: 0.004s\n",
            "Epoch: 450 loss_train: 0.211 acc_train: 0.919 loss_test: 0.219 acc_test: 0.923 AUC = 0.84 time: 0.004s\n",
            "Epoch: 451 loss_train: 0.210 acc_train: 0.921 loss_test: 0.214 acc_test: 0.921 AUC = 0.85 time: 0.005s\n",
            "Epoch: 452 loss_train: 0.211 acc_train: 0.920 loss_test: 0.219 acc_test: 0.924 AUC = 0.84 time: 0.004s\n",
            "Epoch: 453 loss_train: 0.211 acc_train: 0.922 loss_test: 0.220 acc_test: 0.918 AUC = 0.84 time: 0.004s\n",
            "Epoch: 454 loss_train: 0.211 acc_train: 0.921 loss_test: 0.226 acc_test: 0.920 AUC = 0.83 time: 0.005s\n",
            "Epoch: 455 loss_train: 0.210 acc_train: 0.922 loss_test: 0.220 acc_test: 0.920 AUC = 0.84 time: 0.004s\n",
            "Epoch: 456 loss_train: 0.211 acc_train: 0.917 loss_test: 0.215 acc_test: 0.924 AUC = 0.85 time: 0.004s\n",
            "Epoch: 457 loss_train: 0.213 acc_train: 0.921 loss_test: 0.220 acc_test: 0.917 AUC = 0.84 time: 0.004s\n",
            "Epoch: 458 loss_train: 0.215 acc_train: 0.921 loss_test: 0.228 acc_test: 0.922 AUC = 0.83 time: 0.004s\n",
            "Epoch: 459 loss_train: 0.210 acc_train: 0.920 loss_test: 0.227 acc_test: 0.923 AUC = 0.83 time: 0.004s\n",
            "Epoch: 460 loss_train: 0.212 acc_train: 0.919 loss_test: 0.231 acc_test: 0.922 AUC = 0.81 time: 0.005s\n",
            "Epoch: 461 loss_train: 0.213 acc_train: 0.920 loss_test: 0.218 acc_test: 0.920 AUC = 0.85 time: 0.004s\n",
            "Epoch: 462 loss_train: 0.212 acc_train: 0.919 loss_test: 0.220 acc_test: 0.922 AUC = 0.84 time: 0.004s\n",
            "Epoch: 463 loss_train: 0.213 acc_train: 0.920 loss_test: 0.217 acc_test: 0.920 AUC = 0.85 time: 0.004s\n",
            "Epoch: 464 loss_train: 0.209 acc_train: 0.920 loss_test: 0.218 acc_test: 0.922 AUC = 0.84 time: 0.005s\n",
            "Epoch: 465 loss_train: 0.211 acc_train: 0.921 loss_test: 0.224 acc_test: 0.918 AUC = 0.84 time: 0.004s\n",
            "Epoch: 466 loss_train: 0.210 acc_train: 0.920 loss_test: 0.221 acc_test: 0.922 AUC = 0.84 time: 0.004s\n",
            "Epoch: 467 loss_train: 0.212 acc_train: 0.921 loss_test: 0.222 acc_test: 0.920 AUC = 0.83 time: 0.004s\n",
            "Epoch: 468 loss_train: 0.211 acc_train: 0.918 loss_test: 0.215 acc_test: 0.921 AUC = 0.85 time: 0.004s\n",
            "Epoch: 469 loss_train: 0.209 acc_train: 0.920 loss_test: 0.222 acc_test: 0.918 AUC = 0.83 time: 0.004s\n",
            "Epoch: 470 loss_train: 0.213 acc_train: 0.920 loss_test: 0.216 acc_test: 0.922 AUC = 0.85 time: 0.004s\n",
            "Epoch: 471 loss_train: 0.212 acc_train: 0.920 loss_test: 0.224 acc_test: 0.917 AUC = 0.83 time: 0.004s\n",
            "Epoch: 472 loss_train: 0.208 acc_train: 0.921 loss_test: 0.226 acc_test: 0.922 AUC = 0.83 time: 0.004s\n",
            "Epoch: 473 loss_train: 0.210 acc_train: 0.919 loss_test: 0.219 acc_test: 0.923 AUC = 0.84 time: 0.004s\n",
            "Epoch: 474 loss_train: 0.210 acc_train: 0.920 loss_test: 0.221 acc_test: 0.921 AUC = 0.84 time: 0.006s\n",
            "Epoch: 475 loss_train: 0.214 acc_train: 0.918 loss_test: 0.219 acc_test: 0.920 AUC = 0.84 time: 0.004s\n",
            "Epoch: 476 loss_train: 0.208 acc_train: 0.922 loss_test: 0.221 acc_test: 0.921 AUC = 0.84 time: 0.005s\n",
            "Epoch: 477 loss_train: 0.211 acc_train: 0.921 loss_test: 0.220 acc_test: 0.920 AUC = 0.84 time: 0.004s\n",
            "Epoch: 478 loss_train: 0.211 acc_train: 0.921 loss_test: 0.225 acc_test: 0.920 AUC = 0.83 time: 0.005s\n",
            "Epoch: 479 loss_train: 0.209 acc_train: 0.920 loss_test: 0.222 acc_test: 0.921 AUC = 0.84 time: 0.005s\n",
            "Epoch: 480 loss_train: 0.210 acc_train: 0.919 loss_test: 0.215 acc_test: 0.926 AUC = 0.85 time: 0.004s\n",
            "Epoch: 481 loss_train: 0.209 acc_train: 0.919 loss_test: 0.229 acc_test: 0.920 AUC = 0.83 time: 0.004s\n",
            "Epoch: 482 loss_train: 0.207 acc_train: 0.919 loss_test: 0.214 acc_test: 0.920 AUC = 0.85 time: 0.005s\n",
            "Epoch: 483 loss_train: 0.211 acc_train: 0.920 loss_test: 0.217 acc_test: 0.922 AUC = 0.84 time: 0.005s\n",
            "Epoch: 484 loss_train: 0.207 acc_train: 0.921 loss_test: 0.221 acc_test: 0.924 AUC = 0.84 time: 0.004s\n",
            "Epoch: 485 loss_train: 0.210 acc_train: 0.919 loss_test: 0.222 acc_test: 0.917 AUC = 0.84 time: 0.005s\n",
            "Epoch: 486 loss_train: 0.208 acc_train: 0.917 loss_test: 0.225 acc_test: 0.926 AUC = 0.83 time: 0.005s\n",
            "Epoch: 487 loss_train: 0.209 acc_train: 0.920 loss_test: 0.214 acc_test: 0.924 AUC = 0.85 time: 0.004s\n",
            "Epoch: 488 loss_train: 0.208 acc_train: 0.918 loss_test: 0.218 acc_test: 0.920 AUC = 0.85 time: 0.005s\n",
            "Epoch: 489 loss_train: 0.208 acc_train: 0.919 loss_test: 0.218 acc_test: 0.922 AUC = 0.84 time: 0.005s\n",
            "Epoch: 490 loss_train: 0.205 acc_train: 0.921 loss_test: 0.230 acc_test: 0.921 AUC = 0.82 time: 0.004s\n",
            "Epoch: 491 loss_train: 0.207 acc_train: 0.921 loss_test: 0.221 acc_test: 0.920 AUC = 0.84 time: 0.004s\n",
            "Epoch: 492 loss_train: 0.209 acc_train: 0.920 loss_test: 0.223 acc_test: 0.922 AUC = 0.83 time: 0.004s\n",
            "Epoch: 493 loss_train: 0.209 acc_train: 0.921 loss_test: 0.221 acc_test: 0.921 AUC = 0.84 time: 0.007s\n",
            "Epoch: 494 loss_train: 0.211 acc_train: 0.922 loss_test: 0.221 acc_test: 0.923 AUC = 0.84 time: 0.004s\n",
            "Epoch: 495 loss_train: 0.211 acc_train: 0.919 loss_test: 0.216 acc_test: 0.922 AUC = 0.85 time: 0.004s\n",
            "Epoch: 496 loss_train: 0.208 acc_train: 0.921 loss_test: 0.220 acc_test: 0.922 AUC = 0.84 time: 0.004s\n",
            "Epoch: 497 loss_train: 0.210 acc_train: 0.920 loss_test: 0.216 acc_test: 0.923 AUC = 0.85 time: 0.004s\n",
            "Epoch: 498 loss_train: 0.208 acc_train: 0.920 loss_test: 0.218 acc_test: 0.922 AUC = 0.84 time: 0.005s\n",
            "Epoch: 499 loss_train: 0.210 acc_train: 0.921 loss_test: 0.223 acc_test: 0.920 AUC = 0.83 time: 0.004s\n",
            "Epoch: 500 loss_train: 0.206 acc_train: 0.920 loss_test: 0.218 acc_test: 0.923 AUC = 0.84 time: 0.004s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 8.1014s\n",
            "Loading 467th epoch\n",
            "Test set results: loss= 0.2154 accuracy= 0.9220 AUC = 0.84 False Alarm Rate = 0.0049 F1 Score = 0.0857\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import time\n",
        "import json\n",
        "from sklearn import metrics\n",
        "\n",
        "#from load import accuracy, load_multi_data\n",
        "#from models import FusionGAT3\n",
        "\n",
        "\n",
        "seed = 72\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output  = model(features, adj_list_tt)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    y_prob = torch.exp(output[:, 1]).cpu().detach().numpy()\n",
        "\n",
        "    auc_score = roc_auc_score(labels[idx_test].cpu().detach().numpy(), y_prob[idx_test.cpu().detach().numpy()])\n",
        "    print('Epoch: {:03d}'.format(epoch+1),\n",
        "          'loss_train: {:.3f}'.format(loss_train.data.item()),\n",
        "          'acc_train: {:.3f}'.format(acc_train.data.item()),\n",
        "          'loss_test: {:.3f}'.format(loss_test.data.item()),\n",
        "          'acc_test: {:.3f}'.format(acc_test.data.item()),\n",
        "          \"AUC = {:.2f}\".format(auc_score),\n",
        "          'time: {:.3f}s'.format(time.time() - t))\n",
        "\n",
        "    return loss_test.data.item(), acc_test, auc_score\n",
        "\n",
        "def compute_test(model):\n",
        "    model.eval()\n",
        "    output = model(features, adj_list_tt)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    y_prob = torch.exp(output[:, 1]).cpu().detach().numpy()\n",
        "\n",
        "    auc_score = roc_auc_score(labels[idx_test].cpu().detach().numpy(), y_prob[idx_test.cpu().detach().numpy()])\n",
        "    # compute false alarm rate and f1 score\n",
        "    y_pred = np.array([0 if p < 0.5 else 1 for p in y_prob])\n",
        "    false_alarm_rate = np.mean(y_pred[idx_test.cpu().detach().numpy()] == 1)\n",
        "    f1score = f1_score(labels[idx_test].cpu().detach().numpy(), y_pred[idx_test.cpu().detach().numpy()])\n",
        "    precisionn, recalll, _ = precision_recall_curve(labels[idx_test].cpu().detach().numpy(), y_prob[idx_test.cpu().detach().numpy()])\n",
        "\n",
        "    pr_auc = metrics.auc(recalll, precisionn)\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.data.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.data.item()),\n",
        "          \"AUC = {:.2f}\".format(auc_score),\n",
        "          \"False Alarm Rate = {:.4f}\".format(false_alarm_rate),\n",
        "          \"F1 Score = {:.4f}\".format(f1score))\n",
        "    return loss_test.data.item(), acc_test.data.item(), auc_score, false_alarm_rate, f1score, y_pred, pr_auc\n",
        "\n",
        "\n",
        "#parameter setting\n",
        "#\n",
        "\n",
        "cuda = True\n",
        "epochs = 500\n",
        "lrs = [ 0.001]\n",
        "weight_decays = [ 5e-3]\n",
        "dropouts = [0.5]\n",
        "hidden_dims1 = [64]\n",
        "#hidden_dims2 = [16, 64,128]\n",
        "#fusion1_dims = [2,4]\n",
        "#nb_heads = [8, 12]\n",
        "alpha = 0.2 #Alpha for the leaky_relu\n",
        "#lambda_l1s = [0.0001, 0.001, 0.01]\n",
        "patience = 50\n",
        "\n",
        "#lrs = [0.0005,0.005]\n",
        "#weight_decays = 0.0005\n",
        "#dropouts = [0.3]\n",
        "#hidden_dims1 = [64]\n",
        "#hidden_dims2 = [64]\n",
        "#fusion1_dims = [4]\n",
        "#nb_heads = [8]\n",
        "#alpha = 0.2 #Alpha for the leaky_relu\n",
        "#lambda_l1s = [0.0001, 0.001, 0.01]\n",
        "#patience = 50\n",
        "\n",
        "\n",
        "\n",
        "# Load data\n",
        "sv_path = \"\"\n",
        "folder = \"../data/\"\n",
        "att_name = \"att_RR3.csv\"\n",
        "#edge_list_name = [\"edge_pp_main.csv\", \"edge_hh_main.csv\", \"edge_pv_main_1.csv\", \"edge_pv_main_2.csv\", \"edge_pv_main_3.csv\"\n",
        "#, \"edge_pv_main_4.csv\", \"edge_pv_main_5.csv\", \"edge_pv_main_6.csv\", \"edge_pv_main_7.csv\"]\n",
        "edge_list_name = \"edge_sqRR3.csv\"\n",
        "adj_list, features, labels, idx_train, idx_test = load_data(folder, att_name, edge_list_name)\n",
        "\n",
        "#adj_list_tensor = torch.stack(adj_list)\n",
        "features, adj_list_tt, labels = Variable(features), adj_list, Variable(labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "count = 0\n",
        "for lr in lrs:\n",
        "    for dropout in dropouts:\n",
        "        for hid1 in hidden_dims1:\n",
        "                        for weight_decay in weight_decays:\n",
        "\n",
        "\n",
        "                                # Model and optimizer\n",
        "                                model = GCN(nfeat=features.shape[1],\n",
        "                                            nhid=hid1,\n",
        "                                            nclass=int(labels.max()) + 1,\n",
        "                                            dropout=dropout)\n",
        "\n",
        "                                optimizer = optim.Adam(model.parameters(),\n",
        "                                                    lr=lr,\n",
        "                                                    weight_decay=weight_decay)\n",
        "\n",
        "                                if cuda:\n",
        "                                    model.cuda()\n",
        "                                    features = features.cuda()\n",
        "                                    adj_list_tt = adj_list_tt.cuda()\n",
        "                                    labels = labels.cuda()\n",
        "                                    idx_train = torch.tensor(idx_train).cuda()\n",
        "                                    idx_test = torch.tensor(idx_test).cuda()\n",
        "\n",
        "\n",
        "                                # Train model\n",
        "                                t_total = time.time()\n",
        "                                bad_counter = 0\n",
        "                                best_auc = -1\n",
        "                                best_epoch = 0\n",
        "                                for epoch in range(epochs):\n",
        "                                    loss, acc, auc = train(epoch)\n",
        "\n",
        "                                    torch.save(model.state_dict(), sv_path + '{}.pkl'.format(epoch))\n",
        "                                    if auc > best_auc:\n",
        "                                        best_auc = auc\n",
        "                                        best_epoch = epoch\n",
        "                                        bad_counter = 0\n",
        "                                    else:\n",
        "                                        bad_counter += 1\n",
        "\n",
        "                                    if bad_counter == patience:\n",
        "                                        break\n",
        "\n",
        "                                files = glob.glob(sv_path +'*.pkl')\n",
        "                                for file in files:\n",
        "                                    filename = file.split('/')[-1]\n",
        "                                    epoch_nb = int(filename.split('.')[0])\n",
        "                                    if epoch_nb != best_epoch:\n",
        "                                        os.remove(file)\n",
        "\n",
        "                                print(\"Optimization Finished!\")\n",
        "                                print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "                                # Restore best model\n",
        "                                print('Loading {}th epoch'.format(best_epoch))\n",
        "                                model.load_state_dict(torch.load(sv_path +'{}.pkl'.format(best_epoch)))\n",
        "\n",
        "                                # Testing\n",
        "                                test_loss, test_acc, auc, far, f1, y_pred, pr_auc= compute_test(model)\n",
        "                                hyper_para = {}\n",
        "                                hyper_para[\"lr\"] = lr\n",
        "                                hyper_para[\"weight_decay\"] = weight_decay\n",
        "                                hyper_para[\"dropout\"] = dropout\n",
        "                                hyper_para[\"hidden_dim1\"] = hid1\n",
        "                                #hyper_para[\"hidden_dim2\"] = hid2\n",
        "                                #hyper_para[\"lambda\"] = lambda_l1\n",
        "                                #hyper_para[\"fusion1_dim\"] = fusion1_dim\n",
        "                                #hyper_para[\"nb_heads\"] = nb_head\n",
        "                                #hyper_para[\"alpha\"] = alpha\n",
        "                                hyper_para[\"loss\"] = test_loss\n",
        "                                hyper_para[\"accuracy\"] = test_acc\n",
        "                                hyper_para[\"auc\"] = auc\n",
        "                                hyper_para[\"pr_auc\"] = pr_auc\n",
        "                                hyper_para[\"false_alarm_rate\"] = far\n",
        "                                hyper_para[\"f1_score\"] = f1\n",
        "                                with open(sv_path +\"hyperpara.json\", \"a+\") as fp:\n",
        "                                    fp.write('\\n')\n",
        "                                    json.dump(hyper_para, fp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2iG6twzSy1I"
      },
      "source": [
        "# GAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42QOTbtL5bBu",
        "outputId": "2c40b84e-5b18-4121-dec1-84585f9788e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001 loss_train: 1.974 acc_train: 0.374 loss_test: 1.839 acc_test: 0.401 AUC = 0.49 time: 0.195s\n",
            "Epoch: 002 loss_train: 0.716 acc_train: 0.738 loss_test: 0.682 acc_test: 0.768 AUC = 0.54 time: 0.093s\n",
            "Epoch: 003 loss_train: 0.580 acc_train: 0.881 loss_test: 0.611 acc_test: 0.890 AUC = 0.44 time: 0.088s\n",
            "Epoch: 004 loss_train: 0.614 acc_train: 0.913 loss_test: 0.591 acc_test: 0.913 AUC = 0.53 time: 0.070s\n",
            "Epoch: 005 loss_train: 0.609 acc_train: 0.917 loss_test: 0.625 acc_test: 0.918 AUC = 0.55 time: 0.071s\n",
            "Epoch: 006 loss_train: 0.585 acc_train: 0.916 loss_test: 0.650 acc_test: 0.918 AUC = 0.50 time: 0.071s\n",
            "Epoch: 007 loss_train: 0.537 acc_train: 0.918 loss_test: 0.595 acc_test: 0.917 AUC = 0.48 time: 0.069s\n",
            "Epoch: 008 loss_train: 0.530 acc_train: 0.920 loss_test: 0.537 acc_test: 0.918 AUC = 0.52 time: 0.069s\n",
            "Epoch: 009 loss_train: 0.467 acc_train: 0.920 loss_test: 0.600 acc_test: 0.920 AUC = 0.41 time: 0.069s\n",
            "Epoch: 010 loss_train: 0.473 acc_train: 0.918 loss_test: 0.487 acc_test: 0.920 AUC = 0.51 time: 0.069s\n",
            "Epoch: 011 loss_train: 0.473 acc_train: 0.919 loss_test: 0.494 acc_test: 0.918 AUC = 0.51 time: 0.069s\n",
            "Epoch: 012 loss_train: 0.477 acc_train: 0.920 loss_test: 0.514 acc_test: 0.920 AUC = 0.47 time: 0.069s\n",
            "Epoch: 013 loss_train: 0.513 acc_train: 0.920 loss_test: 0.467 acc_test: 0.920 AUC = 0.56 time: 0.070s\n",
            "Epoch: 014 loss_train: 0.506 acc_train: 0.920 loss_test: 0.498 acc_test: 0.920 AUC = 0.55 time: 0.069s\n",
            "Epoch: 015 loss_train: 0.481 acc_train: 0.920 loss_test: 0.497 acc_test: 0.920 AUC = 0.50 time: 0.069s\n",
            "Epoch: 016 loss_train: 0.472 acc_train: 0.920 loss_test: 0.500 acc_test: 0.920 AUC = 0.51 time: 0.069s\n",
            "Epoch: 017 loss_train: 0.444 acc_train: 0.920 loss_test: 0.475 acc_test: 0.918 AUC = 0.52 time: 0.069s\n",
            "Epoch: 018 loss_train: 0.436 acc_train: 0.920 loss_test: 0.496 acc_test: 0.920 AUC = 0.47 time: 0.069s\n",
            "Epoch: 019 loss_train: 0.460 acc_train: 0.920 loss_test: 0.518 acc_test: 0.920 AUC = 0.46 time: 0.070s\n",
            "Epoch: 020 loss_train: 0.429 acc_train: 0.920 loss_test: 0.392 acc_test: 0.920 AUC = 0.64 time: 0.069s\n",
            "Epoch: 021 loss_train: 0.458 acc_train: 0.920 loss_test: 0.480 acc_test: 0.920 AUC = 0.49 time: 0.069s\n",
            "Epoch: 022 loss_train: 0.462 acc_train: 0.920 loss_test: 0.463 acc_test: 0.920 AUC = 0.54 time: 0.069s\n",
            "Epoch: 023 loss_train: 0.440 acc_train: 0.920 loss_test: 0.443 acc_test: 0.920 AUC = 0.55 time: 0.069s\n",
            "Epoch: 024 loss_train: 0.441 acc_train: 0.920 loss_test: 0.467 acc_test: 0.920 AUC = 0.53 time: 0.069s\n",
            "Epoch: 025 loss_train: 0.457 acc_train: 0.920 loss_test: 0.483 acc_test: 0.920 AUC = 0.50 time: 0.069s\n",
            "Epoch: 026 loss_train: 0.425 acc_train: 0.920 loss_test: 0.518 acc_test: 0.920 AUC = 0.50 time: 0.069s\n",
            "Epoch: 027 loss_train: 0.427 acc_train: 0.920 loss_test: 0.438 acc_test: 0.920 AUC = 0.55 time: 0.069s\n",
            "Epoch: 028 loss_train: 0.386 acc_train: 0.920 loss_test: 0.401 acc_test: 0.920 AUC = 0.58 time: 0.069s\n",
            "Epoch: 029 loss_train: 0.405 acc_train: 0.920 loss_test: 0.425 acc_test: 0.920 AUC = 0.54 time: 0.069s\n",
            "Epoch: 030 loss_train: 0.430 acc_train: 0.920 loss_test: 0.417 acc_test: 0.920 AUC = 0.55 time: 0.069s\n",
            "Epoch: 031 loss_train: 0.411 acc_train: 0.920 loss_test: 0.455 acc_test: 0.920 AUC = 0.50 time: 0.069s\n",
            "Epoch: 032 loss_train: 0.408 acc_train: 0.920 loss_test: 0.461 acc_test: 0.920 AUC = 0.50 time: 0.069s\n",
            "Epoch: 033 loss_train: 0.400 acc_train: 0.920 loss_test: 0.418 acc_test: 0.920 AUC = 0.56 time: 0.069s\n",
            "Epoch: 034 loss_train: 0.388 acc_train: 0.920 loss_test: 0.395 acc_test: 0.920 AUC = 0.57 time: 0.069s\n",
            "Epoch: 035 loss_train: 0.406 acc_train: 0.920 loss_test: 0.371 acc_test: 0.920 AUC = 0.59 time: 0.069s\n",
            "Epoch: 036 loss_train: 0.389 acc_train: 0.920 loss_test: 0.450 acc_test: 0.920 AUC = 0.51 time: 0.069s\n",
            "Epoch: 037 loss_train: 0.379 acc_train: 0.920 loss_test: 0.409 acc_test: 0.920 AUC = 0.56 time: 0.069s\n",
            "Epoch: 038 loss_train: 0.370 acc_train: 0.920 loss_test: 0.434 acc_test: 0.920 AUC = 0.52 time: 0.069s\n",
            "Epoch: 039 loss_train: 0.374 acc_train: 0.920 loss_test: 0.412 acc_test: 0.920 AUC = 0.58 time: 0.069s\n",
            "Epoch: 040 loss_train: 0.350 acc_train: 0.920 loss_test: 0.389 acc_test: 0.920 AUC = 0.59 time: 0.069s\n",
            "Epoch: 041 loss_train: 0.370 acc_train: 0.920 loss_test: 0.394 acc_test: 0.920 AUC = 0.57 time: 0.070s\n",
            "Epoch: 042 loss_train: 0.374 acc_train: 0.920 loss_test: 0.364 acc_test: 0.920 AUC = 0.60 time: 0.070s\n",
            "Epoch: 043 loss_train: 0.374 acc_train: 0.920 loss_test: 0.395 acc_test: 0.920 AUC = 0.56 time: 0.070s\n",
            "Epoch: 044 loss_train: 0.361 acc_train: 0.920 loss_test: 0.389 acc_test: 0.920 AUC = 0.59 time: 0.070s\n",
            "Epoch: 045 loss_train: 0.394 acc_train: 0.920 loss_test: 0.371 acc_test: 0.920 AUC = 0.61 time: 0.069s\n",
            "Epoch: 046 loss_train: 0.357 acc_train: 0.920 loss_test: 0.339 acc_test: 0.920 AUC = 0.65 time: 0.069s\n",
            "Epoch: 047 loss_train: 0.374 acc_train: 0.920 loss_test: 0.392 acc_test: 0.920 AUC = 0.56 time: 0.070s\n",
            "Epoch: 048 loss_train: 0.359 acc_train: 0.920 loss_test: 0.359 acc_test: 0.920 AUC = 0.60 time: 0.069s\n",
            "Epoch: 049 loss_train: 0.341 acc_train: 0.920 loss_test: 0.348 acc_test: 0.920 AUC = 0.61 time: 0.070s\n",
            "Epoch: 050 loss_train: 0.371 acc_train: 0.920 loss_test: 0.342 acc_test: 0.920 AUC = 0.64 time: 0.070s\n",
            "Epoch: 051 loss_train: 0.343 acc_train: 0.920 loss_test: 0.391 acc_test: 0.920 AUC = 0.54 time: 0.070s\n",
            "Epoch: 052 loss_train: 0.335 acc_train: 0.920 loss_test: 0.360 acc_test: 0.920 AUC = 0.59 time: 0.070s\n",
            "Epoch: 053 loss_train: 0.355 acc_train: 0.920 loss_test: 0.384 acc_test: 0.920 AUC = 0.56 time: 0.069s\n",
            "Epoch: 054 loss_train: 0.328 acc_train: 0.920 loss_test: 0.340 acc_test: 0.920 AUC = 0.63 time: 0.069s\n",
            "Epoch: 055 loss_train: 0.335 acc_train: 0.920 loss_test: 0.328 acc_test: 0.920 AUC = 0.63 time: 0.069s\n",
            "Epoch: 056 loss_train: 0.332 acc_train: 0.920 loss_test: 0.333 acc_test: 0.920 AUC = 0.62 time: 0.070s\n",
            "Epoch: 057 loss_train: 0.327 acc_train: 0.920 loss_test: 0.340 acc_test: 0.920 AUC = 0.64 time: 0.070s\n",
            "Epoch: 058 loss_train: 0.349 acc_train: 0.920 loss_test: 0.375 acc_test: 0.920 AUC = 0.56 time: 0.070s\n",
            "Epoch: 059 loss_train: 0.344 acc_train: 0.920 loss_test: 0.369 acc_test: 0.920 AUC = 0.59 time: 0.070s\n",
            "Epoch: 060 loss_train: 0.318 acc_train: 0.920 loss_test: 0.343 acc_test: 0.920 AUC = 0.61 time: 0.069s\n",
            "Epoch: 061 loss_train: 0.319 acc_train: 0.920 loss_test: 0.330 acc_test: 0.920 AUC = 0.63 time: 0.069s\n",
            "Epoch: 062 loss_train: 0.316 acc_train: 0.920 loss_test: 0.343 acc_test: 0.920 AUC = 0.63 time: 0.069s\n",
            "Epoch: 063 loss_train: 0.325 acc_train: 0.920 loss_test: 0.359 acc_test: 0.920 AUC = 0.62 time: 0.070s\n",
            "Epoch: 064 loss_train: 0.302 acc_train: 0.920 loss_test: 0.322 acc_test: 0.920 AUC = 0.66 time: 0.070s\n",
            "Epoch: 065 loss_train: 0.316 acc_train: 0.920 loss_test: 0.341 acc_test: 0.920 AUC = 0.62 time: 0.070s\n",
            "Epoch: 066 loss_train: 0.312 acc_train: 0.920 loss_test: 0.354 acc_test: 0.920 AUC = 0.62 time: 0.069s\n",
            "Epoch: 067 loss_train: 0.330 acc_train: 0.920 loss_test: 0.294 acc_test: 0.920 AUC = 0.70 time: 0.070s\n",
            "Epoch: 068 loss_train: 0.327 acc_train: 0.920 loss_test: 0.332 acc_test: 0.920 AUC = 0.66 time: 0.070s\n",
            "Epoch: 069 loss_train: 0.314 acc_train: 0.920 loss_test: 0.338 acc_test: 0.920 AUC = 0.63 time: 0.070s\n",
            "Epoch: 070 loss_train: 0.309 acc_train: 0.920 loss_test: 0.343 acc_test: 0.920 AUC = 0.62 time: 0.072s\n",
            "Epoch: 071 loss_train: 0.316 acc_train: 0.920 loss_test: 0.318 acc_test: 0.920 AUC = 0.66 time: 0.070s\n",
            "Epoch: 072 loss_train: 0.314 acc_train: 0.920 loss_test: 0.338 acc_test: 0.920 AUC = 0.61 time: 0.070s\n",
            "Epoch: 073 loss_train: 0.317 acc_train: 0.920 loss_test: 0.312 acc_test: 0.920 AUC = 0.67 time: 0.070s\n",
            "Epoch: 074 loss_train: 0.309 acc_train: 0.920 loss_test: 0.316 acc_test: 0.920 AUC = 0.68 time: 0.070s\n",
            "Epoch: 075 loss_train: 0.309 acc_train: 0.920 loss_test: 0.298 acc_test: 0.920 AUC = 0.69 time: 0.070s\n",
            "Epoch: 076 loss_train: 0.314 acc_train: 0.920 loss_test: 0.301 acc_test: 0.920 AUC = 0.68 time: 0.070s\n",
            "Epoch: 077 loss_train: 0.325 acc_train: 0.920 loss_test: 0.318 acc_test: 0.920 AUC = 0.65 time: 0.070s\n",
            "Epoch: 078 loss_train: 0.325 acc_train: 0.920 loss_test: 0.351 acc_test: 0.920 AUC = 0.61 time: 0.070s\n",
            "Epoch: 079 loss_train: 0.314 acc_train: 0.920 loss_test: 0.331 acc_test: 0.920 AUC = 0.63 time: 0.069s\n",
            "Epoch: 080 loss_train: 0.305 acc_train: 0.920 loss_test: 0.337 acc_test: 0.920 AUC = 0.66 time: 0.069s\n",
            "Epoch: 081 loss_train: 0.292 acc_train: 0.920 loss_test: 0.323 acc_test: 0.920 AUC = 0.65 time: 0.069s\n",
            "Epoch: 082 loss_train: 0.303 acc_train: 0.920 loss_test: 0.356 acc_test: 0.920 AUC = 0.60 time: 0.069s\n",
            "Epoch: 083 loss_train: 0.319 acc_train: 0.920 loss_test: 0.338 acc_test: 0.920 AUC = 0.62 time: 0.069s\n",
            "Epoch: 084 loss_train: 0.306 acc_train: 0.920 loss_test: 0.316 acc_test: 0.920 AUC = 0.65 time: 0.069s\n",
            "Epoch: 085 loss_train: 0.301 acc_train: 0.920 loss_test: 0.311 acc_test: 0.920 AUC = 0.70 time: 0.069s\n",
            "Epoch: 086 loss_train: 0.299 acc_train: 0.920 loss_test: 0.293 acc_test: 0.920 AUC = 0.69 time: 0.069s\n",
            "Epoch: 087 loss_train: 0.307 acc_train: 0.920 loss_test: 0.318 acc_test: 0.920 AUC = 0.67 time: 0.069s\n",
            "Epoch: 088 loss_train: 0.292 acc_train: 0.920 loss_test: 0.309 acc_test: 0.920 AUC = 0.66 time: 0.069s\n",
            "Epoch: 089 loss_train: 0.289 acc_train: 0.920 loss_test: 0.334 acc_test: 0.920 AUC = 0.62 time: 0.069s\n",
            "Epoch: 090 loss_train: 0.302 acc_train: 0.920 loss_test: 0.282 acc_test: 0.920 AUC = 0.73 time: 0.069s\n",
            "Epoch: 091 loss_train: 0.298 acc_train: 0.920 loss_test: 0.310 acc_test: 0.920 AUC = 0.69 time: 0.069s\n",
            "Epoch: 092 loss_train: 0.290 acc_train: 0.920 loss_test: 0.296 acc_test: 0.920 AUC = 0.70 time: 0.069s\n",
            "Epoch: 093 loss_train: 0.296 acc_train: 0.920 loss_test: 0.279 acc_test: 0.920 AUC = 0.72 time: 0.069s\n",
            "Epoch: 094 loss_train: 0.274 acc_train: 0.920 loss_test: 0.332 acc_test: 0.920 AUC = 0.61 time: 0.069s\n",
            "Epoch: 095 loss_train: 0.293 acc_train: 0.920 loss_test: 0.330 acc_test: 0.920 AUC = 0.63 time: 0.069s\n",
            "Epoch: 096 loss_train: 0.293 acc_train: 0.920 loss_test: 0.311 acc_test: 0.920 AUC = 0.69 time: 0.069s\n",
            "Epoch: 097 loss_train: 0.293 acc_train: 0.920 loss_test: 0.312 acc_test: 0.920 AUC = 0.69 time: 0.070s\n",
            "Epoch: 098 loss_train: 0.279 acc_train: 0.920 loss_test: 0.297 acc_test: 0.920 AUC = 0.71 time: 0.069s\n",
            "Epoch: 099 loss_train: 0.292 acc_train: 0.920 loss_test: 0.322 acc_test: 0.920 AUC = 0.66 time: 0.069s\n",
            "Epoch: 100 loss_train: 0.280 acc_train: 0.920 loss_test: 0.301 acc_test: 0.920 AUC = 0.70 time: 0.070s\n",
            "Epoch: 101 loss_train: 0.287 acc_train: 0.920 loss_test: 0.282 acc_test: 0.920 AUC = 0.72 time: 0.069s\n",
            "Epoch: 102 loss_train: 0.297 acc_train: 0.920 loss_test: 0.306 acc_test: 0.920 AUC = 0.68 time: 0.069s\n",
            "Epoch: 103 loss_train: 0.272 acc_train: 0.920 loss_test: 0.283 acc_test: 0.920 AUC = 0.73 time: 0.070s\n",
            "Epoch: 104 loss_train: 0.280 acc_train: 0.920 loss_test: 0.282 acc_test: 0.920 AUC = 0.72 time: 0.069s\n",
            "Epoch: 105 loss_train: 0.274 acc_train: 0.920 loss_test: 0.260 acc_test: 0.920 AUC = 0.77 time: 0.069s\n",
            "Epoch: 106 loss_train: 0.290 acc_train: 0.920 loss_test: 0.332 acc_test: 0.920 AUC = 0.63 time: 0.069s\n",
            "Epoch: 107 loss_train: 0.293 acc_train: 0.920 loss_test: 0.321 acc_test: 0.920 AUC = 0.65 time: 0.069s\n",
            "Epoch: 108 loss_train: 0.269 acc_train: 0.920 loss_test: 0.324 acc_test: 0.920 AUC = 0.68 time: 0.069s\n",
            "Epoch: 109 loss_train: 0.283 acc_train: 0.920 loss_test: 0.303 acc_test: 0.920 AUC = 0.68 time: 0.070s\n",
            "Epoch: 110 loss_train: 0.276 acc_train: 0.920 loss_test: 0.294 acc_test: 0.920 AUC = 0.72 time: 0.070s\n",
            "Epoch: 111 loss_train: 0.278 acc_train: 0.920 loss_test: 0.278 acc_test: 0.920 AUC = 0.71 time: 0.070s\n",
            "Epoch: 112 loss_train: 0.274 acc_train: 0.920 loss_test: 0.279 acc_test: 0.920 AUC = 0.72 time: 0.070s\n",
            "Epoch: 113 loss_train: 0.278 acc_train: 0.920 loss_test: 0.282 acc_test: 0.920 AUC = 0.73 time: 0.069s\n",
            "Epoch: 114 loss_train: 0.283 acc_train: 0.920 loss_test: 0.308 acc_test: 0.920 AUC = 0.67 time: 0.070s\n",
            "Epoch: 115 loss_train: 0.264 acc_train: 0.920 loss_test: 0.290 acc_test: 0.920 AUC = 0.72 time: 0.070s\n",
            "Epoch: 116 loss_train: 0.292 acc_train: 0.920 loss_test: 0.278 acc_test: 0.920 AUC = 0.72 time: 0.069s\n",
            "Epoch: 117 loss_train: 0.263 acc_train: 0.920 loss_test: 0.300 acc_test: 0.920 AUC = 0.68 time: 0.070s\n",
            "Epoch: 118 loss_train: 0.277 acc_train: 0.920 loss_test: 0.258 acc_test: 0.920 AUC = 0.77 time: 0.069s\n",
            "Epoch: 119 loss_train: 0.288 acc_train: 0.920 loss_test: 0.311 acc_test: 0.920 AUC = 0.67 time: 0.069s\n",
            "Epoch: 120 loss_train: 0.272 acc_train: 0.920 loss_test: 0.313 acc_test: 0.920 AUC = 0.65 time: 0.069s\n",
            "Epoch: 121 loss_train: 0.281 acc_train: 0.920 loss_test: 0.282 acc_test: 0.920 AUC = 0.71 time: 0.069s\n",
            "Epoch: 122 loss_train: 0.283 acc_train: 0.920 loss_test: 0.276 acc_test: 0.920 AUC = 0.74 time: 0.069s\n",
            "Epoch: 123 loss_train: 0.265 acc_train: 0.920 loss_test: 0.278 acc_test: 0.920 AUC = 0.71 time: 0.071s\n",
            "Epoch: 124 loss_train: 0.270 acc_train: 0.920 loss_test: 0.278 acc_test: 0.920 AUC = 0.73 time: 0.070s\n",
            "Epoch: 125 loss_train: 0.287 acc_train: 0.920 loss_test: 0.283 acc_test: 0.920 AUC = 0.70 time: 0.079s\n",
            "Epoch: 126 loss_train: 0.283 acc_train: 0.920 loss_test: 0.291 acc_test: 0.920 AUC = 0.70 time: 0.069s\n",
            "Epoch: 127 loss_train: 0.271 acc_train: 0.920 loss_test: 0.275 acc_test: 0.920 AUC = 0.72 time: 0.071s\n",
            "Epoch: 128 loss_train: 0.275 acc_train: 0.920 loss_test: 0.274 acc_test: 0.920 AUC = 0.74 time: 0.069s\n",
            "Epoch: 129 loss_train: 0.275 acc_train: 0.920 loss_test: 0.318 acc_test: 0.920 AUC = 0.66 time: 0.069s\n",
            "Epoch: 130 loss_train: 0.268 acc_train: 0.920 loss_test: 0.271 acc_test: 0.920 AUC = 0.75 time: 0.069s\n",
            "Epoch: 131 loss_train: 0.282 acc_train: 0.920 loss_test: 0.282 acc_test: 0.920 AUC = 0.72 time: 0.069s\n",
            "Epoch: 132 loss_train: 0.281 acc_train: 0.920 loss_test: 0.272 acc_test: 0.920 AUC = 0.75 time: 0.069s\n",
            "Epoch: 133 loss_train: 0.272 acc_train: 0.920 loss_test: 0.287 acc_test: 0.920 AUC = 0.71 time: 0.069s\n",
            "Epoch: 134 loss_train: 0.282 acc_train: 0.920 loss_test: 0.295 acc_test: 0.920 AUC = 0.69 time: 0.069s\n",
            "Epoch: 135 loss_train: 0.272 acc_train: 0.921 loss_test: 0.272 acc_test: 0.920 AUC = 0.75 time: 0.070s\n",
            "Epoch: 136 loss_train: 0.279 acc_train: 0.920 loss_test: 0.285 acc_test: 0.920 AUC = 0.71 time: 0.069s\n",
            "Epoch: 137 loss_train: 0.259 acc_train: 0.920 loss_test: 0.267 acc_test: 0.920 AUC = 0.74 time: 0.070s\n",
            "Epoch: 138 loss_train: 0.268 acc_train: 0.920 loss_test: 0.257 acc_test: 0.920 AUC = 0.75 time: 0.069s\n",
            "Epoch: 139 loss_train: 0.279 acc_train: 0.920 loss_test: 0.273 acc_test: 0.920 AUC = 0.74 time: 0.070s\n",
            "Epoch: 140 loss_train: 0.270 acc_train: 0.920 loss_test: 0.260 acc_test: 0.920 AUC = 0.75 time: 0.069s\n",
            "Epoch: 141 loss_train: 0.277 acc_train: 0.920 loss_test: 0.274 acc_test: 0.920 AUC = 0.73 time: 0.069s\n",
            "Epoch: 142 loss_train: 0.272 acc_train: 0.920 loss_test: 0.276 acc_test: 0.920 AUC = 0.72 time: 0.069s\n",
            "Epoch: 143 loss_train: 0.266 acc_train: 0.920 loss_test: 0.270 acc_test: 0.920 AUC = 0.74 time: 0.069s\n",
            "Epoch: 144 loss_train: 0.272 acc_train: 0.920 loss_test: 0.268 acc_test: 0.920 AUC = 0.73 time: 0.069s\n",
            "Epoch: 145 loss_train: 0.267 acc_train: 0.920 loss_test: 0.257 acc_test: 0.920 AUC = 0.77 time: 0.069s\n",
            "Epoch: 146 loss_train: 0.265 acc_train: 0.920 loss_test: 0.289 acc_test: 0.920 AUC = 0.70 time: 0.069s\n",
            "Epoch: 147 loss_train: 0.266 acc_train: 0.920 loss_test: 0.282 acc_test: 0.920 AUC = 0.71 time: 0.069s\n",
            "Epoch: 148 loss_train: 0.276 acc_train: 0.920 loss_test: 0.250 acc_test: 0.920 AUC = 0.78 time: 0.069s\n",
            "Epoch: 149 loss_train: 0.271 acc_train: 0.920 loss_test: 0.268 acc_test: 0.920 AUC = 0.74 time: 0.069s\n",
            "Epoch: 150 loss_train: 0.251 acc_train: 0.920 loss_test: 0.269 acc_test: 0.920 AUC = 0.74 time: 0.069s\n",
            "Epoch: 151 loss_train: 0.256 acc_train: 0.921 loss_test: 0.239 acc_test: 0.920 AUC = 0.81 time: 0.069s\n",
            "Epoch: 152 loss_train: 0.272 acc_train: 0.920 loss_test: 0.269 acc_test: 0.920 AUC = 0.74 time: 0.069s\n",
            "Epoch: 153 loss_train: 0.265 acc_train: 0.920 loss_test: 0.258 acc_test: 0.920 AUC = 0.76 time: 0.069s\n",
            "Epoch: 154 loss_train: 0.277 acc_train: 0.920 loss_test: 0.274 acc_test: 0.920 AUC = 0.72 time: 0.069s\n",
            "Epoch: 155 loss_train: 0.264 acc_train: 0.920 loss_test: 0.275 acc_test: 0.920 AUC = 0.73 time: 0.069s\n",
            "Epoch: 156 loss_train: 0.257 acc_train: 0.920 loss_test: 0.259 acc_test: 0.920 AUC = 0.76 time: 0.069s\n",
            "Epoch: 157 loss_train: 0.260 acc_train: 0.920 loss_test: 0.268 acc_test: 0.920 AUC = 0.74 time: 0.069s\n",
            "Epoch: 158 loss_train: 0.249 acc_train: 0.920 loss_test: 0.295 acc_test: 0.920 AUC = 0.68 time: 0.069s\n",
            "Epoch: 159 loss_train: 0.250 acc_train: 0.920 loss_test: 0.286 acc_test: 0.920 AUC = 0.68 time: 0.069s\n",
            "Epoch: 160 loss_train: 0.259 acc_train: 0.920 loss_test: 0.276 acc_test: 0.920 AUC = 0.73 time: 0.069s\n",
            "Epoch: 161 loss_train: 0.261 acc_train: 0.920 loss_test: 0.277 acc_test: 0.920 AUC = 0.71 time: 0.069s\n",
            "Epoch: 162 loss_train: 0.271 acc_train: 0.920 loss_test: 0.294 acc_test: 0.920 AUC = 0.67 time: 0.070s\n",
            "Epoch: 163 loss_train: 0.242 acc_train: 0.920 loss_test: 0.265 acc_test: 0.920 AUC = 0.76 time: 0.069s\n",
            "Epoch: 164 loss_train: 0.262 acc_train: 0.920 loss_test: 0.288 acc_test: 0.920 AUC = 0.70 time: 0.072s\n",
            "Epoch: 165 loss_train: 0.256 acc_train: 0.920 loss_test: 0.290 acc_test: 0.920 AUC = 0.70 time: 0.069s\n",
            "Epoch: 166 loss_train: 0.260 acc_train: 0.920 loss_test: 0.280 acc_test: 0.920 AUC = 0.73 time: 0.069s\n",
            "Epoch: 167 loss_train: 0.246 acc_train: 0.921 loss_test: 0.279 acc_test: 0.920 AUC = 0.72 time: 0.069s\n",
            "Epoch: 168 loss_train: 0.251 acc_train: 0.920 loss_test: 0.260 acc_test: 0.920 AUC = 0.76 time: 0.069s\n",
            "Epoch: 169 loss_train: 0.259 acc_train: 0.920 loss_test: 0.286 acc_test: 0.920 AUC = 0.70 time: 0.069s\n",
            "Epoch: 170 loss_train: 0.265 acc_train: 0.920 loss_test: 0.276 acc_test: 0.920 AUC = 0.72 time: 0.069s\n",
            "Epoch: 171 loss_train: 0.261 acc_train: 0.920 loss_test: 0.276 acc_test: 0.920 AUC = 0.73 time: 0.069s\n",
            "Epoch: 172 loss_train: 0.258 acc_train: 0.920 loss_test: 0.277 acc_test: 0.920 AUC = 0.74 time: 0.069s\n",
            "Epoch: 173 loss_train: 0.251 acc_train: 0.920 loss_test: 0.269 acc_test: 0.920 AUC = 0.73 time: 0.069s\n",
            "Epoch: 174 loss_train: 0.255 acc_train: 0.920 loss_test: 0.250 acc_test: 0.920 AUC = 0.78 time: 0.069s\n",
            "Epoch: 175 loss_train: 0.253 acc_train: 0.920 loss_test: 0.277 acc_test: 0.920 AUC = 0.71 time: 0.069s\n",
            "Epoch: 176 loss_train: 0.252 acc_train: 0.920 loss_test: 0.265 acc_test: 0.920 AUC = 0.75 time: 0.069s\n",
            "Epoch: 177 loss_train: 0.257 acc_train: 0.920 loss_test: 0.256 acc_test: 0.920 AUC = 0.76 time: 0.069s\n",
            "Epoch: 178 loss_train: 0.249 acc_train: 0.920 loss_test: 0.282 acc_test: 0.920 AUC = 0.69 time: 0.069s\n",
            "Epoch: 179 loss_train: 0.257 acc_train: 0.920 loss_test: 0.288 acc_test: 0.920 AUC = 0.70 time: 0.069s\n",
            "Epoch: 180 loss_train: 0.258 acc_train: 0.920 loss_test: 0.255 acc_test: 0.920 AUC = 0.78 time: 0.069s\n",
            "Epoch: 181 loss_train: 0.250 acc_train: 0.920 loss_test: 0.263 acc_test: 0.920 AUC = 0.74 time: 0.069s\n",
            "Epoch: 182 loss_train: 0.258 acc_train: 0.920 loss_test: 0.266 acc_test: 0.920 AUC = 0.75 time: 0.070s\n",
            "Epoch: 183 loss_train: 0.247 acc_train: 0.920 loss_test: 0.296 acc_test: 0.920 AUC = 0.68 time: 0.069s\n",
            "Epoch: 184 loss_train: 0.265 acc_train: 0.920 loss_test: 0.257 acc_test: 0.920 AUC = 0.78 time: 0.069s\n",
            "Epoch: 185 loss_train: 0.251 acc_train: 0.920 loss_test: 0.257 acc_test: 0.920 AUC = 0.77 time: 0.069s\n",
            "Epoch: 186 loss_train: 0.264 acc_train: 0.920 loss_test: 0.277 acc_test: 0.920 AUC = 0.72 time: 0.069s\n",
            "Epoch: 187 loss_train: 0.239 acc_train: 0.920 loss_test: 0.270 acc_test: 0.920 AUC = 0.73 time: 0.069s\n",
            "Epoch: 188 loss_train: 0.260 acc_train: 0.920 loss_test: 0.275 acc_test: 0.920 AUC = 0.72 time: 0.069s\n",
            "Epoch: 189 loss_train: 0.263 acc_train: 0.920 loss_test: 0.254 acc_test: 0.920 AUC = 0.78 time: 0.069s\n",
            "Epoch: 190 loss_train: 0.264 acc_train: 0.920 loss_test: 0.251 acc_test: 0.920 AUC = 0.78 time: 0.069s\n",
            "Epoch: 191 loss_train: 0.258 acc_train: 0.920 loss_test: 0.264 acc_test: 0.920 AUC = 0.74 time: 0.069s\n",
            "Epoch: 192 loss_train: 0.258 acc_train: 0.920 loss_test: 0.278 acc_test: 0.920 AUC = 0.72 time: 0.069s\n",
            "Epoch: 193 loss_train: 0.255 acc_train: 0.920 loss_test: 0.275 acc_test: 0.920 AUC = 0.72 time: 0.069s\n",
            "Epoch: 194 loss_train: 0.246 acc_train: 0.920 loss_test: 0.265 acc_test: 0.920 AUC = 0.75 time: 0.069s\n",
            "Epoch: 195 loss_train: 0.244 acc_train: 0.920 loss_test: 0.265 acc_test: 0.920 AUC = 0.74 time: 0.069s\n",
            "Epoch: 196 loss_train: 0.238 acc_train: 0.920 loss_test: 0.261 acc_test: 0.920 AUC = 0.76 time: 0.069s\n",
            "Epoch: 197 loss_train: 0.250 acc_train: 0.920 loss_test: 0.263 acc_test: 0.920 AUC = 0.74 time: 0.070s\n",
            "Epoch: 198 loss_train: 0.262 acc_train: 0.920 loss_test: 0.272 acc_test: 0.920 AUC = 0.74 time: 0.070s\n",
            "Epoch: 199 loss_train: 0.247 acc_train: 0.920 loss_test: 0.253 acc_test: 0.920 AUC = 0.76 time: 0.070s\n",
            "Epoch: 200 loss_train: 0.247 acc_train: 0.920 loss_test: 0.257 acc_test: 0.921 AUC = 0.76 time: 0.069s\n",
            "Epoch: 201 loss_train: 0.242 acc_train: 0.920 loss_test: 0.257 acc_test: 0.920 AUC = 0.75 time: 0.070s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.6331s\n",
            "Loading 150th epoch\n",
            "Test set results: loss= 0.2363 accuracy= 0.9195 AUC = 0.82 False Alarm Rate = 0.0000 F1 Score = 0.0000\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import time\n",
        "import json\n",
        "from sklearn import metrics\n",
        "\n",
        "seed = 72\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output  = model(features, adj_list_tt)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    y_prob = torch.exp(output[:, 1]).cpu().detach().numpy()\n",
        "\n",
        "    auc_score = roc_auc_score(labels[idx_test].cpu().detach().numpy(), y_prob[idx_test.cpu().detach().numpy()])\n",
        "    print('Epoch: {:03d}'.format(epoch+1),\n",
        "          'loss_train: {:.3f}'.format(loss_train.data.item()),\n",
        "          'acc_train: {:.3f}'.format(acc_train.data.item()),\n",
        "          'loss_test: {:.3f}'.format(loss_test.data.item()),\n",
        "          'acc_test: {:.3f}'.format(acc_test.data.item()),\n",
        "          \"AUC = {:.2f}\".format(auc_score),\n",
        "          'time: {:.3f}s'.format(time.time() - t))\n",
        "\n",
        "    return loss_test.data.item(), acc_test, auc_score\n",
        "\n",
        "def compute_test(model):\n",
        "    model.eval()\n",
        "    output = model(features, adj_list_tt)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    y_prob = torch.exp(output[:, 1]).cpu().detach().numpy()\n",
        "\n",
        "    auc_score = roc_auc_score(labels[idx_test].cpu().detach().numpy(), y_prob[idx_test.cpu().detach().numpy()])\n",
        "    # compute false alarm rate and f1 score\n",
        "    y_pred = np.array([0 if p < 0.5 else 1 for p in y_prob])\n",
        "    false_alarm_rate = np.mean(y_pred[idx_test.cpu().detach().numpy()] == 1)\n",
        "    f1score = f1_score(labels[idx_test].cpu().detach().numpy(), y_pred[idx_test.cpu().detach().numpy()])\n",
        "    precisionn, recalll, _ = precision_recall_curve(labels[idx_test].cpu().detach().numpy(), y_prob[idx_test.cpu().detach().numpy()])\n",
        "\n",
        "    pr_auc = metrics.auc(recalll, precisionn)\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.data.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.data.item()),\n",
        "          \"AUC = {:.2f}\".format(auc_score),\n",
        "          \"False Alarm Rate = {:.4f}\".format(false_alarm_rate),\n",
        "          \"F1 Score = {:.4f}\".format(f1score))\n",
        "    return loss_test.data.item(), acc_test.data.item(), auc_score, false_alarm_rate, f1score, y_pred, pr_auc\n",
        "\n",
        "#parameter setting\n",
        "#\n",
        "\n",
        "cuda = True\n",
        "epochs = 500\n",
        "lrs = [ 0.001]\n",
        "weight_decays = [5e-5]\n",
        "dropouts = [0.3]\n",
        "hidden_dims1 = [64]\n",
        "#hidden_dims2 = [16, 64,128]\n",
        "#fusion1_dims = [2,4]\n",
        "nb_heads = [ 12]\n",
        "alpha = 0.2 #Alpha for the leaky_relu\n",
        "#lambda_l1s = [0.0001, 0.001, 0.01]\n",
        "patience = 50\n",
        "\n",
        "#lrs = [0.0005,0.005]\n",
        "#weight_decays = 0.0005\n",
        "#dropouts = [0.3]\n",
        "#hidden_dims1 = [64]\n",
        "#hidden_dims2 = [64]\n",
        "#fusion1_dims = [4]\n",
        "#nb_heads = [8]\n",
        "#alpha = 0.2 #Alpha for the leaky_relu\n",
        "#lambda_l1s = [0.0001, 0.001, 0.01]\n",
        "#patience = 50\n",
        "\n",
        "\n",
        "\n",
        "# Load data\n",
        "sv_path = \"\"\n",
        "folder = \"../data/\"\n",
        "att_name = \"att_RR3.csv\"\n",
        "#edge_list_name = [\"edge_pp_main.csv\", \"edge_hh_main.csv\", \"edge_pv_main_1.csv\", \"edge_pv_main_2.csv\", \"edge_pv_main_3.csv\"\n",
        "#, \"edge_pv_main_4.csv\", \"edge_pv_main_5.csv\", \"edge_pv_main_6.csv\", \"edge_pv_main_7.csv\"]\n",
        "edge_list_name = \"edge_sqRR3.csv\"\n",
        "adj_list, features, labels, idx_train, idx_test = load_data(folder, att_name, edge_list_name)\n",
        "\n",
        "#adj_list_tensor = torch.stack(adj_list)\n",
        "features, adj_list_tt, labels = Variable(features), adj_list, Variable(labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "count = 0\n",
        "for lr in lrs:\n",
        "    for dropout in dropouts:\n",
        "        for hid1 in hidden_dims1:\n",
        "          for nb_head in nb_heads:\n",
        "                        for weight_decay in weight_decays:\n",
        "\n",
        "\n",
        "                                # Model and optimizer\n",
        "                                model = GAT(nfeat=features.shape[1],\n",
        "                                            nhid=hid1,\n",
        "                                            nclass=int(labels.max()) + 1,\n",
        "                                            dropout=dropout,\n",
        "                                            alpha=alpha,\n",
        "                                            nheads = nb_head)\n",
        "\n",
        "                                optimizer = optim.Adam(model.parameters(),\n",
        "                                                    lr=lr,\n",
        "                                                    weight_decay=weight_decay)\n",
        "\n",
        "                                if cuda:\n",
        "                                    model.cuda()\n",
        "                                    features = features.cuda()\n",
        "                                    adj_list_tt = adj_list_tt.cuda()\n",
        "                                    labels = labels.cuda()\n",
        "                                    idx_train = torch.tensor(idx_train).cuda()\n",
        "                                    idx_test = torch.tensor(idx_test).cuda()\n",
        "\n",
        "\n",
        "                                # Train model\n",
        "                                t_total = time.time()\n",
        "                                bad_counter = 0\n",
        "                                best_auc = -1\n",
        "                                best_epoch = 0\n",
        "                                for epoch in range(epochs):\n",
        "                                    loss, acc, auc = train(epoch)\n",
        "\n",
        "                                    torch.save(model.state_dict(), sv_path + '{}.pkl'.format(epoch))\n",
        "                                    if auc > best_auc:\n",
        "                                        best_auc = auc\n",
        "                                        best_epoch = epoch\n",
        "                                        bad_counter = 0\n",
        "                                    else:\n",
        "                                        bad_counter += 1\n",
        "\n",
        "                                    if bad_counter == patience:\n",
        "                                        break\n",
        "\n",
        "                                files = glob.glob(sv_path +'*.pkl')\n",
        "                                for file in files:\n",
        "                                    filename = file.split('/')[-1]\n",
        "                                    epoch_nb = int(filename.split('.')[0])\n",
        "                                    if epoch_nb != best_epoch:\n",
        "                                        os.remove(file)\n",
        "\n",
        "                                print(\"Optimization Finished!\")\n",
        "                                print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "                                # Restore best model\n",
        "                                print('Loading {}th epoch'.format(best_epoch))\n",
        "                                model.load_state_dict(torch.load(sv_path +'{}.pkl'.format(best_epoch)))\n",
        "\n",
        "                                # Testing\n",
        "                                test_loss, test_acc, auc, far, f1, y_pred, pr_auc= compute_test(model)\n",
        "                                hyper_para = {}\n",
        "                                hyper_para[\"lr\"] = lr\n",
        "                                hyper_para[\"weight_decay\"] = weight_decay\n",
        "                                hyper_para[\"dropout\"] = dropout\n",
        "                                hyper_para[\"hidden_dim1\"] = hid1\n",
        "                                #hyper_para[\"hidden_dim2\"] = hid2\n",
        "                                #hyper_para[\"lambda\"] = lambda_l1\n",
        "                                #hyper_para[\"fusion1_dim\"] = fusion1_dim\n",
        "                                hyper_para[\"nb_heads\"] = nb_head\n",
        "                                hyper_para[\"alpha\"] = alpha\n",
        "                                hyper_para[\"loss\"] = test_loss\n",
        "                                hyper_para[\"accuracy\"] = test_acc\n",
        "                                hyper_para[\"auc\"] = auc\n",
        "                                hyper_para[\"pr_auc\"] = pr_auc\n",
        "                                hyper_para[\"false_alarm_rate\"] = far\n",
        "                                hyper_para[\"f1_score\"] = f1\n",
        "                                with open(sv_path +\"hyperpara.json\", \"a+\") as fp:\n",
        "                                    fp.write('\\n')\n",
        "                                    json.dump(hyper_para, fp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcU2S4LoSwPy"
      },
      "source": [
        "# FGAT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hnxzhfHZ7gM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "\n",
        "import time\n",
        "import json\n",
        "\n",
        "#from load import accuracy, load_multi_data\n",
        "#from models import FusionGAT3\n",
        "\n",
        "\n",
        "seed = 72\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output, l1_loss = model(features, adj_list_tt)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train]) + l1_loss*lambda_l1\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test]) + l1_loss*lambda_l1\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    y_prob = torch.exp(output[:, 1]).cpu().detach().numpy()\n",
        "\n",
        "    auc_score = roc_auc_score(labels[idx_test].cpu().detach().numpy(), y_prob[idx_test.cpu().detach().numpy()])\n",
        "    print('Epoch: {:03d}'.format(epoch+1),\n",
        "          'loss_train: {:.3f}'.format(loss_train.data.item()),\n",
        "          'acc_train: {:.3f}'.format(acc_train.data.item()),\n",
        "          'loss_test: {:.3f}'.format(loss_test.data.item()),\n",
        "          'acc_test: {:.3f}'.format(acc_test.data.item()),\n",
        "          \"AUC = {:.2f}\".format(auc_score),\n",
        "          'time: {:.3f}s'.format(time.time() - t))\n",
        "\n",
        "    return loss_test.data.item(), acc_test, auc_score\n",
        "\n",
        "def compute_test(model):\n",
        "    model.eval()\n",
        "    output, l1_loss = model(features, adj_list_tt)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test]) + l1_loss*lambda_l1\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    y_prob = torch.exp(output[:, 1]).cpu().detach().numpy()\n",
        "\n",
        "    auc_score = roc_auc_score(labels[idx_test].cpu().detach().numpy(), y_prob[idx_test.cpu().detach().numpy()])\n",
        "    # compute false alarm rate and f1 score\n",
        "    y_pred = np.array([0 if p < 0.5 else 1 for p in y_prob])\n",
        "    false_alarm_rate = np.mean(y_pred[idx_test.cpu().detach().numpy()] == 1)\n",
        "    f1score = f1_score(labels[idx_test].cpu().detach().numpy(), y_pred[idx_test.cpu().detach().numpy()])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.data.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.data.item()),\n",
        "          \"AUC = {:.2f}\".format(auc_score),\n",
        "          \"False Alarm Rate = {:.4f}\".format(false_alarm_rate),\n",
        "          \"F1 Score = {:.4f}\".format(f1score))\n",
        "    return loss_test.data.item(), acc_test.data.item(), auc_score, false_alarm_rate, f1score, y_pred\n",
        "\n",
        "\n",
        "#parameter setting\n",
        "#\n",
        "\n",
        "cuda = True\n",
        "epochs = 500\n",
        "\n",
        "lrs = [0.0005]\n",
        "weight_decays = [5e-5, 5e-4]\n",
        "dropouts = [0.3]\n",
        "hidden_dims1 = [64]\n",
        "hidden_dims2 = [64]\n",
        "fusion1_dims = [2,4]\n",
        "nb_heads = [12]\n",
        "alpha = 0.2 #Alpha for the leaky_relu\n",
        "lambda_l1s = [0.0001, 0.001, 0.01]\n",
        "patience = 50\n",
        "\n",
        "# lrs = [0.0005]\n",
        "# weight_decays = [5e-5]\n",
        "# dropouts = [0.3]\n",
        "# hidden_dims1 = [64]\n",
        "# hidden_dims2 = [64]\n",
        "# fusion1_dims = [4]\n",
        "# nb_heads = [12]\n",
        "# alpha = 0.2 #Alpha for the leaky_relu\n",
        "# lambda_l1s = [0.0001]\n",
        "# patience = 50\n",
        "\n",
        "\n",
        "# Load data\n",
        "sv_path = \"\"\n",
        "folder = \"../data/\"\n",
        "att_name = \"att_RR3.csv\"\n",
        "\n",
        "edge_list_name = [\"edge_sqRR3.csv\", \"edge_clRR3.csv\"]\n",
        "\n",
        "adj_list, features, labels, idx_train, idx_test = load_multi_data(folder, att_name, edge_list_name)\n",
        "\n",
        "adj_list_tensor = torch.stack(adj_list)\n",
        "features, adj_list_tt, labels = Variable(features), adj_list_tensor, Variable(labels)\n",
        "\n",
        "count = 0\n",
        "for lr in lrs:\n",
        "    for dropout in dropouts:\n",
        "        for hid1 in hidden_dims1:\n",
        "            for hid2 in hidden_dims2:\n",
        "                for fusion1_dim in fusion1_dims:\n",
        "                    for nb_head in nb_heads:\n",
        "                        for weight_decay in weight_decays:\n",
        "                            for lambda_l1 in lambda_l1s:\n",
        "\n",
        "                                # Model and optimizer\n",
        "                                model = FusionGAT3(nfeat=features.shape[1],\n",
        "                                            nhid1=hid1,\n",
        "                                            nhid2=hid2,\n",
        "                                            fusion1_dim = fusion1_dim,\n",
        "                                            nclass=int(labels.max()) + 1,\n",
        "                                            dropout=dropout,\n",
        "                                            alpha=alpha,\n",
        "                                            adj_list= adj_list_tt,\n",
        "                                            nheads = nb_head)\n",
        "\n",
        "                                optimizer = optim.Adam(model.parameters(),\n",
        "                                                    lr=lr,\n",
        "                                                    weight_decay=weight_decay)\n",
        "\n",
        "                                if cuda:\n",
        "                                    model.cuda()\n",
        "                                    features = features.cuda()\n",
        "                                    adj_list_tt = adj_list_tt.cuda()\n",
        "                                    labels = labels.cuda()\n",
        "                                    idx_train = torch.tensor(idx_train).cuda()\n",
        "                                    idx_test = torch.tensor(idx_test).cuda()\n",
        "\n",
        "\n",
        "                                # Train model\n",
        "                                t_total = time.time()\n",
        "                                bad_counter = 0\n",
        "                                best_auc = -1\n",
        "                                best_epoch = 0\n",
        "                                for epoch in range(epochs):\n",
        "                                    loss, acc, auc = train(epoch)\n",
        "\n",
        "                                    torch.save(model.state_dict(), sv_path + '{}.pkl'.format(epoch))\n",
        "                                    if auc > best_auc:\n",
        "                                        best_auc = auc\n",
        "                                        best_epoch = epoch\n",
        "                                        bad_counter = 0\n",
        "                                    else:\n",
        "                                        bad_counter += 1\n",
        "\n",
        "                                    if bad_counter == patience:\n",
        "                                        break\n",
        "\n",
        "                                files = glob.glob(sv_path +'*.pkl')\n",
        "                                for file in files:\n",
        "                                    filename = file.split('/')[-1]\n",
        "                                    epoch_nb = int(filename.split('.')[0])\n",
        "                                    if epoch_nb != best_epoch:\n",
        "                                        os.remove(file)\n",
        "\n",
        "                                print(\"Optimization Finished!\")\n",
        "                                print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "                                # Restore best model\n",
        "                                print('Loading {}th epoch'.format(best_epoch))\n",
        "                                model.load_state_dict(torch.load(sv_path +'{}.pkl'.format(best_epoch)))\n",
        "\n",
        "                                # Testing\n",
        "                                test_loss, test_acc, auc, far, f1, y_pred = compute_test(model)\n",
        "                                hyper_para = {}\n",
        "                                hyper_para[\"lr\"] = lr\n",
        "                                hyper_para[\"weight_decay\"] = weight_decay\n",
        "                                hyper_para[\"dropout\"] = dropout\n",
        "                                hyper_para[\"hidden_dim1\"] = hid1\n",
        "                                hyper_para[\"hidden_dim2\"] = hid2\n",
        "                                hyper_para[\"lambda\"] = lambda_l1\n",
        "                                hyper_para[\"fusion1_dim\"] = fusion1_dim\n",
        "                                hyper_para[\"nb_heads\"] = nb_head\n",
        "                                hyper_para[\"alpha\"] = alpha\n",
        "                                hyper_para[\"loss\"] = test_loss\n",
        "                                hyper_para[\"accuracy\"] = test_acc\n",
        "                                hyper_para[\"auc\"] = auc\n",
        "                                hyper_para[\"false_alarm_rate\"] = far\n",
        "                                hyper_para[\"f1_score\"] = f1\n",
        "                                with open(sv_path +\"hyperpara.json\", \"a+\") as fp:\n",
        "                                    fp.write('\\n')\n",
        "                                    json.dump(hyper_para, fp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuqyTzYIYRfQ"
      },
      "source": [
        "{\"lr\": 0.0005, \"weight_decay\": 5e-05, \"dropout\": 0.3, \"hidden_dim1\": 64, \"hidden_dim2\": 64, \"lambda\": 0.0001, \"fusion1_dim\": 4, \"nb_heads\": 12, \"alpha\": 0.2, \"loss\": 0.23118849098682404, \"accuracy\": 0.9195121951219513, \"auc\": 0.8472691102001447, \"false_alarm_rate\": 0.0, \"f1_score\": 0.0}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
